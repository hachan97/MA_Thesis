{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10280888,"sourceType":"datasetVersion","datasetId":6361952}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup\n\n**THINGS TO FIX**:\n- ...\n- ...","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\nimport matplotlib.pyplot as plt\nimport datetime\nfrom datasets import Dataset\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\n\n#import torch.nn.attention.flex_attention\n#from torchtune.modules.tokenizers import ModelTokenizer\n#from torchtune.models.llama3 import llama3_tokenizer\n#from torchtune.data import Message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:41:09.800575Z","iopub.execute_input":"2025-02-14T12:41:09.800995Z","iopub.status.idle":"2025-02-14T12:41:18.44309Z","shell.execute_reply.started":"2025-02-14T12:41:09.800961Z","shell.execute_reply":"2025-02-14T12:41:18.44239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n# Login to Hugging Face\nfrom huggingface_hub import login\n\nlogin(Hugging_Face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:42:40.871107Z","iopub.execute_input":"2025-02-14T12:42:40.871404Z","iopub.status.idle":"2025-02-14T12:42:41.112868Z","shell.execute_reply.started":"2025-02-14T12:42:40.871382Z","shell.execute_reply":"2025-02-14T12:42:41.112212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"#df_HoC_2000s_raw = pd.read_csv('H:/MA_Thesis/data/Rauh_Schwalbach_2020_ParlSpeech/df_HoC_2000s.csv')\n\ndf_HoC_2000s_raw = pd.read_csv('/kaggle/input/parlspeech/df_HoC_2000s.csv')\ndf_HoC_2000s_raw.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:03:30.306623Z","iopub.execute_input":"2025-02-14T13:03:30.30695Z","iopub.status.idle":"2025-02-14T13:03:47.770403Z","shell.execute_reply.started":"2025-02-14T13:03:30.306927Z","shell.execute_reply":"2025-02-14T13:03:47.769519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_HoC_2000s = df_HoC_2000s_raw[['date', 'agenda', 'speechnumber', 'speaker', 'party','text']]\ndf_HoC_2000s.columns\ndf_HoC_2000s.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:04:46.695426Z","iopub.execute_input":"2025-02-14T13:04:46.695733Z","iopub.status.idle":"2025-02-14T13:04:46.764033Z","shell.execute_reply.started":"2025-02-14T13:04:46.695711Z","shell.execute_reply":"2025-02-14T13:04:46.763217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analaysis","metadata":{}},{"cell_type":"code","source":"# Most Frequent Speaker\ndf_HoC_2000s['speaker'].value_counts().head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:07:42.177482Z","iopub.execute_input":"2025-02-14T13:07:42.177786Z","iopub.status.idle":"2025-02-14T13:07:42.191793Z","shell.execute_reply.started":"2025-02-14T13:07:42.177762Z","shell.execute_reply":"2025-02-14T13:07:42.190984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"David Cameron has {df_HoC_2000s_raw[df_HoC_2000s_raw['speaker'] == 'David Cameron']['terms'].sum()} terms\")\nprint(f\"Boris Johnson has {df_HoC_2000s_raw[df_HoC_2000s_raw['speaker'] == 'Boris Johnson']['terms'].sum()} terms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:03:35.547247Z","iopub.execute_input":"2025-02-06T14:03:35.547603Z","iopub.status.idle":"2025-02-06T14:03:35.736324Z","shell.execute_reply.started":"2025-02-06T14:03:35.547575Z","shell.execute_reply":"2025-02-06T14:03:35.735384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_HoC_2005 = df_HoC_2000s[df_HoC_2000s['date'].str.contains('2005')]\n#df_HoC_2015 = df_HoC_2000s[df_HoC_2000s['date'].str.contains('2015')]\n#df_HoC_2011 = df_HoC_2000s[df_HoC_2000s['date'].str.contains('2011')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:07:33.829801Z","iopub.execute_input":"2025-02-14T13:07:33.830128Z","iopub.status.idle":"2025-02-14T13:07:34.848396Z","shell.execute_reply.started":"2025-02-14T13:07:33.830105Z","shell.execute_reply":"2025-02-14T13:07:34.847627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Tokenizer","metadata":{}},{"cell_type":"code","source":"BASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:43:45.531477Z","iopub.execute_input":"2025-02-14T12:43:45.531736Z","iopub.status.idle":"2025-02-14T12:43:45.535361Z","shell.execute_reply.started":"2025-02-14T12:43:45.531716Z","shell.execute_reply":"2025-02-14T12:43:45.534448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the tokenizer to measure the length of the text\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, \n                                          add_bos_token=False, \n                                          trust_remote_code=True, \n                                          use_fast=True, \n                                          force_download=False)\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:43:46.493Z","iopub.execute_input":"2025-02-14T12:43:46.493272Z","iopub.status.idle":"2025-02-14T12:43:49.166178Z","shell.execute_reply.started":"2025-02-14T12:43:46.493252Z","shell.execute_reply":"2025-02-14T12:43:49.165331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min(tokenizer.vocab.values()), max(tokenizer.vocab.values()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:43:49.167226Z","iopub.execute_input":"2025-02-14T12:43:49.167508Z","iopub.status.idle":"2025-02-14T12:43:49.360671Z","shell.execute_reply.started":"2025-02-14T12:43:49.167487Z","shell.execute_reply":"2025-02-14T12:43:49.359765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.special_tokens_map)\n\ntest_text = \"Hello, how are you today?\"\ntokens = tokenizer.encode(test_text, return_tensors=\"pt\")\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:43:50.487142Z","iopub.execute_input":"2025-02-14T12:43:50.487431Z","iopub.status.idle":"2025-02-14T12:43:50.523927Z","shell.execute_reply.started":"2025-02-14T12:43:50.48741Z","shell.execute_reply":"2025-02-14T12:43:50.52321Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Data for Training\n\n**References on Preprocessing Dataset for Fine-tuning**\n- https://pytorch.org/torchtune/0.2/tutorials/chat.html\n- https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7\n\nYes, I would like to modify how many 'user' messages are included before and after each assistant response?","metadata":{}},{"cell_type":"code","source":"# Constants\nTOKEN_LENGTH_LIMIT = 3500\nMIN_TOKEN_LENGTH = 512\n\nMAX_NO_ASSISTANT_THRESHOLD = 10       # Reset conversation if assistant is absent for too long\nCHAT_OWNER = \"Theresa May\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:59:28.820247Z","iopub.execute_input":"2025-02-14T12:59:28.820572Z","iopub.status.idle":"2025-02-14T12:59:28.824408Z","shell.execute_reply.started":"2025-02-14T12:59:28.820537Z","shell.execute_reply":"2025-02-14T12:59:28.823541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_convo_1(df, output_path):\n    SYSTEM_PROMPT = \"\"\"You are Theresa May, a politician in the UK's House of Commons.\n    You are responding to Observations.\n    Respond exactly as Theresa May would speak, \n    staying fully in character and address the observation directly.\"\"\"\n\n    SYSTEM_PROMPT_TOKEN_LEN = len(tokenizer.encode(SYSTEM_PROMPT))\n\n    df = df.assign(role=df[\"speaker\"].apply(lambda x: \"assistant\" if x == CHAT_OWNER else \"user\"))  # Assign roles\n    grouped = df.groupby([\"date\", \"agenda\"])\n\n    conversations = []\n\n    for (date, agenda), group in tqdm(grouped):\n        conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n        token_len = SYSTEM_PROMPT_TOKEN_LEN\n        has_assistant_message = False\n        user_message_count = 0  \n\n        for _, row in group.iterrows():\n            role, message = row[\"role\"], row[\"text\"]\n            chat_message = {\"role\": role, \"content\": message}\n\n            # Simulate applying chat template before checking length\n            temp_messages = conversation + [chat_message]\n            temp_prompt = tokenizer.apply_chat_template(temp_messages, tokenize=False, add_generation_prompt=True)\n            temp_token_len = len(tokenizer.encode(temp_prompt))\n\n            # Check if adding this message exceeds the token limit\n            if temp_token_len > TOKEN_LENGTH_LIMIT or user_message_count >= MAX_NO_ASSISTANT_THRESHOLD:\n                if token_len >= MIN_TOKEN_LENGTH and has_assistant_message:\n                    conversations.append({\"conversation\": conversation})\n\n                # Restart conversation\n                conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n                token_len = SYSTEM_PROMPT_TOKEN_LEN\n                has_assistant_message = False\n                user_message_count = 0  \n\n            # Append message to conversation\n            conversation.append(chat_message)\n            token_len = temp_token_len  # Update token count\n\n            if role == \"assistant\":\n                has_assistant_message = True\n                user_message_count = 0  \n            else:\n                user_message_count += 1  \n\n        # Save the last conversation if it meets the minimum length and contains an assistant message\n        if token_len >= MIN_TOKEN_LENGTH and has_assistant_message:\n            conversations.append({\"conversation\": conversation})\n\n    # Write to JSONL file\n    with open(output_path, 'w') as f:\n        for convo in conversations:\n            f.write(json.dumps(convo) + '\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:44:00.131556Z","iopub.execute_input":"2025-02-14T12:44:00.131882Z","iopub.status.idle":"2025-02-14T12:44:00.138978Z","shell.execute_reply.started":"2025-02-14T12:44:00.131853Z","shell.execute_reply":"2025-02-14T12:44:00.138181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_path = '/kaggle/working/preprocessed_TheresaMay.jsonl'\npreprocess_convo_1(df_HoC_2000s, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:07:54.647973Z","iopub.execute_input":"2025-02-14T13:07:54.648257Z","iopub.status.idle":"2025-02-14T13:14:04.479169Z","shell.execute_reply.started":"2025-02-14T13:07:54.648235Z","shell.execute_reply":"2025-02-14T13:14:04.478295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check token lengths in dataset (with proper estimation)\ntotal_tokens = []\nwith open(output_path, 'r') as f:\n    for line in f:\n        data = json.loads(line)\n        \n        # Apply chat template before estimating token count\n        prompt = tokenizer.apply_chat_template(data[\"conversation\"], tokenize=False, add_generation_prompt=True)\n        token_count = len(tokenizer.encode(prompt))  # Tokenize after applying template\n\n        total_tokens.append(token_count)\n\nprint(f\"Min tokens: {min(total_tokens)}, Max tokens: {max(total_tokens)}, Avg tokens: {sum(total_tokens)/len(total_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:40:46.55338Z","iopub.execute_input":"2025-02-14T13:40:46.553676Z","iopub.status.idle":"2025-02-14T13:40:48.700872Z","shell.execute_reply.started":"2025-02-14T13:40:46.553654Z","shell.execute_reply":"2025-02-14T13:40:48.700129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"over_limit = [tokens for tokens in total_tokens if tokens > 4096]\nprint(f\"Number of conversations exceeding 4096 tokens: {len(over_limit)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:55:48.115399Z","iopub.execute_input":"2025-02-14T12:55:48.115681Z","iopub.status.idle":"2025-02-14T12:55:48.120384Z","shell.execute_reply.started":"2025-02-14T12:55:48.11566Z","shell.execute_reply":"2025-02-14T12:55:48.11958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the histogram of 'Token Lengths'\nplt.figure(figsize=(10, 6))\nplt.hist(total_tokens, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Token Counts in Preprocessed Conversations\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:56:29.859278Z","iopub.execute_input":"2025-02-14T12:56:29.85958Z","iopub.status.idle":"2025-02-14T12:56:30.15Z","shell.execute_reply.started":"2025-02-14T12:56:29.859558Z","shell.execute_reply":"2025-02-14T12:56:30.149056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect a Few Samples\ndialog_blocks = []\nwith open(output_path, 'r') as f:\n    for line in f:\n        dialog_blocks.append(json.loads(line))  # Parse each line as a JSON object\n\n# Display the number of dialog blocks\nprint(f\"Total dialog blocks: {len(dialog_blocks)}\")\n#dialog_blocks[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:56:34.075367Z","iopub.execute_input":"2025-02-14T12:56:34.075653Z","iopub.status.idle":"2025-02-14T12:56:34.089205Z","shell.execute_reply.started":"2025-02-14T12:56:34.075629Z","shell.execute_reply":"2025-02-14T12:56:34.08838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare & Tokenize","metadata":{}},{"cell_type":"code","source":"today_date = datetime.datetime.now().strftime(\"%d %b %Y\")\n\n# Load the preprocessed JSONL dataset\nwith open(output_path, \"r\") as f:\n    raw_data = [json.loads(line) for line in f]\n\n# Convert into a dataset format that follows the guide\nformatted_data = []\n\nfor convo in raw_data:\n    messages = []\n    \n    for turn in convo[\"conversation\"]:\n        if turn[\"role\"] == \"system\":\n            messages.append({\"role\": \"system\", \"content\": turn[\"content\"]})\n        elif turn[\"role\"] == \"user\":\n            messages.append({\"role\": \"user\", \"content\": turn[\"content\"]})\n        elif turn[\"role\"] == \"assistant\":\n            messages.append({\"role\": \"assistant\", \"content\": turn[\"content\"]})\n    \n    # Apply chat template\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    prompt = prompt.replace(f\"Cutting Knowledge Date: December 2023\\nToday Date: {today_date}\\n\\n\",\"\")\n    \n    formatted_data.append({\"prompt\": prompt})\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_list(formatted_data)\n\ndef tokenize_function(example):\n    tokens = tokenizer(example['prompt'], \n                       add_special_tokens=False,\n                       padding=\"longest\", \n                       truncation=True, \n                       max_length=4096)\n    \n    tokens['labels'] = [-100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']    ]\n\n    return tokens\n\n# Apply tokenization\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset.save_to_disk(\"/kaggle/working/tokenized_dataset_DavidCameron\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:40:19.769411Z","iopub.execute_input":"2025-02-14T13:40:19.769716Z","iopub.status.idle":"2025-02-14T13:40:21.00201Z","shell.execute_reply.started":"2025-02-14T13:40:19.769693Z","shell.execute_reply":"2025-02-14T13:40:21.001317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:40:06.201907Z","iopub.execute_input":"2025-02-14T13:40:06.20224Z","iopub.status.idle":"2025-02-14T13:40:06.207238Z","shell.execute_reply.started":"2025-02-14T13:40:06.202217Z","shell.execute_reply":"2025-02-14T13:40:06.206413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#decoded_text = tokenizer.decode(tokenized_dataset[2][\"input_ids\"])\n#decoded_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T10:59:41.966523Z","iopub.execute_input":"2025-02-12T10:59:41.966842Z","iopub.status.idle":"2025-02-12T10:59:41.970096Z","shell.execute_reply.started":"2025-02-12T10:59:41.966819Z","shell.execute_reply":"2025-02-12T10:59:41.969392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization & Insights\n\n","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Extract token lengths\ntoken_lengths = [len(sample[\"input_ids\"]) for sample in tokenized_dataset]\n\n# Plot histogram\nplt.figure(figsize=(8, 5))\nplt.hist(token_lengths, bins=50, color=\"blue\", edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Token Length\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Tokenized Sequence Lengths\")\nplt.axvline(x=4096, color=\"red\", linestyle=\"dashed\", label=\"Max Length (4096)\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:40:27.363618Z","iopub.execute_input":"2025-02-14T13:40:27.36396Z","iopub.status.idle":"2025-02-14T13:40:28.35397Z","shell.execute_reply.started":"2025-02-14T13:40:27.363932Z","shell.execute_reply":"2025-02-14T13:40:28.352885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"truncated_samples = sum(1 for sample in tokenized_dataset if len(sample[\"input_ids\"]) == 4096)\ntotal_samples = len(tokenized_dataset)\n\nplt.figure(figsize=(6, 5))\nplt.pie([truncated_samples, total_samples - truncated_samples], \n        labels=[\"Truncated\", \"Not Truncated\"], autopct=\"%1.1f%%\", colors=[\"red\", \"green\"])\nplt.title(\"Percentage of Truncated Samples\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:40:30.146429Z","iopub.execute_input":"2025-02-14T13:40:30.146869Z","iopub.status.idle":"2025-02-14T13:40:30.960474Z","shell.execute_reply.started":"2025-02-14T13:40:30.146836Z","shell.execute_reply":"2025-02-14T13:40:30.959692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate padding ratios\npadding_ratios = [(sample[\"input_ids\"].count(tokenizer.pad_token_id) / len(sample[\"input_ids\"])) \n                  for sample in tokenized_dataset]\n\n# Plot distribution of padding percentages\nplt.figure(figsize=(8, 5))\nplt.hist(padding_ratios, bins=30, color=\"orange\", edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Padding Ratio\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Padding Proportion in Tokenized Samples\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:41:06.536499Z","iopub.execute_input":"2025-02-14T13:41:06.536799Z","iopub.status.idle":"2025-02-14T13:41:07.453358Z","shell.execute_reply.started":"2025-02-14T13:41:06.536776Z","shell.execute_reply":"2025-02-14T13:41:07.452628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_tokens = []\nassistant_tokens = []\n\nfor sample in tokenized_dataset:\n    decoded_text = tokenizer.decode(sample[\"input_ids\"])\n    user_tokens.append(decoded_text.count(\"<|start_header_id|>user\"))\n    assistant_tokens.append(decoded_text.count(\"<|start_header_id|>assistant\"))\n\nplt.figure(figsize=(8, 5))\nplt.hist(user_tokens, bins=30, color=\"blue\", alpha=0.5, label=\"User Tokens\")\nplt.hist(assistant_tokens, bins=30, color=\"green\", alpha=0.5, label=\"Assistant Tokens\")\nplt.xlabel(\"Tokens Per Sample\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of User vs. Assistant Tokens\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T13:41:09.165388Z","iopub.execute_input":"2025-02-14T13:41:09.165681Z","iopub.status.idle":"2025-02-14T13:41:25.305077Z","shell.execute_reply.started":"2025-02-14T13:41:09.16566Z","shell.execute_reply":"2025-02-14T13:41:25.304301Z"}},"outputs":[],"execution_count":null}]}