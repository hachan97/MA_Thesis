{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10908460,"sourceType":"datasetVersion","datasetId":6739203},{"sourceId":225019966,"sourceType":"kernelVersion"},{"sourceId":225413604,"sourceType":"kernelVersion"},{"sourceId":225414297,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom datasets import Dataset\n\n# Text Similarity\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\nimport torch\nfrom rouge_score import rouge_scorer\nfrom sentence_transformers import SentenceTransformer, util\n\n# Statistical Packages\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import mahalanobis\n#from sklearn.preprocessing import StandardScaler\n#from numpy.linalg import inv, pinv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:09:22.730703Z","iopub.execute_input":"2025-03-03T14:09:22.730985Z","iopub.status.idle":"2025-03-03T14:09:39.721927Z","shell.execute_reply.started":"2025-03-03T14:09:22.730961Z","shell.execute_reply":"2025-03-03T14:09:39.721254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Inference/Simulated Data","metadata":{}},{"cell_type":"code","source":"df_Inferenced_TheresaMay_llama_finetuned = pd.read_csv('/kaggle/input/inferencing-llam3-2-finetuned-theresamay-v2-ipynb/df_pairs_TheresaMay_simulated_Llama3.2_finetuned.csv')\ndf_Inferenced_TheresaMay_llama_base = pd.read_csv('/kaggle/input/inferencing-llam3-2-base-theresamay-v2-ipynb/df_pairs_TheresaMay_simulated_Llama3.2_base.csv')\ndf_Inferenced_TheresaMay_gpt = pd.read_csv('/kaggle/input/inferencing-gpt3-5-theresamay-ipynb/df_pairs_TheresaMay_simulated_Gpt3.5.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:09:44.702807Z","iopub.execute_input":"2025-03-03T14:09:44.703476Z","iopub.status.idle":"2025-03-03T14:09:44.816760Z","shell.execute_reply.started":"2025-03-03T14:09:44.703447Z","shell.execute_reply":"2025-03-03T14:09:44.816071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_Inferenced_TheresaMay_llama_finetuned.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:10:08.295980Z","iopub.execute_input":"2025-03-03T14:10:08.296333Z","iopub.status.idle":"2025-03-03T14:10:08.301607Z","shell.execute_reply.started":"2025-03-03T14:10:08.296289Z","shell.execute_reply":"2025-03-03T14:10:08.300713Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Similarity Metrics\n\nSentence Embeddings (BERT, RoBERTa, Sentence-BERT)\n- BERT (Bidirectional Encoder Representations from Transformers): A transformer-based model that provides contextual embeddings for words and sentences.\n- RoBERTa: A robustly optimized version of BERT.\n- Sentence-BERT: A modification of BERT that produces semantically meaningful sentence embeddings.","metadata":{}},{"cell_type":"code","source":"## BLEU Scores\nnltk.download('punkt')\ndef compute_bleu(reference, candidate):\n    reference = [nltk.word_tokenize(reference.lower())]\n    candidate = nltk.word_tokenize(candidate.lower())\n    smoothie = SmoothingFunction().method4  # Avoid zero BLEU for short sentences\n    return sentence_bleu(reference, candidate, smoothing_function=smoothie)\n\n## ROUGE Scores\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\ndef compute_rouge(reference, candidate):\n    scores = scorer.score(reference, candidate)\n    return scores['rouge1'].fmeasure\n\n# Sentence Embeddings Simlarity\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\ndef compute_cosine_similarity(reference, candidate):\n    ref_embedding = embedding_model.encode(reference, convert_to_tensor=True)\n    cand_embedding = embedding_model.encode(candidate, convert_to_tensor=True)\n    return util.pytorch_cos_sim(ref_embedding, cand_embedding).item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:10:12.543485Z","iopub.execute_input":"2025-03-03T14:10:12.543799Z","iopub.status.idle":"2025-03-03T14:10:22.594121Z","shell.execute_reply.started":"2025-03-03T14:10:12.543772Z","shell.execute_reply":"2025-03-03T14:10:22.593183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compute & Plot","metadata":{}},{"cell_type":"code","source":"# Compute similarity metrics for all models\nfor df, model_name in zip([df_Inferenced_TheresaMay_llama_finetuned, \n                           df_Inferenced_TheresaMay_llama_base, \n                           df_Inferenced_TheresaMay_gpt], \n                          ['Llama3.2-Finetuned', 'Llama3.2-Base', 'GPT-3.5']):\n    df['BLEU'] = df.apply(lambda row: compute_bleu(row['response'], row['model_response']), axis=1)\n    df['ROUGE'] = df.apply(lambda row: compute_rouge(row['response'], row['model_response']), axis=1)\n    df['Cosine_Similarity'] = df.apply(lambda row: compute_cosine_similarity(row['response'], row['model_response']), axis=1)\n    df['Model'] = model_name  # Add a column to identify the model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:10:35.094106Z","iopub.execute_input":"2025-03-03T14:10:35.094441Z","iopub.status.idle":"2025-03-03T14:11:30.807280Z","shell.execute_reply.started":"2025-03-03T14:10:35.094413Z","shell.execute_reply":"2025-03-03T14:11:30.806473Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all results\ndf_plotting_TM = pd.concat([\n    df_Inferenced_TheresaMay_llama_finetuned[['BLEU', 'ROUGE', 'Cosine_Similarity', 'Model']],\n    df_Inferenced_TheresaMay_llama_base[['BLEU', 'ROUGE', 'Cosine_Similarity', 'Model']],\n    df_Inferenced_TheresaMay_gpt[['BLEU', 'ROUGE', 'Cosine_Similarity', 'Model']]\n])\n\ndf_plotting_melted_TM = df_plotting_TM.melt(id_vars=['Model'], var_name='Metric', value_name='Score')\ndf_plotting_melted_TM['Metric'] = df_plotting_melted_TM['Metric'].replace('Cosine_Similarity', 'Sentence Embeddings Simlarity')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:12:04.641490Z","iopub.execute_input":"2025-03-03T14:12:04.641771Z","iopub.status.idle":"2025-03-03T14:12:04.653433Z","shell.execute_reply.started":"2025-03-03T14:12:04.641751Z","shell.execute_reply":"2025-03-03T14:12:04.652494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the box plots\nplt.figure(figsize=(14, 8))\nsns.boxplot(x='Metric', y='Score', hue='Model', \n            data=df_plotting_melted_TM, palette=\"Set2\", width=0.6)\nplt.title('Model Similarity - Real vs. Simulated Response (TheresaMay)', fontsize=18, fontweight='bold')\nplt.xlabel('Metric', fontsize=16)\nplt.ylabel('Score', fontsize=16)\nplt.xticks(#rotation=0, ha='right', \n           fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title='Model', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.ylim(0,1)\n\nplt.savefig('/kaggle/working/models_similarity_TheresaMay')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:12:06.097096Z","iopub.execute_input":"2025-03-03T14:12:06.097512Z","iopub.status.idle":"2025-03-03T14:12:06.765361Z","shell.execute_reply.started":"2025-03-03T14:12:06.097478Z","shell.execute_reply":"2025-03-03T14:12:06.764556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load LIWC Results Data","metadata":{}},{"cell_type":"code","source":"# Define the columns to exclude\nexclude_columns = ['prompt_date', 'prompt_agenda', 'prompt_speaker', 'prompt', 'prompt_terms', 'response_terms', 'Text']\n\n# Define a lambda function to filter out the columns to exclude\nusecols = lambda column: column not in exclude_columns\n\n# Load the dataframes with the filtered columns\nLIWC_df_TheresaMay_gpt = pd.read_csv('/kaggle/input/debate-results/LIWC_analysis_TheresaMay_gpt.csv', usecols=usecols)\nLIWC_df_TheresaMay_llamaBase = pd.read_csv('/kaggle/input/debate-results/LIWC_analysis_TheresaMay_llama_base.csv', usecols=usecols)\nLIWC_df_TheresaMay_llamaTuned = pd.read_csv('/kaggle/input/debate-results/LIWC_analysis_TheresaMay_llama_finetuned.csv', usecols=usecols)\n\nprint(LIWC_df_TheresaMay_gpt.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:13:17.946408Z","iopub.execute_input":"2025-03-03T14:13:17.946688Z","iopub.status.idle":"2025-03-03T14:13:18.030789Z","shell.execute_reply.started":"2025-03-03T14:13:17.946667Z","shell.execute_reply":"2025-03-03T14:13:18.030043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for NAs\nna_counts_gpt = LIWC_df_TheresaMay_gpt.isna().sum()\nna_counts_llamaBase = LIWC_df_TheresaMay_llamaBase.isna().sum()\nna_counts_llamaTuned = LIWC_df_TheresaMay_llamaTuned.isna().sum()\n\n# Print columns with NAs\nprint(\"Columns with NAs in LIWC_df_TheresaMay_gpt:\")\nprint(na_counts_gpt[na_counts_gpt > 0])\n\nprint(\"\\nColumns with NAs in LIWC_df_TheresaMay_llamaBase:\")\nprint(na_counts_llamaBase[na_counts_llamaBase > 0])\n\nprint(\"\\nColumns with NAs in LIWC_df_TheresaMay_llamaTuned:\")\nprint(na_counts_llamaTuned[na_counts_llamaTuned > 0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:13:29.076572Z","iopub.execute_input":"2025-03-03T14:13:29.076868Z","iopub.status.idle":"2025-03-03T14:13:29.087536Z","shell.execute_reply.started":"2025-03-03T14:13:29.076844Z","shell.execute_reply":"2025-03-03T14:13:29.086654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace NA values with 0 \nLIWC_df_TheresaMay_gpt = LIWC_df_TheresaMay_gpt.fillna(0)\nLIWC_df_TheresaMay_llamaBase = LIWC_df_TheresaMay_llamaBase.fillna(0)\nLIWC_df_TheresaMay_llamaTuned = LIWC_df_TheresaMay_llamaTuned.fillna(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:13:26.807554Z","iopub.execute_input":"2025-03-03T14:13:26.807822Z","iopub.status.idle":"2025-03-03T14:13:26.814591Z","shell.execute_reply.started":"2025-03-03T14:13:26.807802Z","shell.execute_reply":"2025-03-03T14:13:26.813748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"liwc_features = LIWC_df_TheresaMay_llamaTuned.columns[3:,]\nliwc_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:13:31.218262Z","iopub.execute_input":"2025-03-03T14:13:31.218555Z","iopub.status.idle":"2025-03-03T14:13:31.223962Z","shell.execute_reply.started":"2025-03-03T14:13:31.218533Z","shell.execute_reply":"2025-03-03T14:13:31.223214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"liwc_categories = {\n    \"Summary Variables\": ['Analytic', 'Clout', 'Authentic', \n                          'Tone', 'WPS', 'BigWords', 'Dic'],\n    \"Linguistic Dimensions\": ['Linguistic', 'function', 'pronoun', 'ppron', 'ipron', 'det', \n                              'article', 'number', 'prep', 'auxverb', 'adverb', 'conj', \n                              'negate', 'verb', 'adj', 'quantity'],\n    \"Psychological Processes\": ['Drives', 'affiliation', 'achieve', 'power', 'Cognition', \n                                'allnone', 'cogproc', 'insight', 'cause', 'discrep', 'tentat', \n                                'certitude', 'differ', 'memory', 'Affect', 'tone_pos', 'tone_neg', \n                                'emoti,on', 'emo_pos', 'emo_neg', 'emo,anx', 'emo_anger', 'emo,sad', \n                                'socbehav', 'pro,social', 'polite', 'confl,ict', 'moral', 'soc,refs', \n                                'family', 'friend', 'female', 'male'],\n    'Culture': ['Culture', 'politic', 'ethnicity', 'tech'],\n    'Lifestyle': ['lifestyle', 'leisure', 'home', 'work', 'money', 'relig'],\n    'Physical': ['physical', 'health', 'illness', 'wellness', 'mental', 'substances', 'sexual', 'food', 'death'],\n    'States': ['need', 'want', 'acquire', 'lack', 'fulfill', 'fatigue'],\n    'Motives': ['reward', 'risk', 'curiosity', 'allure'],\n    'Perception': ['Perception', 'attention', 'motion', 'space', 'visual', 'auditory', 'feeling']\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:13:33.361071Z","iopub.execute_input":"2025-03-03T14:13:33.361423Z","iopub.status.idle":"2025-03-03T14:13:33.367640Z","shell.execute_reply.started":"2025-03-03T14:13:33.361395Z","shell.execute_reply":"2025-03-03T14:13:33.366766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models_TM = {\n    \"GPT-3.5\": LIWC_df_TheresaMay_gpt,\n    \"Llama Base\": LIWC_df_TheresaMay_llamaBase,\n    \"Llama Fine-Tuned\": LIWC_df_TheresaMay_llamaTuned\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:13:38.048591Z","iopub.execute_input":"2025-03-03T14:13:38.048900Z","iopub.status.idle":"2025-03-03T14:13:38.052579Z","shell.execute_reply.started":"2025-03-03T14:13:38.048873Z","shell.execute_reply":"2025-03-03T14:13:38.051681Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIWC Cosine Similarity","metadata":{}},{"cell_type":"code","source":"cosine_similarity_results_TM = []\n\nfor model_name, df_model in models_TM.items():\n    for category, features in liwc_categories.items():\n\n        features = [f for f in features if f in df_model.columns]\n        \n        if not features:  # Skip categories without valid features\n            continue\n\n        # Compute Cosine Similarity for each pair of consecutive rows\n        for i in range(0, len(df_model) - 1, 2):\n            real_response = df_model.iloc[i][features].values.reshape(1, -1)\n            model_response = df_model.iloc[i+1][features].values.reshape(1, -1)\n            \n            cos_sim = cosine_similarity(real_response, model_response)[0][0]\n            cosine_similarity_results_TM.append({\n                \"Model\": model_name,\n                \"Category\": category,\n                \"Cosine Similarity\": cos_sim\n            })\n\ndf_LIWC_cosine_sim_TM = pd.DataFrame(cosine_similarity_results_TM)\ndf_LIWC_cosine_sim_filtered_TM = df_LIWC_cosine_sim_TM[~df_LIWC_cosine_sim_TM['Category'].isin([\"Culture\", \"Lifestyle\", \"Physical\", \"States\"])]\n\nplt.figure(figsize=(14, 8))\nsns.boxplot(data = df_LIWC_cosine_sim_filtered_TM, \n            x=\"Category\", y=\"Cosine Similarity\", \n            hue=\"Model\", palette=\"coolwarm\", width=0.6)\nplt.title(\"Linguistic Feature Alignment of LLM Models: LIWC Cosine Similarity\", fontsize=18, fontweight='bold')\nplt.ylabel(\"Cosine Similarity\", fontsize=14)\nplt.xlabel(\"LIWC Categories\", fontsize=14)\nplt.xticks(rotation=45, ha='right', fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title=\"Model\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig('/kaggle/working/models_similarity_LIWC_cosine_TheresaMay.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:18:55.308242Z","iopub.execute_input":"2025-03-03T14:18:55.308571Z","iopub.status.idle":"2025-03-03T14:19:04.595028Z","shell.execute_reply.started":"2025-03-03T14:18:55.308544Z","shell.execute_reply":"2025-03-03T14:19:04.594220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LIWC Mahalanobis Distance","metadata":{}},{"cell_type":"code","source":"mahalanobis_results_TM = []\n\nfor model_name, df_model in models_TM.items():\n    for category, features in liwc_categories.items():\n        # Ensure features exist in the dataset\n        features = [f for f in features if f in df_model.columns]\n        \n        if not features:  # Skip categories without valid features\n            continue\n\n        # Compute Mahalanobis Distance for each pair of consecutive rows\n        for i in range(0, len(df_model) - 1, 2):\n            real_response = df_model.iloc[i][features].values\n            model_response = df_model.iloc[i+1][features].values\n\n            # Compute covariance matrix and inverse\n            cov_matrix = np.cov(df_model[features].dropna().T)\n            inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Use pseudo-inverse for numerical stability\n\n            # Compute Mahalanobis distance\n            mahal_dist = mahalanobis(real_response, model_response, inv_cov_matrix)\n            mahalanobis_results_TM.append({\n                \"Model\": model_name,\n                \"Category\": category,\n                \"Mahalanobis Distance\": mahal_dist\n            })\n\ndf_LIWC_mahalanobis_TM = pd.DataFrame(mahalanobis_results_TM)\ndf_LIWC_mahalanobis_filtered_TM = df_LIWC_mahalanobis_TM[~df_LIWC_mahalanobis_TM['Category'].isin([\"Culture\", \"Lifestyle\", \"Physical\", \"States\"])]\n\nplt.figure(figsize=(14, 8))\nsns.boxplot(data = df_LIWC_mahalanobis_filtered_TM, \n            x=\"Category\", y=\"Mahalanobis Distance\", \n            hue=\"Model\", palette=\"coolwarm\", width=0.6)\nplt.title(\"Linguistic Feature Alignment of LLM Models: LIWC Mahalanobis Distance\", fontsize=18, fontweight='bold')\nplt.ylabel(\"Mahalanobis Distance\", fontsize=14)\nplt.xlabel(\"LIWC Categories\", fontsize=14)\nplt.xticks(rotation=45, ha='right', fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title=\"Model\", fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.savefig('/kaggle/working/models_similarity_LIWC_mahalanobis_Theresa May.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:19:09.731144Z","iopub.execute_input":"2025-03-03T14:19:09.731475Z","iopub.status.idle":"2025-03-03T14:19:27.569630Z","shell.execute_reply.started":"2025-03-03T14:19:09.731447Z","shell.execute_reply":"2025-03-03T14:19:27.568817Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null}]}