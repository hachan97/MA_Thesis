{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"87340102aee54b358cb522e41aa46e82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_922d18fb9bac440c84a480c86e170e62","IPY_MODEL_58269724075f4f49a77e10541117129a","IPY_MODEL_c635ea0c7af1420c9e19e99fac0d8911"],"layout":"IPY_MODEL_fd5e1e41a6f541ccb5b49100bc93dd99"}},"922d18fb9bac440c84a480c86e170e62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77483ad48d6d498fb18134f467bef8f4","placeholder":"​","style":"IPY_MODEL_8bab809e2b1e42ef91ea3e2a5dce5b6f","value":"Map: 100%"}},"58269724075f4f49a77e10541117129a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dcee8fe207847f293472bbece41da3b","max":2213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0c1d4eacad0483394791bcf48c2afd6","value":2213}},"c635ea0c7af1420c9e19e99fac0d8911":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50b7a8edf7034f69a0d469194a318eec","placeholder":"​","style":"IPY_MODEL_7900f333b9984334b3fbc3106383818f","value":" 2213/2213 [00:02&lt;00:00, 888.82 examples/s]"}},"fd5e1e41a6f541ccb5b49100bc93dd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77483ad48d6d498fb18134f467bef8f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bab809e2b1e42ef91ea3e2a5dce5b6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dcee8fe207847f293472bbece41da3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0c1d4eacad0483394791bcf48c2afd6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50b7a8edf7034f69a0d469194a318eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7900f333b9984334b3fbc3106383818f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85a2d7c5a3924e8cb051a3f5173e2af6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9f6335d155b1429daaa3c935b17adde7","IPY_MODEL_68f19a11fe814bcca26e835051900787","IPY_MODEL_94b7a7ecc67944efb5ddf7f01d77e382"],"layout":"IPY_MODEL_3ae974ba8adf4e1fbdf2e2a677385366"}},"9f6335d155b1429daaa3c935b17adde7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b062ec0b7a4347979308126a0eb49e9e","placeholder":"​","style":"IPY_MODEL_f103cbd0af934fe9be65457fe6129819","value":"Loading checkpoint shards: 100%"}},"68f19a11fe814bcca26e835051900787":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b61089229d94d7a81db3960e0fb825c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1049990845b74bb996724702b35fd652","value":2}},"94b7a7ecc67944efb5ddf7f01d77e382":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d04728683004307bde1e05ea7dbbaa0","placeholder":"​","style":"IPY_MODEL_1aef3e9062d54ea192ec6b582a7045b0","value":" 2/2 [00:32&lt;00:00, 14.62s/it]"}},"3ae974ba8adf4e1fbdf2e2a677385366":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b062ec0b7a4347979308126a0eb49e9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f103cbd0af934fe9be65457fe6129819":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b61089229d94d7a81db3960e0fb825c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1049990845b74bb996724702b35fd652":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d04728683004307bde1e05ea7dbbaa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aef3e9062d54ea192ec6b582a7045b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9961575,"sourceType":"datasetVersion","datasetId":6127385}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup Packages","metadata":{"id":"hIkR4TmZN3Yt"}},{"cell_type":"code","source":"!pip install transformers datasets torch huggingface_hub peft trl bitsandbytes","metadata":{"id":"c7o0Aea0NmaB","outputId":"3e2ddd54-de4b-4d9a-a15c-df722e0011ec","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:40:46.32373Z","iopub.execute_input":"2024-11-20T20:40:46.32405Z","iopub.status.idle":"2024-11-20T20:41:10.041709Z","shell.execute_reply.started":"2024-11-20T20:40:46.32402Z","shell.execute_reply":"2024-11-20T20:41:10.040762Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import libraries\n\n# Standard Python libraries\nimport pandas as pd\nfrom datasets import load_dataset, Dataset  # For loading datasets\nimport os\nimport torch\n\n# Hugging Face Transformers\nimport transformers\nfrom transformers import (\n    AutoTokenizer,            # For tokenizing text\n    AutoModelForCausalLM,     # For loading the GPT-2 model\n    Trainer,                  # For training the model\n    TrainingArguments,        # For specifying training arguments\n    logging,                  # For logging\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    pipeline,\n    DataCollatorWithPadding\n)\n\n# PyTorch\nimport torch  # For tensor operations and GPU support\n\n# For PEFT\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model  # For LoRA configuration and model\nfrom trl import SFTTrainer  # For supervised fine-tuning","metadata":{"id":"R46fDGCFN_ly","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:41:10.044026Z","iopub.execute_input":"2024-11-20T20:41:10.044852Z","iopub.status.idle":"2024-11-20T20:41:26.99302Z","shell.execute_reply.started":"2024-11-20T20:41:10.044805Z","shell.execute_reply":"2024-11-20T20:41:26.992345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"id":"pFIpZiSFo3hG","outputId":"a0ddc125-bdd8-4047-e2cd-10be6f90a7e2","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:27:26.181785Z","iopub.execute_input":"2024-11-20T20:27:26.182447Z","iopub.status.idle":"2024-11-20T20:27:26.187749Z","shell.execute_reply.started":"2024-11-20T20:27:26.182409Z","shell.execute_reply":"2024-11-20T20:27:26.186764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_token_here' with your Hugging Face token\nlogin(\"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\")","metadata":{"id":"rdFLz2PjYADL","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:45:51.776992Z","iopub.execute_input":"2024-11-20T20:45:51.777267Z","iopub.status.idle":"2024-11-20T20:45:52.458363Z","shell.execute_reply.started":"2024-11-20T20:45:51.777241Z","shell.execute_reply":"2024-11-20T20:45:52.456324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{"id":"icaFCRI0UOLy"}},{"cell_type":"code","source":"#from google.colab import files\n#uploaded = files.upload()","metadata":{"id":"4mwEDG-OUJ_a","outputId":"ddb9acf8-0375-411b-bc2f-bbbf2bbe3958","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:19:16.686846Z","iopub.execute_input":"2024-11-20T20:19:16.687199Z","iopub.status.idle":"2024-11-20T20:19:16.690974Z","shell.execute_reply.started":"2024-11-20T20:19:16.687167Z","shell.execute_reply":"2024-11-20T20:19:16.690022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Data\n#df_Boris_Johnson = pd.read_csv(\"df_Boris_Johnson_2001-19.csv\")\ndf_Boris_Johnson = pd.read_csv(\"/kaggle/input/df-boris-johnson-2001-19/df_Boris_Johnson_2001-19.csv\")","metadata":{"id":"YlRfVmATegW4","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:27:31.739902Z","iopub.execute_input":"2024-11-20T20:27:31.74083Z","iopub.status.idle":"2024-11-20T20:27:31.775692Z","shell.execute_reply.started":"2024-11-20T20:27:31.740773Z","shell.execute_reply":"2024-11-20T20:27:31.774703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert Pandas DataFrame to Hugging Face Dataset\ndf_Boris_Johnson_HF = Dataset.from_pandas(df_Boris_Johnson)","metadata":{"id":"AAm7QEyBg7WS","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:27:33.468599Z","iopub.execute_input":"2024-11-20T20:27:33.469312Z","iopub.status.idle":"2024-11-20T20:27:33.48706Z","shell.execute_reply.started":"2024-11-20T20:27:33.469274Z","shell.execute_reply":"2024-11-20T20:27:33.485947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenize Data\n\nDifferent models may require different preprocessing steps based on their *architecture*, *tokenizer type*, and *task*","metadata":{"id":"58ehESMeeIRA"}},{"cell_type":"code","source":"# Tokenize your dataset\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")    # Define the Tokenizer\ntokenizer.pad_token = tokenizer.eos_token                               # Set the padding token to the end-of-sequence token\n\ndef tokenize_function(examples):\n    tokenized_output = tokenizer(examples['text'],\n                                 truncation=True,\n                                 padding='max_length', max_length=512)\n    tokenized_output['labels'] = tokenized_output['input_ids'][:]\n\n    return tokenized_output\n\n# Use Hugging Face Dataset's map function to apply Tokenization\ntokenized_df_Boris_Johnson = df_Boris_Johnson_HF.map(tokenize_function, batched=True)","metadata":{"id":"IWjIJQ0se5PI","outputId":"a88953a5-9e6a-4b89-d425-d338edee9319","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:29:06.087596Z","iopub.execute_input":"2024-11-20T20:29:06.088618Z","iopub.status.idle":"2024-11-20T20:29:08.011493Z","shell.execute_reply.started":"2024-11-20T20:29:06.088575Z","shell.execute_reply":"2024-11-20T20:29:08.010304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preview tokenized dataset\n#tokenized_df_Boris_Johnson[1]","metadata":{"id":"BqudYuPaiPEf","outputId":"0b8aa2fa-0330-42ff-aa0d-d08d3d707cb8","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:19:19.889455Z","iopub.execute_input":"2024-11-20T20:19:19.889832Z","iopub.status.idle":"2024-11-20T20:19:19.91193Z","shell.execute_reply.started":"2024-11-20T20:19:19.88979Z","shell.execute_reply":"2024-11-20T20:19:19.911138Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Setup","metadata":{"id":"MxR8Gwq_RzBC"}},{"cell_type":"code","source":"# Optimize Performance with Configurations\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                      # Load model in 4bit, to redeuce memory and computational requirements\n    bnb_4bit_use_double_quant=True,         # Double quantization, further compress the model weights\n    bnb_4bit_quant_type=\"nf4\",              # Quantization type = nf4\n    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in 16bit format, to speed up computation\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"meta-llama/Llama-3.2-3B\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"  # Automatically assigns model to GPU if available\n)","metadata":{"id":"t5O7mYRNOXb6","outputId":"99c584fb-ea1e-444f-c661-aa2a22be38b0","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:25:50.852022Z","iopub.execute_input":"2024-11-20T20:25:50.852409Z","iopub.status.idle":"2024-11-20T20:25:50.922029Z","shell.execute_reply.started":"2024-11-20T20:25:50.852367Z","shell.execute_reply":"2024-11-20T20:25:50.92073Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply PEFT (Adapter, LoRA and others)\nmodel.gradient_checkpointing_enable()               # Reduce memory usage by saving intermediate activations\nmodel = prepare_model_for_kbit_training(model)      # Prepare model for kbit training to reduce memory usage","metadata":{"id":"49St8Yk2a0Gx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inspect Model Architecture\n\nThe attention mechanism in this model is implemented with **modular projections**, as opposed to a **combined module**: `query_key_value` .\nThe model uses distinct linear layers for the query (q_proj), key (k_proj), and value (v_proj) projections","metadata":{"id":"ej943P1ic5D4"}},{"cell_type":"code","source":"# Inspect Model Architecture\nprint(model)","metadata":{"id":"ixxwyp4Ucc6h","outputId":"660e1a47-c111-4411-ac7b-c9072f8ca8c5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define LoRA","metadata":{"id":"zd0ezAzCc8YR"}},{"cell_type":"code","source":"# Define LoRA configuration\nlora_config = LoraConfig(\n    r=8,                                  # Rank of the low-rank matrices, lower ranks -> lower computational load & memory usage\n    lora_alpha=32,                        # Scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # Specifies the modules that should be adapted using LoRA (*Depends on model architecture)\n    lora_dropout=0.1,                     # A Regularization technique used to prevent overfitting\n    bias=\"none\",                          # specifies that no additional bias terms should be added\n    task_type=\"CAUSAL_LM\"                 # Define the model: one that is 'predicting the next word'\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)","metadata":{"id":"suhZIIp7kaWn","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\nprint_trainable_parameters(model)","metadata":{"id":"BWB1QURMa9La","outputId":"0ee74809-aa4c-4ff9-fe8a-0691eae2f583","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define Training Parameters\nDefine training parameters, including batch size, learning rate, and the number of training epochs.","metadata":{"id":"dxJCSt_-jSGR"}},{"cell_type":"code","source":"# Set up Hyperparameters\ntraining_args = transformers.TrainingArguments(\n    output_dir=\"outputs\",\n    optim=\"paged_adamw_8bit\",\n    eval_strategy=\"no\",\n    #report_to=\"none\",                       # Disable WandB integration\n    per_device_train_batch_size=3,          # Adjust the batch size\n    gradient_accumulation_steps=4,          # Increaset gradient-steps to reduce memory usage\n    warmup_steps=2,                         # Helps to stabilize training\n    num_train_epochs=3,                     # Control duration of Training (use either 'max_steps' or 'num_train_epochs')\n    learning_rate=2e-5,\n    logging_steps=10,                       # Frequency of Training metrics logs for detailed feedback on process\n    weight_decay=0.01,\n\n    fp16=True,                              # Enable mixed precision training\n    gradient_checkpointing=True,            # Storing only a subset of activations\n)","metadata":{"id":"apUq0LYGmBJ2","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:43:17.779206Z","iopub.execute_input":"2024-11-20T20:43:17.779552Z","iopub.status.idle":"2024-11-20T20:43:17.816314Z","shell.execute_reply.started":"2024-11-20T20:43:17.779522Z","shell.execute_reply":"2024-11-20T20:43:17.815707Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args= training_args,                                 # input Training Arguments\n    train_dataset= tokenized_df_Boris_Johnson,           # input Tokenized Dataset\n    data_collator= transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),   # Format batches of data for training\n)","metadata":{"id":"cM0vXRznl7CO","trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:41:54.131819Z","iopub.execute_input":"2024-11-20T20:41:54.132138Z","iopub.status.idle":"2024-11-20T20:41:54.444318Z","shell.execute_reply.started":"2024-11-20T20:41:54.132109Z","shell.execute_reply":"2024-11-20T20:41:54.443109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune the Model","metadata":{"id":"JDekRjeKyCz9"}},{"cell_type":"code","source":"# Log in to W&B\nimport wandb\nwandb.login(key=\"e9febb58ac1779cc78d820e36fb9798142a0563b\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T20:46:01.924357Z","iopub.execute_input":"2024-11-20T20:46:01.924675Z","iopub.status.idle":"2024-11-20T20:46:04.432346Z","shell.execute_reply.started":"2024-11-20T20:46:01.924647Z","shell.execute_reply":"2024-11-20T20:46:04.4316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the model\nmodel.config.use_cache = False        # disable caching\ntrainer.train()","metadata":{"id":"EbVIQpWNoirt","outputId":"70c66134-e2dd-4ca5-9f95-a1f16ad00d17","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the Fine-Tuned Model\nmodel.save_pretrained(\"./fine-tuned-llama\")\ntokenizer.save_pretrained(\"./fine-tuned-llama\")","metadata":{"id":"-hnBov-CojD9","trusted":true},"outputs":[],"execution_count":null}]}