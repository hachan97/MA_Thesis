{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10882513,"sourceType":"datasetVersion","datasetId":6361952},{"sourceId":225066241,"sourceType":"kernelVersion"},{"sourceId":225067119,"sourceType":"kernelVersion"},{"sourceId":225070337,"sourceType":"kernelVersion"},{"sourceId":225070367,"sourceType":"kernelVersion"},{"sourceId":225112829,"sourceType":"kernelVersion"},{"sourceId":225112982,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport re\nimport math\nimport json\nimport random\nimport functools\nfrom datetime import datetime, timedelta\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nfrom collections import OrderedDict\n\n# Third-party imports\nimport torch\nimport openai\nimport faiss\nimport tenacity\n\n# LangChain imports\nfrom langchain.utils import mock_now\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.chains import LLMChain\nfrom langchain_core.language_models import BaseLanguageModel\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import HumanMessage, SystemMessage, BaseMemory, Document\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.output_parsers import RegexParser\n\n# Pydantic imports\nfrom pydantic import BaseModel, Field, ConfigDict\n\n# Hugging Face imports\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, pipeline, AutoModel\nfrom peft import PeftModel, PeftConfig\nfrom langchain_huggingface import HuggingFacePipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:00:08.491168Z","iopub.execute_input":"2025-03-02T13:00:08.491443Z","iopub.status.idle":"2025-03-02T13:00:16.611369Z","shell.execute_reply.started":"2025-03-02T13:00:08.491422Z","shell.execute_reply":"2025-03-02T13:00:16.610387Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n## Openai\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:01:27.947511Z","iopub.execute_input":"2025-03-02T13:01:27.948530Z","iopub.status.idle":"2025-03-02T13:01:28.146345Z","shell.execute_reply.started":"2025-03-02T13:01:27.948485Z","shell.execute_reply":"2025-03-02T13:01:28.145459Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Login to Hugging Face\nfrom huggingface_hub import login\n\nlogin(Hugging_Face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:01:32.012486Z","iopub.execute_input":"2025-03-02T13:01:32.012855Z","iopub.status.idle":"2025-03-02T13:01:32.155530Z","shell.execute_reply.started":"2025-03-02T13:01:32.012827Z","shell.execute_reply":"2025-03-02T13:01:32.154808Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"markdown","source":"## GPT 3.5","metadata":{}},{"cell_type":"code","source":"LLM_gpt = ChatOpenAI(model=\"gpt-3.5-turbo\", \n                 max_tokens=1500, \n                 api_key = OPENAI_API_KEY) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:01:33.331499Z","iopub.execute_input":"2025-03-02T13:01:33.331856Z","iopub.status.idle":"2025-03-02T13:01:33.468083Z","shell.execute_reply.started":"2025-03-02T13:01:33.331829Z","shell.execute_reply":"2025-03-02T13:01:33.467435Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"selected_embeddings_model = OpenAIEmbeddings(api_key = OPENAI_API_KEY)\nembedding_size_selectedLLM = len(selected_embeddings_model.embed_query(\"This is a test.\"))\nprint(f\"Embedding size: {embedding_size_selectedLLM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T13:01:34.306997Z","iopub.execute_input":"2025-03-02T13:01:34.307290Z","iopub.status.idle":"2025-03-02T13:01:34.990841Z","shell.execute_reply.started":"2025-03-02T13:01:34.307268Z","shell.execute_reply":"2025-03-02T13:01:34.990088Z"}},"outputs":[{"name":"stdout","text":"Embedding size: 1536\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Llama3.2 - Finetuned","metadata":{}},{"cell_type":"code","source":"# Define model paths\nBASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n\nFINETUNED_MODEL_PATH_ChristineJardine = \"/kaggle/input/finetune-llama-v3-christinejardine/kaggle/working/fine-tuned-llama_ChristineJardine\"\nFINETUNED_MODEL_PATH_TimFarron = \"/kaggle/input/finetune-llama-v3-timfarron/kaggle/working/fine-tuned-llama_TimFarron\"\nFINETUNED_MODEL_PATH_JoStevens = \"/kaggle/input/finetune-llama-v3-jostevens/kaggle/working/fine-tuned-llama_JoStevens\"\nFINETUNED_MODEL_PATH_SteveDouble = \"/kaggle/input/finetune-llama-v3-stevedouble/kaggle/working/fine-tuned-llama_SteveDouble\"\nFINETUNED_MODEL_PATH_RachaelMaskell = \"/kaggle/input/finetune-llama-v3-rachaelmaskell/kaggle/working/fine-tuned-llama_RachaelMaskell\"\nFINETUNED_MODEL_PATH_KitMalthouse = \"/kaggle/input/finetune-llama-v3-kitmalthouse/kaggle/working/fine-tuned-llama_KitMalthouse\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:22:11.215006Z","iopub.execute_input":"2025-03-02T16:22:11.215343Z","iopub.status.idle":"2025-03-02T16:22:11.219425Z","shell.execute_reply.started":"2025-03-02T16:22:11.215321Z","shell.execute_reply":"2025-03-02T16:22:11.218530Z"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:20:04.259928Z","iopub.execute_input":"2025-03-02T16:20:04.260214Z","iopub.status.idle":"2025-03-02T16:20:04.265596Z","shell.execute_reply.started":"2025-03-02T16:20:04.260194Z","shell.execute_reply":"2025-03-02T16:20:04.264740Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"class DebateLLM:\n    def __init__(self, base_model_id, lora_adapter_path, bnb_config):\n        # Load the tokenizer from the base model\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load the base model with quantization configuration\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\"\n        )\n        \n        # Load the LoRA adapter weights onto the base model\n        self.model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n        self.model.eval()\n\ndebate_LLM_ChristineJardine = DebateLLM(BASE_MODEL_ID, FINETUNED_MODEL_PATH_ChristineJardine, bnb_config)\ndebate_LLM_TimFarron        = DebateLLM(BASE_MODEL_ID, FINETUNED_MODEL_PATH_TimFarron, bnb_config)\ndebate_LLM_JoStevens        = DebateLLM(BASE_MODEL_ID, FINETUNED_MODEL_PATH_JoStevens, bnb_config)\ndebate_LLM_SteveDouble      = DebateLLM(BASE_MODEL_ID, FINETUNED_MODEL_PATH_SteveDouble, bnb_config)\ndebate_LLM_RachaelMaskell   = DebateLLM(BASE_MODEL_ID, FINETUNED_MODEL_PATH_RachaelMaskell, bnb_config)\ndebate_LLM_KitMalthouse     = DebateLLM(BASE_MODEL_ID, FINETUNED_MODEL_PATH_KitMalthouse, bnb_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:22:15.743229Z","iopub.execute_input":"2025-03-02T16:22:15.743540Z","iopub.status.idle":"2025-03-02T16:23:05.551212Z","shell.execute_reply.started":"2025-03-02T16:22:15.743517Z","shell.execute_reply":"2025-03-02T16:23:05.550475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eaf223c69a74e93a3f6d9a6d583ea97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ae80b80b374baeabae9c6879c8f57a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66707ec46cb4f2480b07e653f71a911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b76967bbb2e44509570ce25087c20b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e094b0730d74707a3224db318f1339a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81532b8eb9f47838c02322f5c7ced52"}},"metadata":{}}],"execution_count":126},{"cell_type":"markdown","source":"# Generative AI Setup\nThe [codes](https://python.langchain.com/api_reference/experimental/generative_agents.html) for the classes `GenerativeAgentMemory` and `GenerativeAgent` was entirely reused from the **[LangChain Experimental](https://pypi.org/project/langchain-experimental/)** project in the LangChain Python API reference - intended for research and experimental uses, with a few minor tweaks and proper configuration of the prompts.\n","metadata":{}},{"cell_type":"markdown","source":"## Generative Agent Memory","metadata":{}},{"cell_type":"code","source":"class GenerativeAgentMemory(BaseMemory):\n    \"\"\"Memory for the generative agent.\"\"\"\n    \n    llm: BaseLanguageModel\n    \"\"\"The core language model.\"\"\"\n    \n    memory_retriever: TimeWeightedVectorStoreRetriever\n    \"\"\"The retriever to fetch related memories.\"\"\"\n    \n    verbose: bool = False\n    reflection_threshold: Optional[float] = None\n    \"\"\"When aggregate_importance exceeds reflection_threshold, stop to reflect.\"\"\"\n    \n    current_plan: List[str] = []\n    \"\"\"The current plan of the agent.\"\"\"\n    \n    # A weight of 0.15 makes this less important than it\n    # would be otherwise, relative to salience and time\n    importance_weight: float = 0.15\n    \n    \"\"\"How much weight to assign the memory importance.\"\"\"\n    aggregate_importance: float = 0.0  # : :meta private:\n    \n    \"\"\"Track the sum of the 'importance' of recent memories.\n    Triggers reflection when it reaches reflection_threshold.\"\"\"\n    max_tokens_limit: int = 2000  # : :meta private:\n    \n    # input keys\n    queries_key: str = \"queries\"\n    most_recent_memories_token_key: str = \"recent_memories_token\"\n    add_memory_key: str = \"add_memory\"\n    \n    # output keys\n    relevant_memories_key: str = \"relevant_memories\"\n    relevant_memories_simple_key: str = \"relevant_memories_simple\"\n    most_recent_memories_key: str = \"most_recent_memories\"\n    now_key: str = \"now\"\n    reflecting: bool = False\n    \n    def chain(self, prompt: PromptTemplate) -> LLMChain:\n        return LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n    @staticmethod\n    \n    def _parse_list(text: str) -> List[str]:\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n        lines = re.split(r\"\\n\", text.strip())\n        lines = [line for line in lines if line.strip()]  # remove empty lines\n        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n    \n    def _get_topics_of_reflection(self, last_k: int = 50) -> List[str]:\n        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"{observations}\\n\\n\"\n            \"Given only the information above, what are the 3 most salient \"\n            \"high-level questions we can answer about the subjects in the statements?\\n\"\n            \"Provide each question on a new line.\"\n        )\n        observations = self.memory_retriever.memory_stream[-last_k:]\n        observation_str = \"\\n\".join(\n            [self._format_memory_detail(o) for o in observations]\n        )\n        result = self.chain(prompt).run(observations=observation_str)\n        return self._parse_list(result)\n    \n    def _get_insights_on_topic(\n        self, topic: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Generate 'insights' on a topic of reflection, based on pertinent memories.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"Statements relevant to: '{topic}'\\n\"\n            \"---\\n\"\n            \"{related_statements}\\n\"\n            \"---\\n\"\n            \"What 5 high-level novel insights can you infer from the above statements \"\n            \"that are relevant for answering the following question?\\n\"\n            \"Do not include any insights that are not relevant to the question.\\n\"\n            \"Do not repeat any insights that have already been made.\\n\\n\"\n            \"Question: {topic}\\n\\n\"\n            \"(example format: insight (because of 1, 5, 3))\\n\"\n        )\n        related_memories = self.fetch_memories(topic, now=now)\n        related_statements = \"\\n\".join(\n            [\n                self._format_memory_detail(memory, prefix=f\"{i+1}. \")\n                for i, memory in enumerate(related_memories)\n            ]\n        )\n        result = self.chain(prompt).run(\n            topic=topic, related_statements=related_statements\n        )\n        # TODO: Parse the connections between memories and insights\n        return self._parse_list(result)\n    \n    def pause_to_reflect(self, now: Optional[datetime] = None) -> List[str]:\n        \"\"\"Reflect on recent observations and generate 'insights'.\"\"\"\n        if self.verbose:\n            logger.info(\"Character is reflecting\")\n        new_insights = []\n        topics = self._get_topics_of_reflection()\n        for topic in topics:\n            insights = self._get_insights_on_topic(topic, now=now)\n            for insight in insights:\n                self.add_memory(insight, now=now)\n            new_insights.extend(insights)\n        return new_insights\n    \n    def _score_memory_importance(self, memory_content: str) -> float:\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"On the scale of 1 to 10, where 1 is purely mundane\"\n            + \" (e.g., brushing teeth, making bed) and 10 is\"\n            + \" extremely poignant (e.g., a break up, college\"\n            + \" acceptance), rate the likely poignancy of the\"\n            + \" following piece of memory. Respond with a single integer.\"\n            + \"\\nMemory: {memory_content}\"\n            + \"\\nRating: \"\n        )\n        score = self.chain(prompt).run(memory_content=memory_content).strip()\n        if self.verbose:\n            logger.info(f\"Importance score: {score}\")\n        match = re.search(r\"^\\D*(\\d+)\", score)\n        if match:\n            return (float(match.group(1)) / 10) * self.importance_weight\n        else:\n            return 0.0\n    \n    def _score_memories_importance(self, memory_content: str) -> List[float]:\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"On the scale of 1 to 10, where 1 is purely mundane\"\n            + \" (e.g., brushing teeth, making bed) and 10 is\"\n            + \" extremely poignant (e.g., a break up, college\"\n            + \" acceptance), rate the likely poignancy of the\"\n            + \" following piece of memory. Always answer with only a list of numbers.\"\n            + \" If just given one memory still respond in a list.\"\n            + \" Memories are separated by semi colans (;)\"\n            + \"\\nMemories: {memory_content}\"\n            + \"\\nRating: \"\n        )\n        scores = self.chain(prompt).run(memory_content=memory_content).strip()\n        if self.verbose:\n            logger.info(f\"Importance scores: {scores}\")\n        # Split into list of strings and convert to floats\n        scores_list = [float(x) for x in scores.split(\";\")]\n        return scores_list\n    \n    def add_memories(\n        self, memory_content: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Add an observations or memories to the agent's memory.\"\"\"\n        importance_scores = self._score_memories_importance(memory_content)\n        self.aggregate_importance += max(importance_scores)\n        memory_list = memory_content.split(\";\")\n        documents = []\n        for i in range(len(memory_list)):\n            documents.append(\n                Document(\n                    page_content=memory_list[i],\n                    metadata={\"importance\": importance_scores[i]},\n                )\n            )\n        result = self.memory_retriever.add_documents(documents, current_time=now)\n        # After an agent has processed a certain amount of memories (as measured by\n        # aggregate importance), it is time to reflect on recent events to add\n        # more synthesized memories to the agent's memory stream.\n        if (\n            self.reflection_threshold is not None\n            and self.aggregate_importance > self.reflection_threshold\n            and not self.reflecting\n        ):\n            self.reflecting = True\n            self.pause_to_reflect(now=now)\n            # Hack to clear the importance from reflection\n            self.aggregate_importance = 0.0\n            self.reflecting = False\n        return result\n    \n    def add_memory(\n        self, memory_content: str, now: Optional[datetime] = None) -> List[str]:\n        \"\"\"Add an observation or memory to the agent's memory.\"\"\"\n        \n        importance_score = self._score_memory_importance(memory_content)\n        \n        self.aggregate_importance += importance_score\n        \n        document = Document(page_content=memory_content, \n                            metadata={\"importance\": importance_score} )\n        \n        result = self.memory_retriever.add_documents([document], current_time=now)\n        \n        # After an agent has processed a certain amount of memories (as measured by\n        # aggregate importance), it is time to reflect on recent events to add\n        # more synthesized memories to the agent's memory stream.\n        if (\n            self.reflection_threshold is not None\n            and self.aggregate_importance > self.reflection_threshold\n            and not self.reflecting\n        ):\n            self.reflecting = True\n            self.pause_to_reflect(now=now)\n            # Hack to clear the importance from reflection\n            self.aggregate_importance = 0.0\n            self.reflecting = False\n            \n        return result\n    \n    def fetch_memories(\n        self, observation: str, now: Optional[datetime] = None\n    ) -> List[Document]:\n        \"\"\"Fetch related memories.\"\"\"\n        if now is not None:\n            with mock_now(now):\n                return self.memory_retriever.invoke(observation)\n        else:\n            return self.memory_retriever.invoke(observation)\n    \n    def format_memories_detail(self, relevant_memories: List[Document]) -> str:\n        content = []\n        for mem in relevant_memories:\n            content.append(self._format_memory_detail(mem, prefix=\"- \"))\n        return \"\\n\".join([f\"{mem}\" for mem in content])\n    \n    def _format_memory_detail(self, memory: Document, prefix: str = \"\") -> str:\n        created_time = memory.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\n        return f\"{prefix}[{created_time}] {memory.page_content.strip()}\"\n    \n    def format_memories_simple(self, relevant_memories: List[Document]) -> str:\n        return \"; \".join([f\"{mem.page_content}\" for mem in relevant_memories])\n    \n    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\n        \"\"\"Reduce the number of tokens in the documents.\"\"\"\n        result = []\n        for doc in self.memory_retriever.memory_stream[::-1]:\n            if consumed_tokens >= self.max_tokens_limit:\n                break\n            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\n            if consumed_tokens < self.max_tokens_limit:\n                result.append(doc)\n        return self.format_memories_simple(result)\n    \n    def memory_variables(self) -> List[str]:\n        \"\"\"Input keys this memory class will load dynamically.\"\"\"\n        return []\n   \n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        queries = inputs.get(self.queries_key)\n        now = inputs.get(self.now_key)\n        if queries is not None:\n            relevant_memories = [\n                mem for query in queries for mem in self.fetch_memories(query, now=now)\n            ]\n            return {\n                self.relevant_memories_key: self.format_memories_detail(\n                    relevant_memories\n                ),\n                self.relevant_memories_simple_key: self.format_memories_simple(\n                    relevant_memories\n                ),\n            }\n        most_recent_memories_token = inputs.get(self.most_recent_memories_token_key)\n        if most_recent_memories_token is not None:\n            return {\n                self.most_recent_memories_key: self._get_memories_until_limit(\n                    most_recent_memories_token\n                )\n            }\n        return {}\n    \n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        # TODO: fix the save memory key\n        mem = outputs.get(self.add_memory_key)\n        now = outputs.get(self.now_key)\n        if mem:\n            self.add_memory(mem, now=now)\n    \n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        # TODO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:13:42.078601Z","iopub.execute_input":"2025-03-02T16:13:42.078924Z","iopub.status.idle":"2025-03-02T16:13:42.104593Z","shell.execute_reply.started":"2025-03-02T16:13:42.078901Z","shell.execute_reply":"2025-03-02T16:13:42.103714Z"}},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":"## Generative Agent","metadata":{}},{"cell_type":"code","source":"class GenerativeAgent(BaseModel):\n    \"\"\"Agent as a character with memory and innate characteristics.\"\"\"\n    name: str\n    \"\"\"The character's name.\"\"\"\n    \n    age: Optional[int] = None\n    \"\"\"The optional age of the character.\"\"\"\n    \n    traits: str = \"N/A\"\n    \"\"\"Permanent traits to ascribe to the character.\"\"\"\n    \n    status: str\n    \"\"\"The traits of the character you wish not to change.\"\"\"\n    \n    memory: GenerativeAgentMemory\n    \"\"\"The memory object that combines relevance, recency, and 'importance'.\"\"\"\n    \n    llm: BaseLanguageModel\n    llm_debate: Any\n    \"\"\"The underlying language model.\"\"\"\n\n    \n    verbose: bool = False\n    summary: str = \"\"  #: :meta private:\n    \"\"\"Stateful self-summary generated via reflection on the character's memory.\"\"\"\n    \n    summary_refresh_seconds: int = 3600  #: :meta private:\n    \"\"\"How frequently to re-generate the summary.\"\"\"\n    \n    last_refreshed: datetime = Field(default_factory=datetime.now)  # : :meta private:\n    \"\"\"The last time the character's summary was regenerated.\"\"\"\n    \n    daily_summaries: List[str] = Field(default_factory=list)  # : :meta private:\n    \"\"\"Summary of the events in the plan that the agent took.\"\"\"\n    \n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    \n## LLM-related methods\n    def _parse_list(text: str) -> List[str]:\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n        lines = re.split(r\"\\n\", text.strip())\n        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n        \n    def chain(self, \n              prompt: PromptTemplate, \n              llm: Optional[BaseLanguageModel] = None) -> LLMChain:\n        \"\"\"Create a chain with the same settings as the agent.\"\"\"\n        \n        return LLMChain(\n            llm= llm or self.llm,       # If llm is None, it will use self.llm\n            prompt=prompt, \n            verbose=self.verbose, \n            memory=self.memory\n        )\n        \n    def _get_entity_from_observation(self, observation: str) -> str:\n        prompt = PromptTemplate.from_template(\n            \"What is the observed entity in the following observation? {observation}\"\n            + \"\\nEntity=\"\n        )\n        return self.chain(prompt).run(observation=observation).strip()\n        \n    def _get_entity_action(self, observation: str, entity_name: str) -> str:\n        prompt = PromptTemplate.from_template(\n            \"What is the {entity} doing in the following observation? {observation}\"\n            + \"\\nThe {entity} is\"\n        )\n        return (\n            self.chain(prompt).run(entity=entity_name, observation=observation).strip()\n        )\n\n## Summarize Most relevant memories\n    def summarize_related_memories(self, observation: str) -> str:\n        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"\"\"\n            {q1}?\n            Context from memory:\n            {relevant_memories}\n            Relevant context: \n            \"\"\"\n        )\n        entity_name = self._get_entity_from_observation(observation)\n        entity_action = self._get_entity_action(observation, entity_name)\n        q1 = f\"What is the relationship between {self.name} and {entity_name}\"\n        q2 = f\"{entity_name} is {entity_action}\"\n        return self.chain(prompt=prompt).run(q1=q1, queries=[q1, q2]).strip()\n        \n## Generate Summary of the agent + reaction \n    def _generate_reaction(\n        self, \n        observation: str, \n        suffix: str, \n        now: Optional[datetime] = None,\n        llm: Optional[Any] = None,  # Expecting an object with 'model' and 'tokenizer'\n    ) -> str:\n\n        # Gather context information\n        agent_summary_description = self.get_summary(now=now)\n        relevant_memories_str = self.summarize_related_memories(observation)\n        current_time_str = (\n            datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n            if now is None\n            else now.strftime(\"%B %d, %Y, %I:%M %p\")\n        )\n        \n        # Construct the system message (context)\n        system_content = (\n            f\"{agent_summary_description}\\n\"\n            f\"It is {current_time_str}.\\n\"\n            f\"{self.name}'s status: {self.status}\\n\"\n            f\"Summary of relevant context from {self.name}'s memory:\\n{relevant_memories_str}\\n\"\n            f\"\\n\\n\"\n            f\"{suffix}\"\n        )\n\n        # Build messages in chat format\n        messages = [\n            {\"role\": \"system\", \"content\": system_content},\n            {\"role\": \"user\", \"content\": observation},\n        ]\n        \n        # Build the prompt using the tokenizer's chat template function.\n        prompt = llm.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        \n        # Tokenize the prompt manually\n        inputs = llm.tokenizer(prompt, return_tensors=\"pt\", \n                               padding=True, truncation=True).to(\"cuda\")\n        \n        # Generate output using model.generate; adjust generation parameters as needed.\n        outputs = llm.model.generate(**inputs, max_new_tokens=300, num_return_sequences=1)\n        \n        # Decode the generated text\n        generated_text = llm.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        generated_text = generated_text.replace('\\n', ' ')\n\n        # Clean the output response\n        match = re.search(r\"assistant\\s*(.*)\", generated_text, re.IGNORECASE | re.DOTALL)\n        if match:\n            return match.group(1).strip()\n        else:\n            return generated_text.strip()\n    \n## Generate Dialogue response\n    def generate_dialogue_response(self, observation: str, now: Optional[datetime] = None) -> str:\n\n        call_to_action_template = (\n            f\"You are {self.name}, a Member of Parliament responding in a debate session in the UK House of Commons.\\n\"\n            f\"Act as {self.name} would, using your distinct voice and perspective.\\n\"\n            f\"Respond to the input.\\n\"\n            f\"Ensure that your response is direct, fully in character, and reflects your established views and tone.\\n\"\n            f\"Respond exactly as {self.name} would speak in this context.\"\n        )\n        \n        response_text = self._generate_reaction(observation = observation,\n                                                suffix = call_to_action_template,\n                                                now=now, llm=self.llm_debate)\n        \n        # Save the dialogue turn to memory\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"{self.name} observed {observation} and said {response_text}\",\n                self.memory.now_key: now,\n            },\n        )\n        return f\"{self.name} said {response_text}\"\n\n## Decide if the agent wants to respond to the observation\n    def decide_to_respond(self, observation: str, \n                          now: Optional[datetime] = None,\n                          threshold: float = 7.0) -> bool:\n\n        call_to_action_template = (\n            f\"You are {self.name}, a Member of Parliament (MP) currently sitting in the UK House of Commons.\"\n            f\"On a scale of 1 to 10, how salient is the following statement to you as an MP?\"\n            f\"\\n- 1: Not relevant at all\"\n            f\"\\n- 10: Extremely relevant\"\n            f\"\\n\\nRespond ONLY with a single integer between 1 and 10!\"\n            )\n        \n        full_result = self._generate_reaction(observation = observation,\n                                              suffix = call_to_action_template,\n                                              now=now, llm = self.llm_debate)\n\n        match = re.search(r'\\d+', full_result) # Searches for the first occurrence of a 'digit'\n        if match:\n            relevance_score = int(match.group())\n        else:\n            print(f\"Unexpected non-numeric response from agent: {full_result}\") \n            relevance_score = 0\n\n        # Save the decision context to memory\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"In assessing the observation: '{observation}', \"\n                f\"{self.name} rated its relevance as {relevance_score} on a scale of 1 to 10, where 1 is 'not relevant at all' and 10 is 'extremely relevant'\",\n                self.memory.now_key: now,\n            },\n        )\n         \n        # Return True if the relevance score is equal or above the threshold, else False\n        return relevance_score >= threshold\n    \n    ######################################################\n    # Agent stateful' summary methods.                   #\n    # Each dialog or response prompt includes a header   #\n    # summarizing the agent's self-description. This is  #\n    # updated periodically through probing its memories  #\n    ######################################################\n    \n    def _compute_agent_summary(self) -> str:\n        \"\"\"\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"How would you summarize {name}'s core characteristics given the\"\n            + \" following statements:\\n\"\n            + \"{relevant_memories}\"\n            + \"Do not embellish.\"\n            + \"\\n\\nSummary: \"\n        )\n        # The agent seeks to think about their core characteristics.\n        return (\n            self.chain(prompt)\n            .run(name=self.name, queries=[f\"{self.name}'s core characteristics\"])\n            .strip()\n        )\n\n## Get Summary of the agent\n    def get_summary(\n        self, force_refresh: bool = False, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"Return a descriptive summary of the agent.\"\"\"\n        current_time = datetime.now() if now is None else now\n        since_refresh = (current_time - self.last_refreshed).seconds\n        if (\n            not self.summary\n            or since_refresh >= self.summary_refresh_seconds\n            or force_refresh\n        ):\n            self.summary = self._compute_agent_summary()\n            self.last_refreshed = current_time\n        age = self.age if self.age is not None else \"N/A\"\n        return (\n            f\"Name: {self.name} (age: {age})\"\n            + f\"\\nInnate traits: {self.traits}\"\n            + f\"\\n{self.summary}\"\n        )\n    \n    def get_full_header(\n        self, force_refresh: bool = False, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"Return a full header of the agent's status, summary, and current time.\"\"\"\n        now = datetime.now() if now is None else now\n        summary = self.get_summary(force_refresh=force_refresh, now=now)\n        current_time_str = now.strftime(\"%B %d, %Y, %I:%M %p\")\n        return (\n            f\"{summary}\\nIt is {current_time_str}.\\n{self.name}'s status: {self.status}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:14:43.355683Z","iopub.execute_input":"2025-03-02T16:14:43.355982Z","iopub.status.idle":"2025-03-02T16:14:43.377349Z","shell.execute_reply.started":"2025-03-02T16:14:43.355961Z","shell.execute_reply":"2025-03-02T16:14:43.376512Z"}},"outputs":[],"execution_count":116},{"cell_type":"markdown","source":"# Create Agent\n- [GenerativeAgentMemory](https://python.langchain.com/api_reference/experimental/generative_agents/langchain_experimental.generative_agents.memory.GenerativeAgentMemory.html): **Memory** for the generative agent \n   - `llm`\n   - `memory_retriever` = create_new_memory_retriever()\n   - `current_plan`\n   - `reflection_threshold`\n   - `add_memory` add observation/memory\n- [GenerativeAgent](https://python.langchain.com/api_reference/experimental/generative_agents.html): Agent as a character with **memory** and innate **characteristics**,  \n   - basics like `name`, `age` and `llm`\n   - `memory` object that combines relevance, recency, and ‘importance’\n   - `summary` and `summary_refresh_seconds` to set how frequently to re-generate the summary\n   - `summarize_related_memories`: Summarize memories that are most relevant to an observation\n   - `status` fix-objectives / traits of the character you wish not to change\n   - `traits` set Permanent traits to ascribe to the character \n   - `generate_dialogue_response`","metadata":{}},{"cell_type":"code","source":"# Relevance Score function - relevance_score_fn()\ndef relevance_score_fn(score: float) -> float:\n    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n    return 1.0 - score / math.sqrt(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:23:31.535397Z","iopub.execute_input":"2025-03-02T16:23:31.535793Z","iopub.status.idle":"2025-03-02T16:23:31.539916Z","shell.execute_reply.started":"2025-03-02T16:23:31.535759Z","shell.execute_reply":"2025-03-02T16:23:31.539091Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"# Memory Retriever function - create_new_memory_retriever()\ndef create_new_memory_retriever():\n    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n    \n    embeddings_model = selected_embeddings_model  \n    \n    # Initialize the vectorstore as empty\n    embedding_size = embedding_size_selectedLLM           #use: 1536 (GPT3.5) or 3072 (Llamma)\n    \n    index = faiss.IndexFlatL2(embedding_size)\n    vectorstore = FAISS(\n        embeddings_model.embed_query,  \n        index,\n        InMemoryDocstore({}),  # empty Memory docstore\n        {},  # index-to-document store ID mapping\n        relevance_score_fn=relevance_score_fn,\n    )\n    \n    # Time-weighted scoring mechanism\n    return TimeWeightedVectorStoreRetriever(\n        vectorstore=vectorstore,\n        other_score_keys=[\"importance\"],\n        k=5                                   # retrieve up to ___ relevant memories\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:23:32.515203Z","iopub.execute_input":"2025-03-02T16:23:32.515484Z","iopub.status.idle":"2025-03-02T16:23:32.520329Z","shell.execute_reply.started":"2025-03-02T16:23:32.515463Z","shell.execute_reply":"2025-03-02T16:23:32.519391Z"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":"# Agent Creation function - create_debate_agent()\ndef create_debate_agent(name, age, traits, status, \n                        #reflection_threshold, \n                        llm, llm_debate):\n   \n    memory = GenerativeAgentMemory(\n        llm=llm,\n        memory_retriever=create_new_memory_retriever(),\n        verbose=False,\n        #reflection_threshold=reflection_threshold,  # adjust as needed for reflection frequency\n    )\n    \n    agent = GenerativeAgent(\n        name=name,\n        age=age,\n        traits=traits,\n        status=status,\n        memory_retriever=create_new_memory_retriever(),\n        llm=llm,\n        llm_debate = llm_debate,\n        memory=memory,\n    )\n    return agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:23:33.860278Z","iopub.execute_input":"2025-03-02T16:23:33.860579Z","iopub.status.idle":"2025-03-02T16:23:33.865619Z","shell.execute_reply.started":"2025-03-02T16:23:33.860556Z","shell.execute_reply":"2025-03-02T16:23:33.864616Z"}},"outputs":[],"execution_count":129},{"cell_type":"markdown","source":"## Define Agent Traits","metadata":{}},{"cell_type":"code","source":"# Create agents for each MP\nChristineJardine = create_debate_agent(\n    name=\"Christine Jardine\",\n    age=2019-1960,\n    traits=\"Advocate for EU citizens' rights, empathetic communicator\",\n    status=\"Scottish Liberal Democrat politician; Opposes ending free movement; highlights contributions of EU nationals to the UK.\",\n    llm=LLM_gpt, llm_debate = debate_LLM_ChristineJardine\n)\n\nKitMalthouse = create_debate_agent(\n    name=\"Kit Malthouse\",\n    age=2019-1966,\n    traits=\"Firm on immigration control, prioritizes national sovereignty\",\n    status=\"British Conservative Party politician; Supports ending free movement; focuses on controlled immigration policies.\",\n    llm=LLM_gpt, llm_debate = debate_LLM_KitMalthouse\n)\n\nSteveDouble = create_debate_agent(\n    name=\"Steve Double\",\n    age=2019-1966,  # Example age\n    traits=\"Emphasizes national interests, supportive of government policies\",\n    status=\"British Conservative Party politician; Advocates for ending free movement; reassures EU citizens are welcome under new schemes.\",\n    llm=LLM_gpt, llm_debate = debate_LLM_SteveDouble\n)\n\nTimFarron = create_debate_agent(\n    name=\"Tim Farron\",\n    age=2019-1970,  # Example age\n    traits=\"Pro-European, champions individual rights\",\n    status=\"British Liberal Democrats Party politician; Opposes ending free movement; stresses benefits of EU integration.\",\n    llm=LLM_gpt, llm_debate = debate_LLM_TimFarron\n)\n\nJoStevens = create_debate_agent(\n    name=\"Jo Stevens\",\n    age=2019-1966,  # Example age\n    traits=\"Defender of workers' rights, critical of government policies\",\n    status=\"British Labour Party politician; Questions impact of ending free movement on labor markets and rights.\",\n    llm=LLM_gpt, llm_debate = debate_LLM_JoStevens\n)\n\nRachaelMaskell = create_debate_agent(\n    name=\"Rachael Maskell\",\n    age=2019-1972,  # Example age\n    traits=\"Advocate for social justice, focuses on community welfare\",\n    status=\"British Labour and Co-operative Party politician; Concerns about social implications of ending free movement; emphasizes inclusive policies.\",\n    llm=LLM_gpt, llm_debate = debate_LLM_RachaelMaskell\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:24:27.788731Z","iopub.execute_input":"2025-03-02T16:24:27.789057Z","iopub.status.idle":"2025-03-02T16:24:27.796453Z","shell.execute_reply.started":"2025-03-02T16:24:27.789031Z","shell.execute_reply":"2025-03-02T16:24:27.795472Z"}},"outputs":[],"execution_count":131},{"cell_type":"markdown","source":"## Define Base Memories","metadata":{}},{"cell_type":"code","source":"# Creat Memory objects for each agent\nChristineJardine_memory = ChristineJardine.memory\nKitMalthouse_memory = KitMalthouse.memory\nSteveDouble_memory = SteveDouble.memory   \nTimFarron_memory = TimFarron.memory\nJoStevens_memory = JoStevens.memory\nRachaelMaskell_memory = RachaelMaskell.memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:24:35.247738Z","iopub.execute_input":"2025-03-02T16:24:35.248085Z","iopub.status.idle":"2025-03-02T16:24:35.252565Z","shell.execute_reply.started":"2025-03-02T16:24:35.248058Z","shell.execute_reply":"2025-03-02T16:24:35.251592Z"}},"outputs":[],"execution_count":132},{"cell_type":"code","source":"# Base Observations \nChristineJardine_observations = [\n    \"Christine Jardine is a Liberal Democrat MP representing Edinburgh West.\",\n    \"She has expressed concerns about ending free movement, highlighting its potential negative impact on public services and the economy.\",\n    \"Jardine emphasizes the human cost of ending free movement, noting that many EU nationals in her constituency feel unwelcome and uncertain about their future.\"\n]\n\n# Base Observations for Kit Malthouse\nKitMalthouse_observations = [\n    \"Kit Malthouse is a Conservative MP who has served as the Minister for Crime, Policing and the Fire Service.\",\n    \"During debates, he has stated that the government is committed to ending free movement as part of the Brexit process.\",\n    \"Malthouse has reassured that EU citizens residing in the UK are welcome to stay and that the government has implemented the EU Settlement Scheme to facilitate their continued residence.\"\n]\n\nSteveDouble_observations = [\n    \"Steve Double is a Conservative MP representing St Austell and Newquay.\",\n    \"He has expressed support for ending free movement, aligning with the government's stance on controlling immigration post-Brexit.\",\n    \"Double has emphasized that the government has provided a clear message that EU citizens currently residing in the UK are welcome to stay, citing the success of the EU Settlement Scheme.\"\n]\n\nTimFarron_observations = [\n    \"Tim Farron is a Liberal Democrat MP representing Westmorland and Lonsdale.\",\n    \"He has criticized the decision to end free movement, arguing that it sends a negative message to EU citizens and undermines the UK's international standing.\",\n    \"Farron has highlighted the contributions of EU nationals to the UK and has advocated for their rights to be protected post-Brexit.\"\n]\n\nJoStevens_observations = [\n    \"Jo Stevens is a Labour MP representing Cardiff Central.\",\n    \"She has raised concerns about the impact of ending free movement on universities, noting that it could harm the UK's global reputation and deter international students and academics.\",\n    \"Stevens has advocated for the protection of EU nationals' rights and has questioned the government's approach to immigration post-Brexit.\"\n]\n\nRachaelMaskell_observations = [\n    \"Rachael Maskell is a Labour/Co-operative MP representing York Central.\",\n    \"She has expressed concerns about the government's immigration policies post-Brexit, particularly regarding family reunification and the rights of unaccompanied minors.\",\n    \"Maskell has questioned the government's preparedness for the consequences of ending free movement and has called for more compassionate immigration policies.\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:24:46.596280Z","iopub.execute_input":"2025-03-02T16:24:46.596622Z","iopub.status.idle":"2025-03-02T16:24:46.601514Z","shell.execute_reply.started":"2025-03-02T16:24:46.596594Z","shell.execute_reply":"2025-03-02T16:24:46.600662Z"}},"outputs":[],"execution_count":133},{"cell_type":"code","source":"# Loop through the observations and add to memory\ntuples = [(ChristineJardine_observations, ChristineJardine.memory), \n          (KitMalthouse_observations, KitMalthouse.memory), \n          (SteveDouble_observations, SteveDouble.memory), \n          (TimFarron_observations, TimFarron.memory), \n          (JoStevens_observations, JoStevens.memory), \n          (RachaelMaskell_observations, RachaelMaskell.memory)]\n\nfor observations, memory in tuples:\n    for observation in observations:\n        memory.add_memory(observation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:25:05.277188Z","iopub.execute_input":"2025-03-02T16:25:05.277480Z","iopub.status.idle":"2025-03-02T16:25:15.828754Z","shell.execute_reply.started":"2025-03-02T16:25:05.277458Z","shell.execute_reply":"2025-03-02T16:25:15.828093Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"# View stored memories for each MP\nprint(\"Christine Jardine's stored memories:\")\nprint(ChristineJardine_memory.memory_retriever.memory_stream)\n\nprint(\"\\nKit Malthouse's stored memories:\")\nprint(KitMalthouse_memory.memory_retriever.memory_stream)\n\nprint(\"\\nSteve Double's stored memories:\")\nprint(SteveDouble_memory.memory_retriever.memory_stream)\n\nprint(\"\\nTim Farron's stored memories:\")\nprint(TimFarron_memory.memory_retriever.memory_stream)\n\nprint(\"\\nJo Stevens's stored memories:\")\nprint(JoStevens_memory.memory_retriever.memory_stream)\n\nprint(\"\\nRachael Maskell's stored memories:\")\nprint(RachaelMaskell_memory.memory_retriever.memory_stream)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:25:18.855974Z","iopub.execute_input":"2025-03-02T16:25:18.856368Z","iopub.status.idle":"2025-03-02T16:25:18.869521Z","shell.execute_reply.started":"2025-03-02T16:25:18.856322Z","shell.execute_reply":"2025-03-02T16:25:18.868657Z"}},"outputs":[{"name":"stdout","text":"Christine Jardine's stored memories:\n[Document(metadata={'importance': 0.03, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 5, 589159), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 5, 589159), 'buffer_idx': 0}, page_content='Christine Jardine is a Liberal Democrat MP representing Edinburgh West.'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 6, 391886), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 6, 391886), 'buffer_idx': 1}, page_content='She has expressed concerns about ending free movement, highlighting its potential negative impact on public services and the economy.'), Document(metadata={'importance': 0.12, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 6, 984522), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 6, 984522), 'buffer_idx': 2}, page_content='Jardine emphasizes the human cost of ending free movement, noting that many EU nationals in her constituency feel unwelcome and uncertain about their future.')]\n\nKit Malthouse's stored memories:\n[Document(metadata={'importance': 0.03, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 7, 381241), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 7, 381241), 'buffer_idx': 0}, page_content='Kit Malthouse is a Conservative MP who has served as the Minister for Crime, Policing and the Fire Service.'), Document(metadata={'importance': 0.09, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 8, 284967), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 8, 284967), 'buffer_idx': 1}, page_content='During debates, he has stated that the government is committed to ending free movement as part of the Brexit process.'), Document(metadata={'importance': 0.075, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 8, 905762), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 8, 905762), 'buffer_idx': 2}, page_content='Malthouse has reassured that EU citizens residing in the UK are welcome to stay and that the government has implemented the EU Settlement Scheme to facilitate their continued residence.')]\n\nSteve Double's stored memories:\n[Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 9, 323283), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 9, 323283), 'buffer_idx': 0}, page_content='Steve Double is a Conservative MP representing St Austell and Newquay.'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 9, 840570), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 9, 840570), 'buffer_idx': 1}, page_content=\"He has expressed support for ending free movement, aligning with the government's stance on controlling immigration post-Brexit.\"), Document(metadata={'importance': 0.06, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 10, 385602), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 10, 385602), 'buffer_idx': 2}, page_content='Double has emphasized that the government has provided a clear message that EU citizens currently residing in the UK are welcome to stay, citing the success of the EU Settlement Scheme.')]\n\nTim Farron's stored memories:\n[Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 11, 580558), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 11, 580558), 'buffer_idx': 0}, page_content='Tim Farron is a Liberal Democrat MP representing Westmorland and Lonsdale.'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 12, 50025), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 12, 50025), 'buffer_idx': 1}, page_content=\"He has criticized the decision to end free movement, arguing that it sends a negative message to EU citizens and undermines the UK's international standing.\"), Document(metadata={'importance': 0.105, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 12, 723089), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 12, 723089), 'buffer_idx': 2}, page_content='Farron has highlighted the contributions of EU nationals to the UK and has advocated for their rights to be protected post-Brexit.')]\n\nJo Stevens's stored memories:\n[Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 13, 91359), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 13, 91359), 'buffer_idx': 0}, page_content='Jo Stevens is a Labour MP representing Cardiff Central.'), Document(metadata={'importance': 0.06, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 13, 547480), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 13, 547480), 'buffer_idx': 1}, page_content=\"She has raised concerns about the impact of ending free movement on universities, noting that it could harm the UK's global reputation and deter international students and academics.\"), Document(metadata={'importance': 0.105, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 14, 38424), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 14, 38424), 'buffer_idx': 2}, page_content=\"Stevens has advocated for the protection of EU nationals' rights and has questioned the government's approach to immigration post-Brexit.\")]\n\nRachael Maskell's stored memories:\n[Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 14, 493528), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 14, 493528), 'buffer_idx': 0}, page_content='Rachael Maskell is a Labour/Co-operative MP representing York Central.'), Document(metadata={'importance': 0.105, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 15, 120364), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 15, 120364), 'buffer_idx': 1}, page_content=\"She has expressed concerns about the government's immigration policies post-Brexit, particularly regarding family reunification and the rights of unaccompanied minors.\"), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2025, 3, 2, 16, 25, 15, 616280), 'created_at': datetime.datetime(2025, 3, 2, 16, 25, 15, 616280), 'buffer_idx': 2}, page_content=\"Maskell has questioned the government's preparedness for the consequences of ending free movement and has called for more compassionate immigration policies.\")]\n","output_type":"stream"}],"execution_count":136},{"cell_type":"markdown","source":"# Create Simulation","metadata":{}},{"cell_type":"markdown","source":"## Input Memory","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf_HoC_miniDebate = pd.read_csv('/kaggle/input/parlspeech/df_HoC_miniDebate.csv')\ndf_HoC_miniDebate.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:25:37.535914Z","iopub.execute_input":"2025-03-02T16:25:37.536240Z","iopub.status.idle":"2025-03-02T16:25:37.576361Z","shell.execute_reply.started":"2025-03-02T16:25:37.536213Z","shell.execute_reply":"2025-03-02T16:25:37.575478Z"}},"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"         date                         agenda  speechnumber            speaker  \\\n0  2019-10-02  Free Movement of EU Nationals           426  Christine Jardine   \n1  2019-10-02  Free Movement of EU Nationals           427      Kit Malthouse   \n2  2019-10-02  Free Movement of EU Nationals           428       Steve Double   \n\n    party                                               text  \n0  LibDem  I beg to move, That this House has considered ...  \n1     Con                               That is not correct.  \n2     Con  I must take exception to the language used by ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>agenda</th>\n      <th>speechnumber</th>\n      <th>speaker</th>\n      <th>party</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2019-10-02</td>\n      <td>Free Movement of EU Nationals</td>\n      <td>426</td>\n      <td>Christine Jardine</td>\n      <td>LibDem</td>\n      <td>I beg to move, That this House has considered ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2019-10-02</td>\n      <td>Free Movement of EU Nationals</td>\n      <td>427</td>\n      <td>Kit Malthouse</td>\n      <td>Con</td>\n      <td>That is not correct.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2019-10-02</td>\n      <td>Free Movement of EU Nationals</td>\n      <td>428</td>\n      <td>Steve Double</td>\n      <td>Con</td>\n      <td>I must take exception to the language used by ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":137},{"cell_type":"code","source":"initial_observation = 'Christine Jardine said, \"' + df_HoC_miniDebate.iloc[0]['text'] + '\"'\ninitial_observation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:25:51.078498Z","iopub.execute_input":"2025-03-02T16:25:51.078865Z","iopub.status.idle":"2025-03-02T16:25:51.084382Z","shell.execute_reply.started":"2025-03-02T16:25:51.078835Z","shell.execute_reply":"2025-03-02T16:25:51.083715Z"}},"outputs":[{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"'Christine Jardine said, \"I beg to move, That this House has considered proposed changes to free movement of EU nationals. I am delighted to raise the issue of freedom of movement in the EU, and I thank you, Sir David, for your chairmanship. €End freedom of movement” is a Brexiteer slogan that we have all become so accustomed to that it is easy to forget what it is really saying, and what it would really mean to this country, people living here and British citizens living abroad. We all know the basic numbers: freedom of movement allows 1.3 million British citizens to live, work, study, fall in love, marry, or retire across the European Union while more than 50,000 non-UK EU citizens work in our national health service, including support staff, nurses and doctors, all of whom play a vital role in our nation\\'s health. More than 80,000 EU citizens work in social care, and even more in the UK construction industry. As the Government love to tell us, unemployment is at its lowest rate for 40 years, but where are the British workers who are queuing up and clamouring to take those jobs? If we end freedom of movement, who will care for our sick and elderly? Who will build the 300,000 homes a year that Britain needs? The Government\\'s own figures show that non-UK EU citizens bring far more to our economy and public services than they use. If free movement ends, services will suffer because we will not have the people to continue to provide them at the same level. Those are the numbers, but what about the human cost and the sheer inhumanity of ending freedom of movement? Edinburgh West has constituents from France, Spain, Poland and many other EU countries who have made their lives in the city. Their children were born there, but now they are being told that they are not welcome. They feel they have no option but to leave.\"'"},"metadata":{}}],"execution_count":138},{"cell_type":"markdown","source":"## Run Debate\n1. Each agent add new-observation into memory. \n2. Each agent does a quick reflection on this new-observation, to whether to \"respond or not respond\" - depending on personal saliency (a custom function within the class `GenerativeAgent`). Output `decide_to_respond` as either True or False\n3. Randomly select one agent from the list of agents that decide to respond to the observation.\n4. Print this selected generate_dialogue_response as the new observation.","metadata":{}},{"cell_type":"code","source":"# List of agents in the debate\nagents = [ChristineJardine, KitMalthouse, SteveDouble, TimFarron, JoStevens, RachaelMaskell]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:25:54.693213Z","iopub.execute_input":"2025-03-02T16:25:54.693524Z","iopub.status.idle":"2025-03-02T16:25:54.697241Z","shell.execute_reply.started":"2025-03-02T16:25:54.693502Z","shell.execute_reply":"2025-03-02T16:25:54.696527Z"}},"outputs":[],"execution_count":139},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom typing import List\n\ndef run_HoC_debate_framework_3(agents: List[GenerativeAgent], \n                               initial_observation: str, \n                               save_path: str) -> None:\n    \"\"\"Runs a conversation between agents and saves the transcript as a structured table.\"\"\"\n    \n    max_turns = 25  # Adjust as needed\n    turns = 0\n    last_speaker = ChristineJardine  # Track last speaker\n    \n    observation = initial_observation\n    print(observation)\n\n    debate_records = []\n    debate_records.append({\"speaker\": \"Christine Jardine\", \"text\": observation})  # Add moderator intro\n    \n    # Debate loop\n    while turns < max_turns:\n        # Step 1: Each agent adds the new observation into memory\n        for agent in agents:\n            agent.memory.add_memory(observation)\n            \n        # Step 2: Select an agent who decides to respond (excluding the last speaker)\n        responding_agents = [agent for agent in agents if agent.decide_to_respond(observation) and agent != last_speaker]\n        \n        if responding_agents:\n            agent = random.choice(responding_agents)\n            last_speaker = agent\n            \n            # Generate response\n            observation = agent.generate_dialogue_response(observation)\n            \n            # Print response to console\n            print(observation + \"\\n\")\n            \n            # Append response to the debate records list\n            debate_records.append({\"speaker\": agent.name, \"text\": observation})\n        \n        # Increment turn count\n        turns += 1\n    \n    # Convert debate records to a DataFrame\n    df_debate = pd.DataFrame(debate_records)\n    \n    # Save the DataFrame as a CSV file\n    df_debate.to_csv(save_path, index=False, encoding=\"utf-8\")\n    \n    print(f\"Debate transcript saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:26:26.894920Z","iopub.execute_input":"2025-03-02T16:26:26.895222Z","iopub.status.idle":"2025-03-02T16:26:26.901961Z","shell.execute_reply.started":"2025-03-02T16:26:26.895200Z","shell.execute_reply":"2025-03-02T16:26:26.900999Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"debate_output_path = \"/kaggle/working/debate_transcript_llama_finetuned.csv\"\n\nrun_HoC_debate_framework_3(agents, initial_observation, debate_output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T16:26:39.566611Z","iopub.execute_input":"2025-03-02T16:26:39.566927Z","iopub.status.idle":"2025-03-02T16:31:31.674393Z","shell.execute_reply.started":"2025-03-02T16:26:39.566904Z","shell.execute_reply":"2025-03-02T16:31:31.673462Z"}},"outputs":[{"name":"stdout","text":"Christine Jardine said, \"I beg to move, That this House has considered proposed changes to free movement of EU nationals. I am delighted to raise the issue of freedom of movement in the EU, and I thank you, Sir David, for your chairmanship. €End freedom of movement” is a Brexiteer slogan that we have all become so accustomed to that it is easy to forget what it is really saying, and what it would really mean to this country, people living here and British citizens living abroad. We all know the basic numbers: freedom of movement allows 1.3 million British citizens to live, work, study, fall in love, marry, or retire across the European Union while more than 50,000 non-UK EU citizens work in our national health service, including support staff, nurses and doctors, all of whom play a vital role in our nation's health. More than 80,000 EU citizens work in social care, and even more in the UK construction industry. As the Government love to tell us, unemployment is at its lowest rate for 40 years, but where are the British workers who are queuing up and clamouring to take those jobs? If we end freedom of movement, who will care for our sick and elderly? Who will build the 300,000 homes a year that Britain needs? The Government's own figures show that non-UK EU citizens bring far more to our economy and public services than they use. If free movement ends, services will suffer because we will not have the people to continue to provide them at the same level. Those are the numbers, but what about the human cost and the sheer inhumanity of ending freedom of movement? Edinburgh West has constituents from France, Spain, Poland and many other EU countries who have made their lives in the city. Their children were born there, but now they are being told that they are not welcome. They feel they have no option but to leave.\"\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Kit Malthouse said I rise to address the Honourable Lady Jardine's remarks. While I acknowledge her eloquence, I must respectfully disagree with her assertion that the Government's position on free movement is driven by a desire to harm the most vulnerable. That is simply not the case. We are committed to ensuring that our immigration system is fair, effective, and sustainable for the long-term benefit of our country. The vast majority of EU nationals who are living in the UK are law-abiding citizens who contribute positively to our society. However, we must not forget that the EU's own citizens have a right to live and work in our country, just as British citizens do in theirs. We cannot have a one-way street on immigration, where one country gets to decide\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Steve Double said I rise to support my hon. Friend the Member for Richmond (Yorks) West. I must say, I find it quite astonishing that the Opposition is trying to turn this debate into a discussion about the rights of EU citizens. The fact is, we have a clear and well-established system in place for those who have been here since before the referendum. The EU Settlement Scheme, which has already helped over 2.4 million people, is a shining example of our commitment to supporting those who have contributed to our society. It is not about creating a two-tier system, as the Opposition seems to suggest. We are not trying to turn our backs on those who have made the UK their home. What we are doing is creating a system that is\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Kit Malthouse said I will not allow that, I will not allow the Opposition to muddy the waters with emotive language. My hon. Friend the Member for Richmond (Yorks) West is right to say that we have a system in place for those who have been here since before the referendum. That is precisely the point - we have a system for those who have made the UK their home, and that is something we should be proud of. But we cannot have a situation where those who arrived after the referendum are treated differently from those who arrived before it. That is not what this country is about. We have a long history of being a nation of immigrants, but we also have a responsibility to ensure that our immigration system is fair, sustainable, and works for\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Christine Jardine said I must say, I'm disappointed, but not surprised, by the hon. Gentleman's response. He seems to be stuck in a binary mindset, where he equates fairness with treating everyone the same, regardless of their circumstances. That is not a fair or effective way to run an immigration system. The fact is, thousands of EU nationals have made the UK their home, built lives, and contributed significantly to our society. They are not just statistics or abstract concepts; they are people, families, and friends. To suggest that we should treat them as somehow less deserving of our compassion and our support because they arrived after the referendum is not only unfair, but it is also utterly unrealistic. The hon. Gentleman talks about a system, but\n\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Tim Farron said I think the hon. Gentleman is trying to have a nuanced conversation, but he is being completely unrealistic. The system is not just about treating people the same; it is about creating a system that is fair, that is effective, and that is worthy of our values. The hon. Gentleman's response to the hon. Member for North East Somerset (Mr. Eastman) was to say that we should not be too quick to welcome people, that we should not be too quick to offer them a right to stay. But that is precisely the problem. The hon. Gentleman is saying that we should be too quick to welcome people, that we should be too quick to offer them a right to stay. That is not a policy;\n\nDebate transcript saved to /kaggle/working/debate_transcript_llama_finetuned.csv\n","output_type":"stream"}],"execution_count":141}]}