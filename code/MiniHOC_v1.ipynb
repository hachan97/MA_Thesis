{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10280888,"sourceType":"datasetVersion","datasetId":6361952}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport re\nimport math\nimport json\nimport random\nimport functools\nfrom datetime import datetime, timedelta\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nfrom collections import OrderedDict\n\n# Third-party imports\nimport torch\nimport openai\nimport faiss\nimport tenacity\n\n# LangChain imports\nfrom langchain.utils import mock_now\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.chains import LLMChain\nfrom langchain_core.language_models import BaseLanguageModel\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import HumanMessage, SystemMessage, BaseMemory, Document\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.output_parsers import RegexParser\n\n# Pydantic imports\nfrom pydantic import BaseModel, Field, ConfigDict\n\n# Hugging Face imports\nimport transformers\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline, AutoModel)\nfrom peft import PeftModel, PeftConfig\nfrom langchain_huggingface import HuggingFacePipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T14:55:45.206909Z","iopub.execute_input":"2025-02-24T14:55:45.207258Z","iopub.status.idle":"2025-02-24T14:56:03.499530Z","shell.execute_reply.started":"2025-02-24T14:55:45.207225Z","shell.execute_reply":"2025-02-24T14:56:03.498658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n## Openai\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T14:50:28.154432Z","iopub.execute_input":"2025-02-24T14:50:28.154679Z","iopub.status.idle":"2025-02-24T14:50:28.600296Z","shell.execute_reply.started":"2025-02-24T14:50:28.154653Z","shell.execute_reply":"2025-02-24T14:50:28.599530Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to Hugging Face\nfrom huggingface_hub import login\n\nlogin(Hugging_Face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T14:50:39.036395Z","iopub.execute_input":"2025-02-24T14:50:39.036717Z","iopub.status.idle":"2025-02-24T14:50:39.519289Z","shell.execute_reply.started":"2025-02-24T14:50:39.036688Z","shell.execute_reply":"2025-02-24T14:50:39.518551Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"markdown","source":"## Load Model: GPT","metadata":{}},{"cell_type":"code","source":"LLM_gpt  = ChatOpenAI(model=\"gpt-3.5-turbo\", \n                 max_tokens=1500, \n                 api_key = OPENAI_API_KEY) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T15:31:27.373806Z","iopub.execute_input":"2025-02-24T15:31:27.374570Z","iopub.status.idle":"2025-02-24T15:31:27.400869Z","shell.execute_reply.started":"2025-02-24T15:31:27.374530Z","shell.execute_reply":"2025-02-24T15:31:27.400236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_embeddings_model = OpenAIEmbeddings(api_key = OPENAI_API_KEY)\nembedding_size_selectedLLM = len(selected_embeddings_model.embed_query(\"This is a test.\"))\nprint(f\"Embedding size: {embedding_size_selectedLLM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T15:46:43.988160Z","iopub.execute_input":"2025-02-24T15:46:43.988889Z","iopub.status.idle":"2025-02-24T15:46:45.743001Z","shell.execute_reply.started":"2025-02-24T15:46:43.988854Z","shell.execute_reply":"2025-02-24T15:46:45.742183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generative AI Setup\nThe [codes](https://python.langchain.com/api_reference/experimental/generative_agents.html) for the classes `GenerativeAgentMemory` and `GenerativeAgent` was entirely reused from the **[LangChain Experimental](https://pypi.org/project/langchain-experimental/)** project in the LangChain Python API reference, with a few minor tweaks and proper configuration of the prompts.\n","metadata":{}},{"cell_type":"markdown","source":"## Generative Agent Memory","metadata":{}},{"cell_type":"code","source":"class GenerativeAgentMemory(BaseMemory):\n    \"\"\"Memory for the generative agent.\"\"\"\n    \n    llm: BaseLanguageModel\n    \"\"\"The core language model.\"\"\"\n    \n    memory_retriever: TimeWeightedVectorStoreRetriever\n    \"\"\"The retriever to fetch related memories.\"\"\"\n    \n    verbose: bool = False\n    reflection_threshold: Optional[float] = None\n    \"\"\"When aggregate_importance exceeds reflection_threshold, stop to reflect.\"\"\"\n    \n    current_plan: List[str] = []\n    \"\"\"The current plan of the agent.\"\"\"\n    \n    # A weight of 0.15 makes this less important than it\n    # would be otherwise, relative to salience and time\n    importance_weight: float = 0.15\n    \"\"\"How much weight to assign the memory importance.\"\"\"\n    aggregate_importance: float = 0.0  # : :meta private:\n    \"\"\"Track the sum of the 'importance' of recent memories.\n    Triggers reflection when it reaches reflection_threshold.\"\"\"\n    max_tokens_limit: int = 1200  # : :meta private:\n    \n    # input keys\n    queries_key: str = \"queries\"\n    most_recent_memories_token_key: str = \"recent_memories_token\"\n    add_memory_key: str = \"add_memory\"\n    \n    # output keys\n    relevant_memories_key: str = \"relevant_memories\"\n    relevant_memories_simple_key: str = \"relevant_memories_simple\"\n    most_recent_memories_key: str = \"most_recent_memories\"\n    now_key: str = \"now\"\n    reflecting: bool = False\n    \n    def chain(self, prompt: PromptTemplate) -> LLMChain:\n        return LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n    @staticmethod\n    \n    def _parse_list(text: str) -> List[str]:\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n        lines = re.split(r\"\\n\", text.strip())\n        lines = [line for line in lines if line.strip()]  # remove empty lines\n        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n    \n    def _get_topics_of_reflection(self, last_k: int = 50) -> List[str]:\n        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"{observations}\\n\\n\"\n            \"Given only the information above, what are the 3 most salient \"\n            \"high-level questions we can answer about the subjects in the statements?\\n\"\n            \"Provide each question on a new line.\"\n        )\n        observations = self.memory_retriever.memory_stream[-last_k:]\n        observation_str = \"\\n\".join(\n            [self._format_memory_detail(o) for o in observations]\n        )\n        result = self.chain(prompt).run(observations=observation_str)\n        return self._parse_list(result)\n    \n    def _get_insights_on_topic(\n        self, topic: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Generate 'insights' on a topic of reflection, based on pertinent memories.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"Statements relevant to: '{topic}'\\n\"\n            \"---\\n\"\n            \"{related_statements}\\n\"\n            \"---\\n\"\n            \"What 5 high-level novel insights can you infer from the above statements \"\n            \"that are relevant for answering the following question?\\n\"\n            \"Do not include any insights that are not relevant to the question.\\n\"\n            \"Do not repeat any insights that have already been made.\\n\\n\"\n            \"Question: {topic}\\n\\n\"\n            \"(example format: insight (because of 1, 5, 3))\\n\"\n        )\n        related_memories = self.fetch_memories(topic, now=now)\n        related_statements = \"\\n\".join(\n            [\n                self._format_memory_detail(memory, prefix=f\"{i+1}. \")\n                for i, memory in enumerate(related_memories)\n            ]\n        )\n        result = self.chain(prompt).run(\n            topic=topic, related_statements=related_statements\n        )\n        # TODO: Parse the connections between memories and insights\n        return self._parse_list(result)\n    \n    def pause_to_reflect(self, now: Optional[datetime] = None) -> List[str]:\n        \"\"\"Reflect on recent observations and generate 'insights'.\"\"\"\n        if self.verbose:\n            logger.info(\"Character is reflecting\")\n        new_insights = []\n        topics = self._get_topics_of_reflection()\n        for topic in topics:\n            insights = self._get_insights_on_topic(topic, now=now)\n            for insight in insights:\n                self.add_memory(insight, now=now)\n            new_insights.extend(insights)\n        return new_insights\n    \n    def _score_memory_importance(self, memory_content: str) -> float:\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"On the scale of 1 to 10, where 1 is purely mundane\"\n            + \" (e.g., brushing teeth, making bed) and 10 is\"\n            + \" extremely poignant (e.g., a break up, college\"\n            + \" acceptance), rate the likely poignancy of the\"\n            + \" following piece of memory. Respond with a single integer.\"\n            + \"\\nMemory: {memory_content}\"\n            + \"\\nRating: \"\n        )\n        score = self.chain(prompt).run(memory_content=memory_content).strip()\n        if self.verbose:\n            logger.info(f\"Importance score: {score}\")\n        match = re.search(r\"^\\D*(\\d+)\", score)\n        if match:\n            return (float(match.group(1)) / 10) * self.importance_weight\n        else:\n            return 0.0\n    \n    def _score_memories_importance(self, memory_content: str) -> List[float]:\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"On the scale of 1 to 10, where 1 is purely mundane\"\n            + \" (e.g., brushing teeth, making bed) and 10 is\"\n            + \" extremely poignant (e.g., a break up, college\"\n            + \" acceptance), rate the likely poignancy of the\"\n            + \" following piece of memory. Always answer with only a list of numbers.\"\n            + \" If just given one memory still respond in a list.\"\n            + \" Memories are separated by semi colans (;)\"\n            + \"\\nMemories: {memory_content}\"\n            + \"\\nRating: \"\n        )\n        scores = self.chain(prompt).run(memory_content=memory_content).strip()\n        if self.verbose:\n            logger.info(f\"Importance scores: {scores}\")\n        # Split into list of strings and convert to floats\n        scores_list = [float(x) for x in scores.split(\";\")]\n        return scores_list\n    \n    def add_memories(\n        self, memory_content: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Add an observations or memories to the agent's memory.\"\"\"\n        importance_scores = self._score_memories_importance(memory_content)\n        self.aggregate_importance += max(importance_scores)\n        memory_list = memory_content.split(\";\")\n        documents = []\n        for i in range(len(memory_list)):\n            documents.append(\n                Document(\n                    page_content=memory_list[i],\n                    metadata={\"importance\": importance_scores[i]},\n                )\n            )\n        result = self.memory_retriever.add_documents(documents, current_time=now)\n        # After an agent has processed a certain amount of memories (as measured by\n        # aggregate importance), it is time to reflect on recent events to add\n        # more synthesized memories to the agent's memory stream.\n        if (\n            self.reflection_threshold is not None\n            and self.aggregate_importance > self.reflection_threshold\n            and not self.reflecting\n        ):\n            self.reflecting = True\n            self.pause_to_reflect(now=now)\n            # Hack to clear the importance from reflection\n            self.aggregate_importance = 0.0\n            self.reflecting = False\n        return result\n    \n    def add_memory(\n        self, memory_content: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Add an observation or memory to the agent's memory.\"\"\"\n        importance_score = self._score_memory_importance(memory_content)\n        self.aggregate_importance += importance_score\n        document = Document(\n            page_content=memory_content, metadata={\"importance\": importance_score}\n        )\n        result = self.memory_retriever.add_documents([document], current_time=now)\n        # After an agent has processed a certain amount of memories (as measured by\n        # aggregate importance), it is time to reflect on recent events to add\n        # more synthesized memories to the agent's memory stream.\n        if (\n            self.reflection_threshold is not None\n            and self.aggregate_importance > self.reflection_threshold\n            and not self.reflecting\n        ):\n            self.reflecting = True\n            self.pause_to_reflect(now=now)\n            # Hack to clear the importance from reflection\n            self.aggregate_importance = 0.0\n            self.reflecting = False\n        return result\n    \n    def fetch_memories(\n        self, observation: str, now: Optional[datetime] = None\n    ) -> List[Document]:\n        \"\"\"Fetch related memories.\"\"\"\n        if now is not None:\n            with mock_now(now):\n                return self.memory_retriever.invoke(observation)\n        else:\n            return self.memory_retriever.invoke(observation)\n    \n    def format_memories_detail(self, relevant_memories: List[Document]) -> str:\n        content = []\n        for mem in relevant_memories:\n            content.append(self._format_memory_detail(mem, prefix=\"- \"))\n        return \"\\n\".join([f\"{mem}\" for mem in content])\n    \n    def _format_memory_detail(self, memory: Document, prefix: str = \"\") -> str:\n        created_time = memory.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\n        return f\"{prefix}[{created_time}] {memory.page_content.strip()}\"\n    \n    def format_memories_simple(self, relevant_memories: List[Document]) -> str:\n        return \"; \".join([f\"{mem.page_content}\" for mem in relevant_memories])\n    \n    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\n        \"\"\"Reduce the number of tokens in the documents.\"\"\"\n        result = []\n        for doc in self.memory_retriever.memory_stream[::-1]:\n            if consumed_tokens >= self.max_tokens_limit:\n                break\n            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\n            if consumed_tokens < self.max_tokens_limit:\n                result.append(doc)\n        return self.format_memories_simple(result)\n    @property\n    \n    def memory_variables(self) -> List[str]:\n        \"\"\"Input keys this memory class will load dynamically.\"\"\"\n        return []\n   \n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        queries = inputs.get(self.queries_key)\n        now = inputs.get(self.now_key)\n        if queries is not None:\n            relevant_memories = [\n                mem for query in queries for mem in self.fetch_memories(query, now=now)\n            ]\n            return {\n                self.relevant_memories_key: self.format_memories_detail(\n                    relevant_memories\n                ),\n                self.relevant_memories_simple_key: self.format_memories_simple(\n                    relevant_memories\n                ),\n            }\n        most_recent_memories_token = inputs.get(self.most_recent_memories_token_key)\n        if most_recent_memories_token is not None:\n            return {\n                self.most_recent_memories_key: self._get_memories_until_limit(\n                    most_recent_memories_token\n                )\n            }\n        return {}\n    \n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        # TODO: fix the save memory key\n        mem = outputs.get(self.add_memory_key)\n        now = outputs.get(self.now_key)\n        if mem:\n            self.add_memory(mem, now=now)\n    \n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        # TODO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:49:15.475120Z","iopub.execute_input":"2025-02-24T19:49:15.475455Z","iopub.status.idle":"2025-02-24T19:49:15.505090Z","shell.execute_reply.started":"2025-02-24T19:49:15.475424Z","shell.execute_reply":"2025-02-24T19:49:15.504341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generative Agent","metadata":{}},{"cell_type":"code","source":"class GenerativeAgent(BaseModel):\n    \"\"\"Agent as a character with memory and innate characteristics.\"\"\"\n    name: str\n    \"\"\"The character's name.\"\"\"\n    age: Optional[int] = None\n    \"\"\"The optional age of the character.\"\"\"\n    traits: str = \"N/A\"\n    \"\"\"Permanent traits to ascribe to the character.\"\"\"\n    status: str\n    \"\"\"The traits of the character you wish not to change.\"\"\"\n    memory: GenerativeAgentMemory\n    \"\"\"The memory object that combines relevance, recency, and 'importance'.\"\"\"\n    llm: BaseLanguageModel\n    \"\"\"The underlying language model.\"\"\"\n    verbose: bool = False\n    summary: str = \"\"  #: :meta private:\n    \"\"\"Stateful self-summary generated via reflection on the character's memory.\"\"\"\n    summary_refresh_seconds: int = 3600  #: :meta private:\n    \"\"\"How frequently to re-generate the summary.\"\"\"\n    last_refreshed: datetime = Field(default_factory=datetime.now)  # : :meta private:\n    \"\"\"The last time the character's summary was regenerated.\"\"\"\n    daily_summaries: List[str] = Field(default_factory=list)  # : :meta private:\n    \"\"\"Summary of the events in the plan that the agent took.\"\"\"\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    # LLM-related methods\n    @staticmethod\n    \n    def _parse_list(text: str) -> List[str]:\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n        lines = re.split(r\"\\n\", text.strip())\n        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n        \n    def chain(self, prompt: PromptTemplate) -> LLMChain:\n        \"\"\"Create a chain with the same settings as the agent.\"\"\"\n        return LLMChain(\n            llm=self.llm, prompt=prompt, verbose=self.verbose, memory=self.memory\n        )\n        \n    def _get_entity_from_observation(self, observation: str) -> str:\n        prompt = PromptTemplate.from_template(\n            \"What is the observed entity in the following observation? {observation}\"\n            + \"\\nEntity=\"\n        )\n        return self.chain(prompt).run(observation=observation).strip()\n        \n    def _get_entity_action(self, observation: str, entity_name: str) -> str:\n        prompt = PromptTemplate.from_template(\n            \"What is the {entity} doing in the following observation? {observation}\"\n            + \"\\nThe {entity} is\"\n        )\n        return (\n            self.chain(prompt).run(entity=entity_name, observation=observation).strip()\n        )\n\n## Summarize Most relevant memories\n    def summarize_related_memories(self, observation: str) -> str:\n        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"\"\"\n            {q1}?\n            Context from memory:\n            {relevant_memories}\n            Relevant context: \n            \"\"\"\n        )\n        entity_name = self._get_entity_from_observation(observation)\n        entity_action = self._get_entity_action(observation, entity_name)\n        q1 = f\"What is the relationship between {self.name} and {entity_name}\"\n        q2 = f\"{entity_name} is {entity_action}\"\n        return self.chain(prompt=prompt).run(q1=q1, queries=[q1, q2]).strip()\n        \n## Generate Summary of the agent + reaction \n    def _generate_reaction(\n        self, observation: str, suffix: str, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"React to a given observation or dialogue act.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"{agent_summary_description}\"\n            + \"\\nIt is {current_time}.\"\n            + \"\\n{agent_name}'s status: {agent_status}\"\n            + \"\\nSummary of relevant context from {agent_name}'s memory:\"\n            + \"\\n{relevant_memories}\"\n            + \"\\nMost recent observations: {most_recent_memories}\"\n            + \"\\nObservation: {observation}\"\n            + \"\\n\\n\"\n            + suffix\n        )\n        agent_summary_description = self.get_summary(now=now)\n        relevant_memories_str = self.summarize_related_memories(observation)\n        current_time_str = (\n            datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n            if now is None\n            else now.strftime(\"%B %d, %Y, %I:%M %p\")\n        )\n        kwargs: Dict[str, Any] = dict(\n            agent_summary_description=agent_summary_description,\n            current_time=current_time_str,\n            relevant_memories=relevant_memories_str,\n            agent_name=self.name,\n            observation=observation,\n            agent_status=self.status,\n        )\n        consumed_tokens = self.llm.get_num_tokens(\n            prompt.format(most_recent_memories=\"\", **kwargs)\n        )\n        kwargs[self.memory.most_recent_memories_token_key] = consumed_tokens\n        return self.chain(prompt=prompt).run(**kwargs).strip()\n        \n## Clean response\n    def _clean_response(self, text: str) -> str:\n        return re.sub(f\"^{self.name} \", \"\", text.strip()).strip()\n    \n## Generate Dialogue response\n    def generate_dialogue_response(\n        self, observation: str, now: Optional[datetime] = None) -> Tuple[bool, str]:\n        \"\"\"React to a given observation.\"\"\"\n        \n        call_to_action_template = (\n            \"You are {agent_name}, a Member of Parliament responding in a debate session in the UK House of Commons.\\n\"\n            \"Act as {agent_name} would, using your distinct voice and perspective. \\n\"\n            \"Respond to the most recent observations\"\n            \"Ensure that your response is direct, fully in character, and reflects your established views and tone. \\n\"\n            \"Respond exactly as {agent_name} would speak in this context.\"\n        )\n        \n        # Generating response with updated prompt\n        full_result = self._generate_reaction(observation, call_to_action_template, now=now)\n        #result = re.findall(r'\"(.*?)\"', full_result)[0]\n        \n        response_text = self._clean_response(full_result.strip())\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"{self.name} observed \"\n                f\"{observation} and said {response_text}\",\n                self.memory.now_key: now,\n            },\n        )\n        return True, f\"{self.name} said {response_text}\"\n\n## Decide if the agent wants to respond to the observation\n    def decide_to_respond(self, observation: str, now: Optional[datetime] = None,\n                          threshold: float = 7.0) -> bool:\n        \"\"\"Decide whether the agent wants to respond to the observation.\"\"\"\n\n        call_to_action_template = (\n            \"You are {agent_name}, a Member of Parliament currently sitting in the UK House of Commons.\"\n            \"\\nDiscussion Statement:\\n{observation}\\n\\n\"\n            \"On a scale of 1 to 10, how salient is this discussion to you as an MP?\"\n            \"\\n- 1: Not relevant at all\"\n            \"\\n- 10: Extremely relevant\"\n            \"\\n\\nRespond ONLY with a single integer between 1 and 10.\"\n            )\n        \n        full_result = self._generate_reaction(observation, call_to_action_template, now=now)\n        result = full_result.strip().lower()  # Normalize result to lowercase for consistent comparison\n        \n        try:\n            relevance_score = float(result)\n        except ValueError:\n            #logging.warning(f\"Unexpected non-numeric response from agent: {result}\")\n            print(f\"Unexpected non-numeric response from agent: {result}\")\n            relevance_score = 5  # Default low relevance for unexpected responses\n\n        # Save the decision context to memory\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"{self.name} observed \"\n                f\"that the relevance of the discussion '{observation}' was scored as {result}\",\n                self.memory.now_key: now,\n            },\n        )\n         \n        # Check if the model returned \"yes\" or \"no\"\n        if relevance_score < threshold:\n            return False\n        elif relevance_score >= threshold:\n            return True\n        else:\n            print(f\"Unexpected response: {result}\")  # For debugging purposes\n            return False\n    \n    ######################################################\n    # Agent stateful' summary methods.                   #\n    # Each dialog or response prompt includes a header   #\n    # summarizing the agent's self-description. This is  #\n    # updated periodically through probing its memories  #\n    ######################################################\n    \n    def _compute_agent_summary(self) -> str:\n        \"\"\"\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"How would you summarize {name}'s core characteristics given the\"\n            + \" following statements:\\n\"\n            + \"{relevant_memories}\"\n            + \"Do not embellish.\"\n            + \"\\n\\nSummary: \"\n        )\n        # The agent seeks to think about their core characteristics.\n        return (\n            self.chain(prompt)\n            .run(name=self.name, queries=[f\"{self.name}'s core characteristics\"])\n            .strip()\n        )\n    \n    def get_summary(\n        self, force_refresh: bool = False, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"Return a descriptive summary of the agent.\"\"\"\n        current_time = datetime.now() if now is None else now\n        since_refresh = (current_time - self.last_refreshed).seconds\n        if (\n            not self.summary\n            or since_refresh >= self.summary_refresh_seconds\n            or force_refresh\n        ):\n            self.summary = self._compute_agent_summary()\n            self.last_refreshed = current_time\n        age = self.age if self.age is not None else \"N/A\"\n        return (\n            f\"Name: {self.name} (age: {age})\"\n            + f\"\\nInnate traits: {self.traits}\"\n            + f\"\\n{self.summary}\"\n        )\n    \n    def get_full_header(\n        self, force_refresh: bool = False, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"Return a full header of the agent's status, summary, and current time.\"\"\"\n        now = datetime.now() if now is None else now\n        summary = self.get_summary(force_refresh=force_refresh, now=now)\n        current_time_str = now.strftime(\"%B %d, %Y, %I:%M %p\")\n        return (\n            f\"{summary}\\nIt is {current_time_str}.\\n{self.name}'s status: {self.status}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:49:17.145856Z","iopub.execute_input":"2025-02-24T19:49:17.146167Z","iopub.status.idle":"2025-02-24T19:49:17.170407Z","shell.execute_reply.started":"2025-02-24T19:49:17.146138Z","shell.execute_reply":"2025-02-24T19:49:17.169543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Agent\n- [GenerativeAgentMemory](https://python.langchain.com/api_reference/experimental/generative_agents/langchain_experimental.generative_agents.memory.GenerativeAgentMemory.html): **Memory** for the generative agent \n   - `llm`\n   - `memory_retriever` = create_new_memory_retriever()\n   - `current_plan`\n   - `reflection_threshold`\n   - `add_memory` add observation/memory\n- [GenerativeAgent](https://python.langchain.com/api_reference/experimental/generative_agents.html): Agent as a character with **memory** and innate **characteristics**,  \n   - basics like `name`, `age` and `llm`\n   - `memory` object that combines relevance, recency, and ‘importance’\n   - `summary` and `summary_refresh_seconds` to set how frequently to re-generate the summary\n   - `summarize_related_memories`: Summarize memories that are most relevant to an observation\n   - `status` fix-objectives / traits of the character you wish not to change\n   - `traits` set Permanent traits to ascribe to the character \n   - `generate_dialogue_response`","metadata":{}},{"cell_type":"code","source":"# Relevance Score function - relevance_score_fn()\ndef relevance_score_fn(score: float) -> float:\n    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n    return 1.0 - score / math.sqrt(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:45.190120Z","iopub.execute_input":"2025-02-24T19:24:45.190459Z","iopub.status.idle":"2025-02-24T19:24:45.194658Z","shell.execute_reply.started":"2025-02-24T19:24:45.190426Z","shell.execute_reply":"2025-02-24T19:24:45.193846Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Memory Retriever function - create_new_memory_retriever()\ndef create_new_memory_retriever():\n    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n    \n    embeddings_model = selected_embeddings_model  \n    \n    # Initialize the vectorstore as empty\n    embedding_size = embedding_size_selectedLLM           #use: 1536 (GPT3.5) or 3072 (Llamma)\n    \n    index = faiss.IndexFlatL2(embedding_size)\n    vectorstore = FAISS(\n        embeddings_model.embed_query,  #use: embeddings_model.embed_query OR llama_embedding_function\n        index,\n        InMemoryDocstore({}),  # empty Memory docstore\n        {},  # index-to-document store ID mapping\n        relevance_score_fn=relevance_score_fn,\n    )\n    \n    # Time-weighted scoring mechanism\n    return TimeWeightedVectorStoreRetriever(\n        vectorstore=vectorstore,\n        other_score_keys=[\"importance\"],\n        k=15  # retrieve up to 15 relevant memories\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:46.341967Z","iopub.execute_input":"2025-02-24T19:24:46.342569Z","iopub.status.idle":"2025-02-24T19:24:46.347546Z","shell.execute_reply.started":"2025-02-24T19:24:46.342526Z","shell.execute_reply":"2025-02-24T19:24:46.346674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Agent Creation function - create_debate_agent()\ndef create_debate_agent(name, age, traits, status, \n                        #reflection_threshold, \n                        llm):\n   \n    memory = GenerativeAgentMemory(\n        llm=llm,\n        memory_retriever=create_new_memory_retriever(),\n        verbose=False,\n        #reflection_threshold=reflection_threshold,  # adjust as needed for reflection frequency\n    )\n    \n    agent = GenerativeAgent(\n        name=name,\n        age=age,\n        traits=traits,\n        status=status,\n        memory_retriever=create_new_memory_retriever(),\n        llm=llm,\n        memory=memory,\n    )\n    return agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:47.611237Z","iopub.execute_input":"2025-02-24T19:24:47.611560Z","iopub.status.idle":"2025-02-24T19:24:47.616466Z","shell.execute_reply.started":"2025-02-24T19:24:47.611533Z","shell.execute_reply":"2025-02-24T19:24:47.615534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Agent Traits\n\n'Christine Jardine', 'Kit Malthouse', 'Steve Double', 'Tim Farron', 'Jo Stevens', 'Rachael Maskell'","metadata":{}},{"cell_type":"code","source":"# Create agents for each MP\nChristineJardine = create_debate_agent(\n    name=\"Christine Jardine\",\n    age=2019-1960,  # Example age\n    traits=\"Advocate for EU citizens' rights, empathetic communicator\",\n    status=\"Scottish Liberal Democrat politician; Opposes ending free movement; highlights contributions of EU nationals to the UK.\",\n    llm=LLM_gpt\n)\n\nKitMalthouse = create_debate_agent(\n    name=\"Kit Malthouse\",\n    age=2019-1966,  # Example age\n    traits=\"Firm on immigration control, prioritizes national sovereignty\",\n    status=\"British Conservative Party politician; Supports ending free movement; focuses on controlled immigration policies.\",\n    llm=LLM_gpt\n)\n\nSteveDouble = create_debate_agent(\n    name=\"Steve Double\",\n    age=2019-1966,  # Example age\n    traits=\"Emphasizes national interests, supportive of government policies\",\n    status=\"British Conservative Party politician; Advocates for ending free movement; reassures EU citizens are welcome under new schemes.\",\n    llm=LLM_gpt\n)\n\nTimFarron = create_debate_agent(\n    name=\"Tim Farron\",\n    age=2019-1970,  # Example age\n    traits=\"Pro-European, champions individual rights\",\n    status=\"British Liberal Democrats Party politician; Opposes ending free movement; stresses benefits of EU integration.\",\n    llm=LLM_gpt\n)\n\nJoStevens = create_debate_agent(\n    name=\"Jo Stevens\",\n    age=2019-1966,  # Example age\n    traits=\"Defender of workers' rights, critical of government policies\",\n    status=\"British Labour Party politician; Questions impact of ending free movement on labor markets and rights.\",\n    llm=LLM_gpt\n)\n\nRachaelMaskell = create_debate_agent(\n    name=\"Rachael Maskell\",\n    age=2019-1972,  # Example age\n    traits=\"Advocate for social justice, focuses on community welfare\",\n    status=\"British Labour and Co-operative Party politician; Concerns about social implications of ending free movement; emphasizes inclusive policies.\",\n    llm=LLM_gpt\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:49.278716Z","iopub.execute_input":"2025-02-24T19:24:49.279366Z","iopub.status.idle":"2025-02-24T19:24:49.286885Z","shell.execute_reply.started":"2025-02-24T19:24:49.279334Z","shell.execute_reply":"2025-02-24T19:24:49.285853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define Base Memories","metadata":{}},{"cell_type":"code","source":"# Creat Memory objects for each agent\nChristineJardine_memory = ChristineJardine.memory\nKitMalthouse_memory = KitMalthouse.memory\nSteveDouble_memory = SteveDouble.memory   \nTimFarron_memory = TimFarron.memory\nJoStevens_memory = JoStevens.memory\nRachaelMaskell_memory = RachaelMaskell.memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:51.234672Z","iopub.execute_input":"2025-02-24T19:24:51.235538Z","iopub.status.idle":"2025-02-24T19:24:51.239767Z","shell.execute_reply.started":"2025-02-24T19:24:51.235503Z","shell.execute_reply":"2025-02-24T19:24:51.238758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Base Observations \nChristineJardine_observations = [\n    \"Christine Jardine is a Liberal Democrat MP representing Edinburgh West.\",\n    \"She has expressed concerns about ending free movement, highlighting its potential negative impact on public services and the economy.\",\n    \"Jardine emphasizes the human cost of ending free movement, noting that many EU nationals in her constituency feel unwelcome and uncertain about their future.\"\n]\n\n# Base Observations for Kit Malthouse\nKitMalthouse_observations = [\n    \"Kit Malthouse is a Conservative MP who has served as the Minister for Crime, Policing and the Fire Service.\",\n    \"During debates, he has stated that the government is committed to ending free movement as part of the Brexit process.\",\n    \"Malthouse has reassured that EU citizens residing in the UK are welcome to stay and that the government has implemented the EU Settlement Scheme to facilitate their continued residence.\"\n]\n\nSteveDouble_observations = [\n    \"Steve Double is a Conservative MP representing St Austell and Newquay.\",\n    \"He has expressed support for ending free movement, aligning with the government's stance on controlling immigration post-Brexit.\",\n    \"Double has emphasized that the government has provided a clear message that EU citizens currently residing in the UK are welcome to stay, citing the success of the EU Settlement Scheme.\"\n]\n\nTimFarron_observations = [\n    \"Tim Farron is a Liberal Democrat MP representing Westmorland and Lonsdale.\",\n    \"He has criticized the decision to end free movement, arguing that it sends a negative message to EU citizens and undermines the UK's international standing.\",\n    \"Farron has highlighted the contributions of EU nationals to the UK and has advocated for their rights to be protected post-Brexit.\"\n]\n\nJoStevens_observations = [\n    \"Jo Stevens is a Labour MP representing Cardiff Central.\",\n    \"She has raised concerns about the impact of ending free movement on universities, noting that it could harm the UK's global reputation and deter international students and academics.\",\n    \"Stevens has advocated for the protection of EU nationals' rights and has questioned the government's approach to immigration post-Brexit.\"\n]\n\nRachaelMaskell_observations = [\n    \"Rachael Maskell is a Labour/Co-operative MP representing York Central.\",\n    \"She has expressed concerns about the government's immigration policies post-Brexit, particularly regarding family reunification and the rights of unaccompanied minors.\",\n    \"Maskell has questioned the government's preparedness for the consequences of ending free movement and has called for more compassionate immigration policies.\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:52.653163Z","iopub.execute_input":"2025-02-24T19:24:52.653536Z","iopub.status.idle":"2025-02-24T19:24:52.659361Z","shell.execute_reply.started":"2025-02-24T19:24:52.653503Z","shell.execute_reply":"2025-02-24T19:24:52.658533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through the observations and add to memory\ntuples = [(ChristineJardine_observations, ChristineJardine.memory), \n          (KitMalthouse_observations, KitMalthouse.memory), \n          (SteveDouble_observations, SteveDouble.memory), \n          (TimFarron_observations, TimFarron.memory), \n          (JoStevens_observations, JoStevens.memory), \n          (RachaelMaskell_observations, RachaelMaskell.memory)]\n\nfor observations, memory in tuples:\n    for observation in observations:\n        memory.add_memory(observation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T18:57:46.942950Z","iopub.execute_input":"2025-02-24T18:57:46.943275Z","iopub.status.idle":"2025-02-24T18:57:56.761143Z","shell.execute_reply.started":"2025-02-24T18:57:46.943245Z","shell.execute_reply":"2025-02-24T18:57:56.760499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View stored memories for each MP\nprint(\"Christine Jardine's stored memories:\")\nprint(ChristineJardine_memory.memory_retriever.memory_stream)\n\nprint(\"\\nKit Malthouse's stored memories:\")\nprint(KitMalthouse_memory.memory_retriever.memory_stream)\n\nprint(\"\\nSteve Double's stored memories:\")\nprint(SteveDouble_memory.memory_retriever.memory_stream)\n\nprint(\"\\nTim Farron's stored memories:\")\nprint(TimFarron_memory.memory_retriever.memory_stream)\n\nprint(\"\\nJo Stevens's stored memories:\")\nprint(JoStevens_memory.memory_retriever.memory_stream)\n\nprint(\"\\nRachael Maskell's stored memories:\")\nprint(RachaelMaskell_memory.memory_retriever.memory_stream)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:24:55.603656Z","iopub.execute_input":"2025-02-24T19:24:55.604294Z","iopub.status.idle":"2025-02-24T19:24:55.609989Z","shell.execute_reply.started":"2025-02-24T19:24:55.604257Z","shell.execute_reply":"2025-02-24T19:24:55.609006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create Simulation","metadata":{}},{"cell_type":"markdown","source":"## Input Memory","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf_HoC_2000s_raw = pd.read_csv('/kaggle/input/parlspeech/df_HoC_2000s.csv')\ndf_HoC_2000s_raw.columns\n\ndf_HoC_2000s = df_HoC_2000s_raw[['date', 'agenda', 'speechnumber', 'speaker', 'party','text']]\n\ndf_HoC_miniDebate = df_HoC_2000s[df_HoC_2000s['agenda'].str.contains('Free Movement of EU Nationals', case=False, na=False)]\n\ndel df_HoC_2000s_raw\ndel df_HoC_2000s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T18:27:40.771510Z","iopub.execute_input":"2025-02-24T18:27:40.771849Z","iopub.status.idle":"2025-02-24T18:27:59.098088Z","shell.execute_reply.started":"2025-02-24T18:27:40.771818Z","shell.execute_reply":"2025-02-24T18:27:59.097363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initial_observation = 'Christine Jardine said, \"' + df_HoC_miniDebate.iloc[0]['text'] + '\"'\ninitial_observation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T18:59:03.312562Z","iopub.execute_input":"2025-02-24T18:59:03.313221Z","iopub.status.idle":"2025-02-24T18:59:03.319060Z","shell.execute_reply.started":"2025-02-24T18:59:03.313188Z","shell.execute_reply":"2025-02-24T18:59:03.318173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run Debate\n1. Each agent add new-observation into memory. \n2. Each agent does a quick reflection on this new-observation, to whether to \"respond or not respond\" - depending on personal saliency (a custom function within the class `GenerativeAgent`). Output `decide_to_respond` as either True or False\n3. Randomly select one agent from the list of agents that decide to respond to the observation.\n4. Print this selected generate_dialogue_response as the new observation.","metadata":{}},{"cell_type":"code","source":"# List of agents in the debate\nagents = [ChristineJardine, KitMalthouse, SteveDouble, TimFarron, JoStevens, RachaelMaskell]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:25:03.421038Z","iopub.execute_input":"2025-02-24T19:25:03.421668Z","iopub.status.idle":"2025-02-24T19:25:03.425461Z","shell.execute_reply.started":"2025-02-24T19:25:03.421634Z","shell.execute_reply":"2025-02-24T19:25:03.424537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_HoC_debate_framework_3(agents: List[GenerativeAgent], \n                               initial_observation: str, \n                               save_path: str) -> None:\n    \"\"\"Runs a conversation between agents and saves the transcript to a file.\"\"\"\n    \n    max_turns = 5\n    turns = 0\n    last_speaker = ChristineJardine\n    \n    # Start the debate with an initial observation\n    observation = initial_observation\n    \n    # Open the file for writing\n    with open(save_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(\"House of Commons Debate Simulation\\n\")\n        file.write(\"=\" * 50 + \"\\n\")\n        file.write(f\"Debate Topic: Free Movement of EU Nationals\\n\")\n        file.write(\"=\" * 50 + \"\\n\\n\")\n        \n        # Write the initial observation\n        print(observation)\n        file.write(f\"{observation}\\n\\n\")\n    \n        # Debate loop\n        while turns < max_turns:\n            # Step 1: Each agent adds the new observation into memory\n            for agent in agents:\n                agent.memory.add_memory(observation)\n                \n            # Step 2: Randomly select an agent who decides to respond\n            responding_agents = [agent for agent in agents if agent.decide_to_respond(observation) and agent != last_speaker]\n            \n            if responding_agents:\n                agent = random.choice(responding_agents)\n                last_speaker = agent\n                \n                # Generate response\n                stay_in_dialogue, observation = agent.generate_dialogue_response(observation)\n                \n                # Print response to console\n                print(observation + \"\\n\")\n                \n                # Append response to file\n                with open(save_path, \"a\", encoding=\"utf-8\") as file:\n                    file.write(f\"{observation}\\n\\n\")\n            \n            # Increment turn count\n            turns += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:06:42.975893Z","iopub.execute_input":"2025-02-24T19:06:42.976502Z","iopub.status.idle":"2025-02-24T19:06:42.983596Z","shell.execute_reply.started":"2025-02-24T19:06:42.976471Z","shell.execute_reply":"2025-02-24T19:06:42.982672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom typing import List\n\ndef run_HoC_debate_framework_3(agents: List[GenerativeAgent], \n                               initial_observation: str, \n                               save_path: str) -> None:\n    \"\"\"Runs a conversation between agents and saves the transcript as a structured table.\"\"\"\n    \n    max_turns = 25  # Adjust as needed\n    turns = 0\n    last_speaker = ChristineJardine  # Track last speaker\n    \n    observation = initial_observation\n    print(observation)\n\n    debate_records = []\n    debate_records.append({\"speaker\": \"Christine Jardine\", \"text\": observation})  # Add moderator intro\n    \n    # Debate loop\n    while turns < max_turns:\n        # Step 1: Each agent adds the new observation into memory\n        for agent in agents:\n            agent.memory.add_memory(observation)\n            \n        # Step 2: Select an agent who decides to respond (excluding the last speaker)\n        responding_agents = [agent for agent in agents if agent.decide_to_respond(observation) and agent != last_speaker]\n        \n        if responding_agents:\n            agent = random.choice(responding_agents)\n            last_speaker = agent\n            \n            # Generate response\n            stay_in_dialogue, observation = agent.generate_dialogue_response(observation)\n            \n            # Print response to console\n            print(observation + \"\\n\")\n            \n            # Append response to the debate records list\n            debate_records.append({\"speaker\": agent.name, \"text\": observation})\n        \n        # Increment turn count\n        turns += 1\n    \n    # Convert debate records to a DataFrame\n    df_debate = pd.DataFrame(debate_records)\n    \n    # Save the DataFrame as a CSV file\n    df_debate.to_csv(save_path, index=False, encoding=\"utf-8\")\n    \n    print(f\"Debate transcript saved to {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:44:54.164060Z","iopub.execute_input":"2025-02-24T19:44:54.164707Z","iopub.status.idle":"2025-02-24T19:44:54.171555Z","shell.execute_reply.started":"2025-02-24T19:44:54.164672Z","shell.execute_reply":"2025-02-24T19:44:54.170677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"debate_output_path = \"/kaggle/working/debate_transcript_gpt.csv\"\n\nrun_HoC_debate_framework_3(agents, initial_observation, debate_output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T19:44:57.734109Z","iopub.execute_input":"2025-02-24T19:44:57.734924Z","iopub.status.idle":"2025-02-24T19:46:58.906876Z","shell.execute_reply.started":"2025-02-24T19:44:57.734890Z","shell.execute_reply":"2025-02-24T19:46:58.906030Z"}},"outputs":[],"execution_count":null}]}