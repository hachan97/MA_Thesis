{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10280888,"sourceType":"datasetVersion","datasetId":6361952}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup\n\n**THINGS TO FIX**:\n- `max_token` length exceded !!\n- alot of Truncated text - is this a problem?\n- ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\nimport matplotlib.pyplot as plt\nimport datetime\nfrom datasets import Dataset\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm\n\n#import torch.nn.attention.flex_attention\n#from torchtune.modules.tokenizers import ModelTokenizer\n#from torchtune.models.llama3 import llama3_tokenizer\n#from torchtune.data import Message","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:42:19.627140Z","iopub.execute_input":"2025-02-13T14:42:19.627322Z","iopub.status.idle":"2025-02-13T14:42:28.430602Z","shell.execute_reply.started":"2025-02-13T14:42:19.627304Z","shell.execute_reply":"2025-02-13T14:42:28.429978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n# Login to Hugging Face\nfrom huggingface_hub import login\n\nlogin(Hugging_Face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:43:00.493053Z","iopub.execute_input":"2025-02-13T14:43:00.493333Z","iopub.status.idle":"2025-02-13T14:43:00.710199Z","shell.execute_reply.started":"2025-02-13T14:43:00.493312Z","shell.execute_reply":"2025-02-13T14:43:00.709305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"#df_HoC_2000s_raw = pd.read_csv('H:/MA_Thesis/data/Rauh_Schwalbach_2020_ParlSpeech/df_HoC_2000s.csv')\n\ndf_HoC_2000s_raw = pd.read_csv('/kaggle/input/parlspeech/df_HoC_2000s.csv')\ndf_HoC_2000s_raw.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:43:02.714870Z","iopub.execute_input":"2025-02-13T14:43:02.715204Z","iopub.status.idle":"2025-02-13T14:43:37.549562Z","shell.execute_reply.started":"2025-02-13T14:43:02.715179Z","shell.execute_reply":"2025-02-13T14:43:37.548844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_HoC_2000s = df_HoC_2000s_raw[['date', 'agenda', 'speechnumber', 'speaker', 'party','text']]\ndf_HoC_2000s.columns\ndf_HoC_2000s.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:43:37.550632Z","iopub.execute_input":"2025-02-13T14:43:37.550939Z","iopub.status.idle":"2025-02-13T14:43:37.635516Z","shell.execute_reply.started":"2025-02-13T14:43:37.550907Z","shell.execute_reply":"2025-02-13T14:43:37.634814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analaysis","metadata":{}},{"cell_type":"code","source":"df_HoC_2000s['speaker'].value_counts().head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:41:15.241302Z","iopub.status.idle":"2025-02-11T16:41:15.241834Z","shell.execute_reply":"2025-02-11T16:41:15.241629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"David Cameron has {df_HoC_2000s_raw[df_HoC_2000s_raw['speaker'] == 'David Cameron']['terms'].sum()} terms\")\nprint(f\"Boris Johnson has {df_HoC_2000s_raw[df_HoC_2000s_raw['speaker'] == 'Boris Johnson']['terms'].sum()} terms\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T14:03:35.547247Z","iopub.execute_input":"2025-02-06T14:03:35.547603Z","iopub.status.idle":"2025-02-06T14:03:35.736324Z","shell.execute_reply.started":"2025-02-06T14:03:35.547575Z","shell.execute_reply":"2025-02-06T14:03:35.735384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_HoC_2005 = df_HoC_2000s[df_HoC_2000s['date'].str.contains('2005')]\ndf_HoC_2015 = df_HoC_2000s[df_HoC_2000s['date'].str.contains('2015')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:03.803928Z","iopub.execute_input":"2025-02-13T14:45:03.804263Z","iopub.status.idle":"2025-02-13T14:45:04.479035Z","shell.execute_reply.started":"2025-02-13T14:45:03.804237Z","shell.execute_reply":"2025-02-13T14:45:04.478124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Tokenizer","metadata":{}},{"cell_type":"code","source":"BASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:06.490807Z","iopub.execute_input":"2025-02-13T14:45:06.491163Z","iopub.status.idle":"2025-02-13T14:45:06.494845Z","shell.execute_reply.started":"2025-02-13T14:45:06.491134Z","shell.execute_reply":"2025-02-13T14:45:06.493838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the tokenizer to measure the length of the text\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, \n                                          add_bos_token=False, \n                                          trust_remote_code=True, \n                                          use_fast=True, \n                                          force_download=False)\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:07.593310Z","iopub.execute_input":"2025-02-13T14:45:07.593706Z","iopub.status.idle":"2025-02-13T14:45:09.579640Z","shell.execute_reply.started":"2025-02-13T14:45:07.593673Z","shell.execute_reply":"2025-02-13T14:45:09.578746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min(tokenizer.vocab.values()), max(tokenizer.vocab.values()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:10.549383Z","iopub.execute_input":"2025-02-13T14:45:10.549662Z","iopub.status.idle":"2025-02-13T14:45:10.738623Z","shell.execute_reply.started":"2025-02-13T14:45:10.549633Z","shell.execute_reply":"2025-02-13T14:45:10.737678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.special_tokens_map)\n\ntest_text = \"Hello, how are you today?\"\ntokens = tokenizer.encode(test_text, return_tensors=\"pt\")\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:11.929119Z","iopub.execute_input":"2025-02-13T14:45:11.929526Z","iopub.status.idle":"2025-02-13T14:45:11.970602Z","shell.execute_reply.started":"2025-02-13T14:45:11.929488Z","shell.execute_reply":"2025-02-13T14:45:11.969796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Data for Training\n\n**References on Preprocessing Dataset for Fine-tuning**\n- https://pytorch.org/torchtune/0.2/tutorials/chat.html\n- https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7\n\nYes, I would like to modify how many 'user' messages are included before and after each assistant response?","metadata":{}},{"cell_type":"code","source":"# Constants\nTOKEN_LENGTH_LIMIT = 3300\nMIN_TOKEN_LENGTH = 512\n\nMAX_NO_ASSISTANT_THRESHOLD = 10       # Reset conversation if assistant is absent for too long\nCHAT_OWNER = \"David Cameron\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:17.008839Z","iopub.execute_input":"2025-02-13T14:45:17.009193Z","iopub.status.idle":"2025-02-13T14:45:17.013097Z","shell.execute_reply.started":"2025-02-13T14:45:17.009165Z","shell.execute_reply":"2025-02-13T14:45:17.012425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_convo_1(df, output_path):\n    SYSTEM_PROMPT = \"\"\"You are David Cameron, a politician in the UK's House of Commons.\n    You are responding to Observations.\n    Respond exactly as David Cameron would speak, \n    staying fully in character and address the observation directly.\"\"\"\n\n    SYSTEM_PROMPT_TOKEN_LEN = len(tokenizer.encode(SYSTEM_PROMPT))\n\n    df = df.assign(role=df[\"speaker\"].apply(lambda x: \"assistant\" if x == CHAT_OWNER else \"user\"))  # Assign roles\n    grouped = df.groupby([\"date\", \"agenda\"])\n\n    conversations = []\n\n    for (date, agenda), group in tqdm(grouped):\n        conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n        token_len = SYSTEM_PROMPT_TOKEN_LEN\n        has_assistant_message = False\n        user_message_count = 0  \n\n        for _, row in group.iterrows():\n            role, message = row[\"role\"], row[\"text\"]\n            chat_message = {\"role\": role, \"content\": message}\n            chat_message_len = len(tokenizer.encode(message))\n\n            # Check if adding this message exceeds the token limit\n            if token_len + chat_message_len > TOKEN_LENGTH_LIMIT or user_message_count >= MAX_NO_ASSISTANT_THRESHOLD:\n                if token_len >= MIN_TOKEN_LENGTH and has_assistant_message:\n                    conversations.append({\"conversation\": conversation})\n\n                # Restart conversation\n                conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n                token_len = SYSTEM_PROMPT_TOKEN_LEN\n                has_assistant_message = False\n                user_message_count = 0  \n\n            # Append message to conversation\n            conversation.append(chat_message)\n            token_len += chat_message_len\n\n            if role == \"assistant\":\n                has_assistant_message = True\n                user_message_count = 0  \n            else:\n                user_message_count += 1  \n\n        # Save the last conversation if it meets the minimum length and contains an assistant message\n        if token_len >= MIN_TOKEN_LENGTH and has_assistant_message:\n            conversations.append({\"conversation\": conversation})\n\n    # Write to JSONL file\n    with open(output_path, 'w') as f:\n        for convo in conversations:\n            f.write(json.dumps(convo) + '\\n')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:45:18.697623Z","iopub.execute_input":"2025-02-13T14:45:18.697957Z","iopub.status.idle":"2025-02-13T14:45:18.705151Z","shell.execute_reply.started":"2025-02-13T14:45:18.697924Z","shell.execute_reply":"2025-02-13T14:45:18.704467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_path = '/kaggle/working/preprocessed_DavidCameron.jsonl'\npreprocess_convo_1(df_HoC_2000s, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:54:01.925004Z","iopub.execute_input":"2025-02-13T14:54:01.925296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check token lengths in dataset\ntotal_tokens = []\nwith open(output_path, 'r') as f:\n    for line in f:\n        data = json.loads(line)\n        \n        conversation = \" \".join([msg[\"content\"] for msg in data[\"conversation\"]])\n        token_count = len(tokenizer.encode(conversation))\n        total_tokens.append(token_count)\n\nprint(f\"Min tokens: {min(total_tokens)}, Max tokens: {max(total_tokens)}, Avg tokens: {sum(total_tokens)/len(total_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:50:32.368723Z","iopub.execute_input":"2025-02-13T14:50:32.369093Z","iopub.status.idle":"2025-02-13T14:50:33.617006Z","shell.execute_reply.started":"2025-02-13T14:50:32.369063Z","shell.execute_reply":"2025-02-13T14:50:33.616113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"over_limit = [tokens for tokens in total_tokens if tokens > 4096]\nprint(f\"Number of conversations exceeding 4096 tokens: {len(over_limit)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:52:04.220592Z","iopub.execute_input":"2025-02-13T14:52:04.220917Z","iopub.status.idle":"2025-02-13T14:52:04.225363Z","shell.execute_reply.started":"2025-02-13T14:52:04.220865Z","shell.execute_reply":"2025-02-13T14:52:04.224465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the histogram of 'Token Lengths'\nplt.figure(figsize=(10, 6))\nplt.hist(total_tokens, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Token Counts in Preprocessed Conversations\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:53:51.685361Z","iopub.execute_input":"2025-02-13T14:53:51.685682Z","iopub.status.idle":"2025-02-13T14:53:51.890054Z","shell.execute_reply.started":"2025-02-13T14:53:51.685657Z","shell.execute_reply":"2025-02-13T14:53:51.889310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect a Few Samples\ndialog_blocks = []\nwith open(output_path, 'r') as f:\n    for line in f:\n        dialog_blocks.append(json.loads(line))  # Parse each line as a JSON object\n\n# Display the number of dialog blocks\nprint(f\"Total dialog blocks: {len(dialog_blocks)}\")\n#dialog_blocks[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T09:52:24.941285Z","iopub.execute_input":"2025-02-12T09:52:24.941611Z","iopub.status.idle":"2025-02-12T09:52:24.958961Z","shell.execute_reply.started":"2025-02-12T09:52:24.941583Z","shell.execute_reply":"2025-02-12T09:52:24.958063Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare & Tokenize","metadata":{}},{"cell_type":"code","source":"today_date = datetime.datetime.now().strftime(\"%d %b %Y\")\n\n# Load the preprocessed JSONL dataset\nwith open(output_path, \"r\") as f:\n    raw_data = [json.loads(line) for line in f]\n\n# Convert into a dataset format that follows the guide\nformatted_data = []\n\nfor convo in raw_data:\n    messages = []\n    \n    for turn in convo[\"conversation\"]:\n        if turn[\"role\"] == \"system\":\n            messages.append({\"role\": \"system\", \"content\": turn[\"content\"]})\n        elif turn[\"role\"] == \"user\":\n            messages.append({\"role\": \"user\", \"content\": turn[\"content\"]})\n        elif turn[\"role\"] == \"assistant\":\n            messages.append({\"role\": \"assistant\", \"content\": turn[\"content\"]})\n    \n    # Apply chat template\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    prompt = prompt.replace(f\"Cutting Knowledge Date: December 2023\\nToday Date: {today_date}\\n\\n\",\"\")\n    \n    formatted_data.append({\"prompt\": prompt})\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_list(formatted_data)\n\ndef tokenize_function(example):\n    tokens = tokenizer(example['prompt'], \n                       add_special_tokens=False,\n                       padding=\"longest\", \n                       truncation=True, \n                       max_length=4096)\n    \n    tokens['labels'] = [-100 if token == tokenizer.pad_token_id else token for token in tokens['input_ids']    ]\n\n    return tokens\n\n# Apply tokenization\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset.save_to_disk(\"/kaggle/working/tokenized_dataset_DavidCameron\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T11:05:02.117670Z","iopub.execute_input":"2025-02-12T11:05:02.118013Z","iopub.status.idle":"2025-02-12T11:05:02.929927Z","shell.execute_reply.started":"2025-02-12T11:05:02.117985Z","shell.execute_reply":"2025-02-12T11:05:02.929013Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T11:04:40.788678Z","iopub.execute_input":"2025-02-12T11:04:40.789202Z","iopub.status.idle":"2025-02-12T11:04:40.795041Z","shell.execute_reply.started":"2025-02-12T11:04:40.789156Z","shell.execute_reply":"2025-02-12T11:04:40.794165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#decoded_text = tokenizer.decode(tokenized_dataset[2][\"input_ids\"])\n#decoded_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T10:59:41.966523Z","iopub.execute_input":"2025-02-12T10:59:41.966842Z","iopub.status.idle":"2025-02-12T10:59:41.970096Z","shell.execute_reply.started":"2025-02-12T10:59:41.966819Z","shell.execute_reply":"2025-02-12T10:59:41.969392Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization & Insights\n\n","metadata":{}},{"cell_type":"code","source":"# Extract token lengths\ntoken_lengths = [len(sample[\"input_ids\"]) for sample in tokenized_dataset]\n\n# Plot histogram\nplt.figure(figsize=(8, 5))\nplt.hist(token_lengths, bins=50, color=\"blue\", edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Token Length\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Tokenized Sequence Lengths\")\nplt.axvline(x=4096, color=\"red\", linestyle=\"dashed\", label=\"Max Length (4096)\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T11:49:06.509567Z","iopub.execute_input":"2025-02-12T11:49:06.509906Z","iopub.status.idle":"2025-02-12T11:49:07.322779Z","shell.execute_reply.started":"2025-02-12T11:49:06.509874Z","shell.execute_reply":"2025-02-12T11:49:07.321868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store original token lengths before truncation\noriginal_lengths = [len(tokenizer.encode(sample[\"prompt\"])) for sample in dataset]  # Raw dataset\n\n# Store tokenized lengths after truncation\ntruncated_lengths = [len(sample[\"input_ids\"]) for sample in tokenized_dataset_DavidCameron]\n\n# Calculate the number of truncated tokens per sample\ntokens_lost = [max(0, orig - trunc) for orig, trunc in zip(original_lengths, truncated_lengths)]\n\n# Compute total percentage of tokens lost\ntotal_tokens = sum(original_lengths)\ntotal_tokens_lost = sum(tokens_lost)\ntruncation_percentage = (total_tokens_lost / total_tokens) * 100\n\nprint(f\"Total Tokens: {total_tokens}\")\nprint(f\"Total Tokens Lost: {total_tokens_lost}\")\nprint(f\"Overall Truncation Percentage: {truncation_percentage:.2f}%\")\n\n# Plot histogram of tokens lost per sample\nplt.figure(figsize=(8, 5))\nplt.hist(tokens_lost, bins=30, color=\"red\", alpha=0.7, edgecolor=\"black\")\nplt.xlabel(\"Number of Tokens Truncated\")\nplt.ylabel(\"Number of Samples\")\nplt.title(\"Distribution of Tokens Truncated Per Sample\")\nplt.show()\n\n# Plot percentage of lost tokens\nplt.figure(figsize=(6, 5))\nplt.pie([total_tokens_lost, total_tokens - total_tokens_lost], \n        labels=[\"Truncated Tokens\", \"Retained Tokens\"], \n        autopct=\"%1.1f%%\", \n        colors=[\"red\", \"green\"])\nplt.title(\"Percentage of Truncated Tokens\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"truncated_samples = sum(1 for sample in tokenized_dataset if len(sample[\"input_ids\"]) == 4096)\ntotal_samples = len(tokenized_dataset)\n\nplt.figure(figsize=(6, 5))\nplt.pie([truncated_samples, total_samples - truncated_samples], \n        labels=[\"Truncated\", \"Not Truncated\"], autopct=\"%1.1f%%\", colors=[\"red\", \"green\"])\nplt.title(\"Percentage of Truncated Samples\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate padding ratios\npadding_ratios = [(sample[\"input_ids\"].count(tokenizer.pad_token_id) / len(sample[\"input_ids\"])) \n                  for sample in tokenized_dataset]\n\n# Plot distribution of padding percentages\nplt.figure(figsize=(8, 5))\nplt.hist(padding_ratios, bins=30, color=\"orange\", edgecolor=\"black\", alpha=0.7)\nplt.xlabel(\"Padding Ratio\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Padding Proportion in Tokenized Samples\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T11:10:26.011100Z","iopub.execute_input":"2025-02-12T11:10:26.011415Z","iopub.status.idle":"2025-02-12T11:10:26.758953Z","shell.execute_reply.started":"2025-02-12T11:10:26.011391Z","shell.execute_reply":"2025-02-12T11:10:26.758223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_tokens = []\nassistant_tokens = []\n\nfor sample in tokenized_dataset:\n    decoded_text = tokenizer.decode(sample[\"input_ids\"])\n    user_tokens.append(decoded_text.count(\"<|start_header_id|>user\"))\n    assistant_tokens.append(decoded_text.count(\"<|start_header_id|>assistant\"))\n\nplt.figure(figsize=(8, 5))\nplt.hist(user_tokens, bins=30, color=\"blue\", alpha=0.5, label=\"User Tokens\")\nplt.hist(assistant_tokens, bins=30, color=\"green\", alpha=0.5, label=\"Assistant Tokens\")\nplt.xlabel(\"Tokens Per Sample\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of User vs. Assistant Tokens\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T11:11:46.591058Z","iopub.execute_input":"2025-02-12T11:11:46.591427Z","iopub.status.idle":"2025-02-12T11:11:50.530162Z","shell.execute_reply.started":"2025-02-12T11:11:46.591400Z","shell.execute_reply":"2025-02-12T11:11:50.529202Z"}},"outputs":[],"execution_count":null}]}