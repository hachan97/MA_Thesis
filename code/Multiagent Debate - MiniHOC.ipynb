{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":180724,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":154008,"modelId":176490}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install openai faiss-cpu langchain langchain-community langchain_openai langchain_huggingface peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:53:24.574826Z","iopub.execute_input":"2024-12-06T14:53:24.575220Z","iopub.status.idle":"2024-12-06T14:53:48.272996Z","shell.execute_reply.started":"2024-12-06T14:53:24.575183Z","shell.execute_reply":"2024-12-06T14:53:48.271675Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-1.57.0-py3-none-any.whl.metadata (24 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\nCollecting langchain\n  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\nCollecting langchain_openai\n  Downloading langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\nCollecting langchain_huggingface\n  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.9.2)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (21.3)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.21 (from langchain)\n  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting tiktoken<1,>=0.7 (from langchain_openai)\n  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.25.1)\nCollecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (0.20.0)\nRequirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain_huggingface) (4.45.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0+cpu)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.21->langchain) (1.33)\nCollecting packaging (from faiss-cpu)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.4)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.3.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading openai-1.57.0-py3-none-any.whl (389 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.9/389.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.9-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.2.11-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading jiter-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, jiter, httpx-sse, tiktoken, requests-toolbelt, faiss-cpu, pydantic-settings, openai, langsmith, langchain-core, sentence-transformers, peft, langchain-text-splitters, langchain_openai, langchain_huggingface, langchain, langchain-community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed faiss-cpu-1.9.0.post1 httpx-sse-0.4.0 jiter-0.8.0 langchain-0.3.9 langchain-community-0.3.9 langchain-core-0.3.21 langchain-text-splitters-0.3.2 langchain_huggingface-0.1.2 langchain_openai-0.2.11 langsmith-0.1.147 openai-1.57.0 packaging-24.2 peft-0.14.0 pydantic-settings-2.6.1 requests-toolbelt-1.0.0 sentence-transformers-3.3.1 tiktoken-0.8.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Standard library imports\nimport os\nimport re\nimport math\nimport json\nimport random\nimport functools\nfrom datetime import datetime, timedelta\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\nfrom collections import OrderedDict\n\n# Third-party imports\nimport torch\nimport openai\nimport faiss\nimport tenacity\n\n# LangChain imports\nfrom langchain.utils import mock_now\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain.retrievers import TimeWeightedVectorStoreRetriever\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.chains import LLMChain\nfrom langchain_core.language_models import BaseLanguageModel\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import HumanMessage, SystemMessage, BaseMemory, Document\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.output_parsers import RegexParser\n\n# Pydantic imports\nfrom pydantic import BaseModel, Field, ConfigDict\n\n# Hugging Face imports\nimport transformers\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline, AutoModel)\nfrom peft import PeftModel, PeftConfig\nfrom langchain_huggingface import HuggingFacePipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:53:59.219279Z","iopub.execute_input":"2024-12-06T14:53:59.219710Z","iopub.status.idle":"2024-12-06T14:54:24.192485Z","shell.execute_reply.started":"2024-12-06T14:53:59.219665Z","shell.execute_reply":"2024-12-06T14:54:24.191268Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n## Openai\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n\n#from dotenv import load_dotenv # Get OPENAI_API_KEY from .env file\n#load_dotenv()\n#openai.api_key = os.getenv(\"OPENAI_API_KEY\") # Set API Key\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:54:27.743380Z","iopub.execute_input":"2024-12-06T14:54:27.744317Z","iopub.status.idle":"2024-12-06T14:54:27.977878Z","shell.execute_reply.started":"2024-12-06T14:54:27.744236Z","shell.execute_reply":"2024-12-06T14:54:27.976587Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Login to Hugging Face\nfrom huggingface_hub import login\n\nlogin(Hugging_Face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:54:29.600459Z","iopub.execute_input":"2024-12-06T14:54:29.600949Z","iopub.status.idle":"2024-12-06T14:54:29.703175Z","shell.execute_reply.started":"2024-12-06T14:54:29.600906Z","shell.execute_reply":"2024-12-06T14:54:29.702023Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"markdown","source":"## Load Model: GPT","metadata":{}},{"cell_type":"code","source":"LLM = ChatOpenAI(model=\"gpt-3.5-turbo\", \n                 max_tokens=1500, \n                 api_key = OPENAI_API_KEY) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:54:31.518877Z","iopub.execute_input":"2024-12-06T14:54:31.519313Z","iopub.status.idle":"2024-12-06T14:54:31.555033Z","shell.execute_reply.started":"2024-12-06T14:54:31.519264Z","shell.execute_reply":"2024-12-06T14:54:31.553495Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"selected_embeddings_model = OpenAIEmbeddings(api_key = OPENAI_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:54:32.734030Z","iopub.execute_input":"2024-12-06T14:54:32.734461Z","iopub.status.idle":"2024-12-06T14:54:32.769453Z","shell.execute_reply.started":"2024-12-06T14:54:32.734425Z","shell.execute_reply":"2024-12-06T14:54:32.768315Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"embedding_size_selectedLLM = len(selected_embeddings_model.embed_query(\"This is a test.\"))\nprint(f\"Embedding size: {embedding_size_selectedLLM}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:54:34.143501Z","iopub.execute_input":"2024-12-06T14:54:34.144622Z","iopub.status.idle":"2024-12-06T14:54:35.406853Z","shell.execute_reply.started":"2024-12-06T14:54:34.144572Z","shell.execute_reply":"2024-12-06T14:54:35.405356Z"}},"outputs":[{"name":"stdout","text":"Embedding size: 1536\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Load Model: Llama","metadata":{}},{"cell_type":"code","source":"# Set up Tokenizer & Model & Pipeline\nmodel_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\nmodel = AutoModel.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define LLM Pipeline\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    tokenizer=tokenizer,\n    max_new_tokens = 500,  # Maximum new tokens to generate\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    truncation=True,\n    #temperature=0.7,  # Sampling temperature\n    #top_p=0.9,        # Nucleus sampling\n    #repetition_penalty=1.2,  # Penalize repetition\n)\n\nLLM = HuggingFacePipeline(pipeline=pipeline)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test Model Output\ndef response(prompt):\n     sequences = pipeline(prompt,\n                          do_sample=True,   # adjust\n                          top_k=10,        # adjust vocabulary size\n                          )\n     #print('Question: ' , prompt + '\\n')\n     print('response: ', sequences[0]['generated_text'][len(prompt):] + '\\n')\n\n\ndiscussion = \"Should the UK rejoin the European Union?\"\nagent_name = \"Boris Johnson\" \nprompt = (\n    f\"Consider the following discussion:\\n\\n\"\n    f\"{discussion}\\n\\n\"\n    f\"As {agent_name}, on a scale of 1 to 10, how relevant is this discussion to you? \"\n    f\"Provide a number between 1 (not relevant at all) and 10 (extremely relevant).\"\n    f\"Output only a single number, nothing else!\"\n)\n\n\nresponse(prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When switching to any other LLaMA-based model, you need to replace the embedding model (OpenAIEmbeddings) with an embedding generation mechanism that works with your local LLaMA model. OpenAI provides embeddings as a service, but with LLaMA, **you need to generate embeddings using the model locally**.","metadata":{}},{"cell_type":"code","source":"# Custom Llama Embeddings\nclass CustomLlamaEmbeddings:   # Copilot genereated\n    def __init__(self, model, tokenizer, device=\"cuda\"):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        \n    def embed_documents(self, texts):\n        \"\"\"Generate embeddings for a single query string.\"\"\"\n        # Tokenize inputs\n        inputs = self.tokenizer(\n            texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n        \n        # Get hidden states from the model\n        with torch.no_grad():\n            outputs = self.model(**inputs, return_dict=True, output_hidden_states=True)\n            \n        # Use the mean pooling of the last hidden state as embeddings\n        embeddings = outputs.hidden_states[-1].mean(dim=1)\n        return embeddings.cpu().numpy()\n        \n    def embed_query(self, text):\n        \"\"\"Generate embeddings for a list of documents.\"\"\"\n        return self.embed_documents([text])[0]\n\nselected_embeddings_model = CustomLlamaEmbeddings(model=model, tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_id)\n# Print the embedding size\nembedding_size_selectedLLM = config.hidden_size\nprint(f\"Embedding Size (hidden size): {embedding_size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Model: Tuned Boris","metadata":{}},{"cell_type":"code","source":"# base_model_name = \"meta-llama/Llama-3.2-3B\"\nPEFT_MODEL = \"/kaggle/input/llama_boris/pytorch/default/1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                      # Load model in 4bit, to redeuce memory and computational requirements\n    bnb_4bit_use_double_quant=True,         # Double quantization, further compress the model weights\n    bnb_4bit_quant_type=\"nf4\",              # Quantization type = nf4\n    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in 16bit format, to speed up computation\n    load_in_8bit_fp32_cpu_offload=True\n)\nconfig = PeftConfig.from_pretrained(PEFT_MODEL)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\ntokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the Fine-tuned model\nprompt = \"Should the UK rejoin the European Union?\"\n# Tokenize the input prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Generate a response\noutput = model.generate(\n    input_ids=inputs[\"input_ids\"],\n    attention_mask=inputs[\"attention_mask\"],  # Explicitly set the attention mask\n    max_length=300,              # Maximum length of the generated response\n    temperature=0.7,             # Sampling temperature for more creative responses\n    top_p=0.9,                   # Nucleus sampling for generating diverse text\n    repetition_penalty=1.2,      # Penalize repetition in the response\n    do_sample=True,              # Enable sampling for non-deterministic output\n    pad_token_id=tokenizer.eos_token_id,      # Explicitly set the pad token ID\n)\n# Decode and print the response\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomLlamaEmbeddings:\n    def __init__(self, model, tokenizer, device=\"cuda\"):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        \n    def embed_documents(self, texts):\n        # Tokenize inputs\n        inputs = self.tokenizer(\n            texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n        \n        # Get hidden states from the model\n        with torch.no_grad():\n            outputs = self.model(**inputs, return_dict=True, output_hidden_states=True)\n            \n        # Use the mean pooling of the last hidden state as embeddings\n        embeddings = outputs.hidden_states[-1].mean(dim=1)\n        return embeddings.cpu().numpy()\n        \n    def embed_query(self, text):\n        \"\"\"Generate embeddings for a list of documents.\"\"\"\n        return self.embed_documents([text])[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define LLM and Embeddings\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=500,  # Maximum tokens in the output\n    temperature=0.7,  # Sampling temperature\n    top_p=0.9,        # Nucleus sampling\n    repetition_penalty=1.2,  # Penalize repetition\n)\nLLM = HuggingFacePipeline(pipeline=pipe)\nselected_embeddings_model = CustomLlamaEmbeddings(model, tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings = selected_embeddings_model.embed_documents([\"Sample document\"])\nprint(f\"Embedding dimension: {embeddings.shape[1]}\")  # Check the embedding size","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generative AI Setup\nThe [codes](https://python.langchain.com/api_reference/experimental/generative_agents.html) for the classes `GenerativeAgentMemory` and `GenerativeAgent` was entirely reused from the **[LangChain Experimental](https://pypi.org/project/langchain-experimental/)** project in the LangChain Python API reference - intended for research and experimental uses, with a few minor tweaks and proper configuration of the prompts.\n","metadata":{}},{"cell_type":"markdown","source":"## Generative Agent Memory","metadata":{}},{"cell_type":"code","source":"class GenerativeAgentMemory(BaseMemory):\n    \"\"\"Memory for the generative agent.\"\"\"\n    \n    llm: BaseLanguageModel\n    \"\"\"The core language model.\"\"\"\n    \n    memory_retriever: TimeWeightedVectorStoreRetriever\n    \"\"\"The retriever to fetch related memories.\"\"\"\n    \n    verbose: bool = False\n    reflection_threshold: Optional[float] = None\n    \"\"\"When aggregate_importance exceeds reflection_threshold, stop to reflect.\"\"\"\n    \n    current_plan: List[str] = []\n    \"\"\"The current plan of the agent.\"\"\"\n    \n    # A weight of 0.15 makes this less important than it\n    # would be otherwise, relative to salience and time\n    importance_weight: float = 0.15\n    \"\"\"How much weight to assign the memory importance.\"\"\"\n    aggregate_importance: float = 0.0  # : :meta private:\n    \"\"\"Track the sum of the 'importance' of recent memories.\n    Triggers reflection when it reaches reflection_threshold.\"\"\"\n    max_tokens_limit: int = 1200  # : :meta private:\n    \n    # input keys\n    queries_key: str = \"queries\"\n    most_recent_memories_token_key: str = \"recent_memories_token\"\n    add_memory_key: str = \"add_memory\"\n    \n    # output keys\n    relevant_memories_key: str = \"relevant_memories\"\n    relevant_memories_simple_key: str = \"relevant_memories_simple\"\n    most_recent_memories_key: str = \"most_recent_memories\"\n    now_key: str = \"now\"\n    reflecting: bool = False\n    \n    def chain(self, prompt: PromptTemplate) -> LLMChain:\n        return LLMChain(llm=self.llm, prompt=prompt, verbose=self.verbose)\n    @staticmethod\n    \n    def _parse_list(text: str) -> List[str]:\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n        lines = re.split(r\"\\n\", text.strip())\n        lines = [line for line in lines if line.strip()]  # remove empty lines\n        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n    \n    def _get_topics_of_reflection(self, last_k: int = 50) -> List[str]:\n        \"\"\"Return the 3 most salient high-level questions about recent observations.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"{observations}\\n\\n\"\n            \"Given only the information above, what are the 3 most salient \"\n            \"high-level questions we can answer about the subjects in the statements?\\n\"\n            \"Provide each question on a new line.\"\n        )\n        observations = self.memory_retriever.memory_stream[-last_k:]\n        observation_str = \"\\n\".join(\n            [self._format_memory_detail(o) for o in observations]\n        )\n        result = self.chain(prompt).run(observations=observation_str)\n        return self._parse_list(result)\n    \n    def _get_insights_on_topic(\n        self, topic: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Generate 'insights' on a topic of reflection, based on pertinent memories.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"Statements relevant to: '{topic}'\\n\"\n            \"---\\n\"\n            \"{related_statements}\\n\"\n            \"---\\n\"\n            \"What 5 high-level novel insights can you infer from the above statements \"\n            \"that are relevant for answering the following question?\\n\"\n            \"Do not include any insights that are not relevant to the question.\\n\"\n            \"Do not repeat any insights that have already been made.\\n\\n\"\n            \"Question: {topic}\\n\\n\"\n            \"(example format: insight (because of 1, 5, 3))\\n\"\n        )\n        related_memories = self.fetch_memories(topic, now=now)\n        related_statements = \"\\n\".join(\n            [\n                self._format_memory_detail(memory, prefix=f\"{i+1}. \")\n                for i, memory in enumerate(related_memories)\n            ]\n        )\n        result = self.chain(prompt).run(\n            topic=topic, related_statements=related_statements\n        )\n        # TODO: Parse the connections between memories and insights\n        return self._parse_list(result)\n    \n    def pause_to_reflect(self, now: Optional[datetime] = None) -> List[str]:\n        \"\"\"Reflect on recent observations and generate 'insights'.\"\"\"\n        if self.verbose:\n            logger.info(\"Character is reflecting\")\n        new_insights = []\n        topics = self._get_topics_of_reflection()\n        for topic in topics:\n            insights = self._get_insights_on_topic(topic, now=now)\n            for insight in insights:\n                self.add_memory(insight, now=now)\n            new_insights.extend(insights)\n        return new_insights\n    \n    def _score_memory_importance(self, memory_content: str) -> float:\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"On the scale of 1 to 10, where 1 is purely mundane\"\n            + \" (e.g., brushing teeth, making bed) and 10 is\"\n            + \" extremely poignant (e.g., a break up, college\"\n            + \" acceptance), rate the likely poignancy of the\"\n            + \" following piece of memory. Respond with a single integer.\"\n            + \"\\nMemory: {memory_content}\"\n            + \"\\nRating: \"\n        )\n        score = self.chain(prompt).run(memory_content=memory_content).strip()\n        if self.verbose:\n            logger.info(f\"Importance score: {score}\")\n        match = re.search(r\"^\\D*(\\d+)\", score)\n        if match:\n            return (float(match.group(1)) / 10) * self.importance_weight\n        else:\n            return 0.0\n    \n    def _score_memories_importance(self, memory_content: str) -> List[float]:\n        \"\"\"Score the absolute importance of the given memory.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"On the scale of 1 to 10, where 1 is purely mundane\"\n            + \" (e.g., brushing teeth, making bed) and 10 is\"\n            + \" extremely poignant (e.g., a break up, college\"\n            + \" acceptance), rate the likely poignancy of the\"\n            + \" following piece of memory. Always answer with only a list of numbers.\"\n            + \" If just given one memory still respond in a list.\"\n            + \" Memories are separated by semi colans (;)\"\n            + \"\\nMemories: {memory_content}\"\n            + \"\\nRating: \"\n        )\n        scores = self.chain(prompt).run(memory_content=memory_content).strip()\n        if self.verbose:\n            logger.info(f\"Importance scores: {scores}\")\n        # Split into list of strings and convert to floats\n        scores_list = [float(x) for x in scores.split(\";\")]\n        return scores_list\n    \n    def add_memories(\n        self, memory_content: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Add an observations or memories to the agent's memory.\"\"\"\n        importance_scores = self._score_memories_importance(memory_content)\n        self.aggregate_importance += max(importance_scores)\n        memory_list = memory_content.split(\";\")\n        documents = []\n        for i in range(len(memory_list)):\n            documents.append(\n                Document(\n                    page_content=memory_list[i],\n                    metadata={\"importance\": importance_scores[i]},\n                )\n            )\n        result = self.memory_retriever.add_documents(documents, current_time=now)\n        # After an agent has processed a certain amount of memories (as measured by\n        # aggregate importance), it is time to reflect on recent events to add\n        # more synthesized memories to the agent's memory stream.\n        if (\n            self.reflection_threshold is not None\n            and self.aggregate_importance > self.reflection_threshold\n            and not self.reflecting\n        ):\n            self.reflecting = True\n            self.pause_to_reflect(now=now)\n            # Hack to clear the importance from reflection\n            self.aggregate_importance = 0.0\n            self.reflecting = False\n        return result\n    \n    def add_memory(\n        self, memory_content: str, now: Optional[datetime] = None\n    ) -> List[str]:\n        \"\"\"Add an observation or memory to the agent's memory.\"\"\"\n        importance_score = self._score_memory_importance(memory_content)\n        self.aggregate_importance += importance_score\n        document = Document(\n            page_content=memory_content, metadata={\"importance\": importance_score}\n        )\n        result = self.memory_retriever.add_documents([document], current_time=now)\n        # After an agent has processed a certain amount of memories (as measured by\n        # aggregate importance), it is time to reflect on recent events to add\n        # more synthesized memories to the agent's memory stream.\n        if (\n            self.reflection_threshold is not None\n            and self.aggregate_importance > self.reflection_threshold\n            and not self.reflecting\n        ):\n            self.reflecting = True\n            self.pause_to_reflect(now=now)\n            # Hack to clear the importance from reflection\n            self.aggregate_importance = 0.0\n            self.reflecting = False\n        return result\n    \n    def fetch_memories(\n        self, observation: str, now: Optional[datetime] = None\n    ) -> List[Document]:\n        \"\"\"Fetch related memories.\"\"\"\n        if now is not None:\n            with mock_now(now):\n                return self.memory_retriever.invoke(observation)\n        else:\n            return self.memory_retriever.invoke(observation)\n    \n    def format_memories_detail(self, relevant_memories: List[Document]) -> str:\n        content = []\n        for mem in relevant_memories:\n            content.append(self._format_memory_detail(mem, prefix=\"- \"))\n        return \"\\n\".join([f\"{mem}\" for mem in content])\n    \n    def _format_memory_detail(self, memory: Document, prefix: str = \"\") -> str:\n        created_time = memory.metadata[\"created_at\"].strftime(\"%B %d, %Y, %I:%M %p\")\n        return f\"{prefix}[{created_time}] {memory.page_content.strip()}\"\n    \n    def format_memories_simple(self, relevant_memories: List[Document]) -> str:\n        return \"; \".join([f\"{mem.page_content}\" for mem in relevant_memories])\n    \n    def _get_memories_until_limit(self, consumed_tokens: int) -> str:\n        \"\"\"Reduce the number of tokens in the documents.\"\"\"\n        result = []\n        for doc in self.memory_retriever.memory_stream[::-1]:\n            if consumed_tokens >= self.max_tokens_limit:\n                break\n            consumed_tokens += self.llm.get_num_tokens(doc.page_content)\n            if consumed_tokens < self.max_tokens_limit:\n                result.append(doc)\n        return self.format_memories_simple(result)\n    @property\n    \n    def memory_variables(self) -> List[str]:\n        \"\"\"Input keys this memory class will load dynamically.\"\"\"\n        return []\n   \n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Return key-value pairs given the text input to the chain.\"\"\"\n        queries = inputs.get(self.queries_key)\n        now = inputs.get(self.now_key)\n        if queries is not None:\n            relevant_memories = [\n                mem for query in queries for mem in self.fetch_memories(query, now=now)\n            ]\n            return {\n                self.relevant_memories_key: self.format_memories_detail(\n                    relevant_memories\n                ),\n                self.relevant_memories_simple_key: self.format_memories_simple(\n                    relevant_memories\n                ),\n            }\n        most_recent_memories_token = inputs.get(self.most_recent_memories_token_key)\n        if most_recent_memories_token is not None:\n            return {\n                self.most_recent_memories_key: self._get_memories_until_limit(\n                    most_recent_memories_token\n                )\n            }\n        return {}\n    \n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> None:\n        \"\"\"Save the context of this model run to memory.\"\"\"\n        # TODO: fix the save memory key\n        mem = outputs.get(self.add_memory_key)\n        now = outputs.get(self.now_key)\n        if mem:\n            self.add_memory(mem, now=now)\n    \n    def clear(self) -> None:\n        \"\"\"Clear memory contents.\"\"\"\n        # TODO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:12:25.502704Z","iopub.execute_input":"2024-12-06T15:12:25.503117Z","iopub.status.idle":"2024-12-06T15:12:25.544685Z","shell.execute_reply.started":"2024-12-06T15:12:25.503080Z","shell.execute_reply":"2024-12-06T15:12:25.543533Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Generative Agent","metadata":{}},{"cell_type":"code","source":"class GenerativeAgent(BaseModel):\n    \"\"\"Agent as a character with memory and innate characteristics.\"\"\"\n    name: str\n    \"\"\"The character's name.\"\"\"\n    age: Optional[int] = None\n    \"\"\"The optional age of the character.\"\"\"\n    traits: str = \"N/A\"\n    \"\"\"Permanent traits to ascribe to the character.\"\"\"\n    status: str\n    \"\"\"The traits of the character you wish not to change.\"\"\"\n    memory: GenerativeAgentMemory\n    \"\"\"The memory object that combines relevance, recency, and 'importance'.\"\"\"\n    llm: BaseLanguageModel\n    \"\"\"The underlying language model.\"\"\"\n    verbose: bool = False\n    summary: str = \"\"  #: :meta private:\n    \"\"\"Stateful self-summary generated via reflection on the character's memory.\"\"\"\n    summary_refresh_seconds: int = 3600  #: :meta private:\n    \"\"\"How frequently to re-generate the summary.\"\"\"\n    last_refreshed: datetime = Field(default_factory=datetime.now)  # : :meta private:\n    \"\"\"The last time the character's summary was regenerated.\"\"\"\n    daily_summaries: List[str] = Field(default_factory=list)  # : :meta private:\n    \"\"\"Summary of the events in the plan that the agent took.\"\"\"\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n    # LLM-related methods\n    @staticmethod\n    \n    def _parse_list(text: str) -> List[str]:\n        \"\"\"Parse a newline-separated string into a list of strings.\"\"\"\n        lines = re.split(r\"\\n\", text.strip())\n        return [re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", line).strip() for line in lines]\n        \n    def chain(self, prompt: PromptTemplate) -> LLMChain:\n        \"\"\"Create a chain with the same settings as the agent.\"\"\"\n        return LLMChain(\n            llm=self.llm, prompt=prompt, verbose=self.verbose, memory=self.memory\n        )\n        \n    def _get_entity_from_observation(self, observation: str) -> str:\n        prompt = PromptTemplate.from_template(\n            \"What is the observed entity in the following observation? {observation}\"\n            + \"\\nEntity=\"\n        )\n        return self.chain(prompt).run(observation=observation).strip()\n        \n    def _get_entity_action(self, observation: str, entity_name: str) -> str:\n        prompt = PromptTemplate.from_template(\n            \"What is the {entity} doing in the following observation? {observation}\"\n            + \"\\nThe {entity} is\"\n        )\n        return (\n            self.chain(prompt).run(entity=entity_name, observation=observation).strip()\n        )\n\n## Summarize Most relevant memories\n    def summarize_related_memories(self, observation: str) -> str:\n        \"\"\"Summarize memories that are most relevant to an observation.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"\"\"\n            {q1}?\n            Context from memory:\n            {relevant_memories}\n            Relevant context: \n            \"\"\"\n        )\n        entity_name = self._get_entity_from_observation(observation)\n        entity_action = self._get_entity_action(observation, entity_name)\n        q1 = f\"What is the relationship between {self.name} and {entity_name}\"\n        q2 = f\"{entity_name} is {entity_action}\"\n        return self.chain(prompt=prompt).run(q1=q1, queries=[q1, q2]).strip()\n        \n## Generate Summary of the agent + reaction \n    def _generate_reaction(\n        self, observation: str, suffix: str, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"React to a given observation or dialogue act.\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"{agent_summary_description}\"\n            + \"\\nIt is {current_time}.\"\n            + \"\\n{agent_name}'s status: {agent_status}\"\n            + \"\\nSummary of relevant context from {agent_name}'s memory:\"\n            + \"\\n{relevant_memories}\"\n            + \"\\nMost recent observations: {most_recent_memories}\"\n            + \"\\nObservation: {observation}\"\n            + \"\\n\\n\"\n            + suffix\n        )\n        agent_summary_description = self.get_summary(now=now)\n        relevant_memories_str = self.summarize_related_memories(observation)\n        current_time_str = (\n            datetime.now().strftime(\"%B %d, %Y, %I:%M %p\")\n            if now is None\n            else now.strftime(\"%B %d, %Y, %I:%M %p\")\n        )\n        kwargs: Dict[str, Any] = dict(\n            agent_summary_description=agent_summary_description,\n            current_time=current_time_str,\n            relevant_memories=relevant_memories_str,\n            agent_name=self.name,\n            observation=observation,\n            agent_status=self.status,\n        )\n        consumed_tokens = self.llm.get_num_tokens(\n            prompt.format(most_recent_memories=\"\", **kwargs)\n        )\n        kwargs[self.memory.most_recent_memories_token_key] = consumed_tokens\n        return self.chain(prompt=prompt).run(**kwargs).strip()\n        \n## Clean response\n    def _clean_response(self, text: str) -> str:\n        return re.sub(f\"^{self.name} \", \"\", text.strip()).strip()\n        \n## Generate Reaction\n    def generate_reaction(\n        self, observation: str, now: Optional[datetime] = None\n    ) -> Tuple[bool, str]:\n        \"\"\"React to a given observation.\"\"\"\n        call_to_action_template = (\n            \"Should {agent_name} react to the observation, and if so,\"\n            + \" what would be an appropriate reaction? Respond in one line.\"\n            + ' If the action is to engage in dialogue, write:\\nSAY: \"what to say\"'\n            + \"\\notherwise, write:\\nREACT: {agent_name}'s reaction (if anything).\"\n            + \"\\nEither do nothing, react, or say something but not both.\\n\\n\"\n        )\n        full_result = self._generate_reaction(\n            observation, call_to_action_template, now=now\n        )\n        result = full_result.strip().split(\"\\n\")[0]\n        # AAA\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"{self.name} observed \"\n                f\"{observation} and reacted by {result}\",\n                self.memory.now_key: now,\n            },\n        )\n        \n        if \"REACT:\" in result:\n            reaction = self._clean_response(result.split(\"REACT:\")[-1])\n            return False, f\"{self.name} {reaction}\"\n        \n        if \"SAY:\" in result:\n            said_value = self._clean_response(result.split(\"SAY:\")[-1])\n            return True, f\"{self.name} said {said_value}\"\n        \n        else:\n            return False, result\n    \n## Generate Dialogue response\n    def generate_dialogue_response(\n        self, observation: str, now: Optional[datetime] = None) -> Tuple[bool, str]:\n        \"\"\"React to a given observation.\"\"\"\n        \n        call_to_action_template = (\n            \"What would {agent_name} say in response to the observation provided?\\n\"\n            \"Respond directly with what {agent_name} would say next.\\n\\n\"\n            )\n        \n        # Generating response with updated prompt\n        full_result = self._generate_reaction(observation, call_to_action_template, now=now)\n        result = re.findall(r'\"(.*?)\"', full_result)[0]\n        \n        response_text = self._clean_response(result.strip())\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"{self.name} observed \"\n                f\"{observation} and said {response_text}\",\n                self.memory.now_key: now,\n            },\n        )\n        return True, f\"{self.name} said {response_text}\"\n\n## Decide if the agent wants to respond to the observation\n    def decide_to_respond(self, observation: str, now: Optional[datetime] = None,\n                          threshold: float = 7.0) -> bool:\n        \"\"\"Decide whether the agent wants to respond to the observation.\"\"\"\n\n        call_to_action_template = (\n            \"Consider the following discussion:\\n\\n\"\n            \"{observation}\\n\\n\"\n            \"As {agent_name}, on a scale of 1 to 10, how relevant is this discussion to you? \"\n            \"Provide a number between 1 (not relevant at all) and 10 (extremely relevant).\"\n            \"Output only a number, nothing else!\"\n            )\n        \n        full_result = self._generate_reaction(observation, call_to_action_template, now=now)\n        result = full_result.strip().lower()  # Normalize result to lowercase for consistent comparison\n        \n        try:\n            relevance_score = float(result)\n        except ValueError:\n            logging.warning(f\"Unexpected non-numeric response from agent: {result}\")\n            relevance_score = 0  # Default low relevance for unexpected responses\n\n        # Save the decision context to memory\n        self.memory.save_context(\n            {},\n            {\n                self.memory.add_memory_key: f\"{self.name} observed \"\n                f\"that the relevance of the discussion '{observation}' was scored as {result}\",\n                self.memory.now_key: now,\n            },\n        )\n         \n        # Check if the model returned \"yes\" or \"no\"\n        if relevance_score < threshold:\n            return False\n        elif relevance_score >= threshold:\n            return True\n        else:\n            print(f\"Unexpected response: {result}\")  # For debugging purposes\n            return False\n    \n    ######################################################\n    # Agent stateful' summary methods.                   #\n    # Each dialog or response prompt includes a header   #\n    # summarizing the agent's self-description. This is  #\n    # updated periodically through probing its memories  #\n    ######################################################\n    \n    def _compute_agent_summary(self) -> str:\n        \"\"\"\"\"\"\n        prompt = PromptTemplate.from_template(\n            \"How would you summarize {name}'s core characteristics given the\"\n            + \" following statements:\\n\"\n            + \"{relevant_memories}\"\n            + \"Do not embellish.\"\n            + \"\\n\\nSummary: \"\n        )\n        # The agent seeks to think about their core characteristics.\n        return (\n            self.chain(prompt)\n            .run(name=self.name, queries=[f\"{self.name}'s core characteristics\"])\n            .strip()\n        )\n    \n    def get_summary(\n        self, force_refresh: bool = False, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"Return a descriptive summary of the agent.\"\"\"\n        current_time = datetime.now() if now is None else now\n        since_refresh = (current_time - self.last_refreshed).seconds\n        if (\n            not self.summary\n            or since_refresh >= self.summary_refresh_seconds\n            or force_refresh\n        ):\n            self.summary = self._compute_agent_summary()\n            self.last_refreshed = current_time\n        age = self.age if self.age is not None else \"N/A\"\n        return (\n            f\"Name: {self.name} (age: {age})\"\n            + f\"\\nInnate traits: {self.traits}\"\n            + f\"\\n{self.summary}\"\n        )\n    \n    def get_full_header(\n        self, force_refresh: bool = False, now: Optional[datetime] = None\n    ) -> str:\n        \"\"\"Return a full header of the agent's status, summary, and current time.\"\"\"\n        now = datetime.now() if now is None else now\n        summary = self.get_summary(force_refresh=force_refresh, now=now)\n        current_time_str = now.strftime(\"%B %d, %Y, %I:%M %p\")\n        return (\n            f\"{summary}\\nIt is {current_time_str}.\\n{self.name}'s status: {self.status}\"\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:28:21.318720Z","iopub.execute_input":"2024-12-06T15:28:21.319341Z","iopub.status.idle":"2024-12-06T15:28:21.356986Z","shell.execute_reply.started":"2024-12-06T15:28:21.319284Z","shell.execute_reply":"2024-12-06T15:28:21.355714Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"# Create Agent\n- [GenerativeAgentMemory](https://python.langchain.com/api_reference/experimental/generative_agents/langchain_experimental.generative_agents.memory.GenerativeAgentMemory.html): **Memory** for the generative agent \n   - `llm`\n   - `memory_retriever` = create_new_memory_retriever()\n   - `current_plan`\n   - `reflection_threshold`\n   - `add_memory` add observation/memory\n- [GenerativeAgent](https://python.langchain.com/api_reference/experimental/generative_agents.html): Agent as a character with **memory** and innate **characteristics**,  \n   - basics like `name`, `age` and `llm`\n   - `memory` object that combines relevance, recency, and ‘importance’\n   - `summary` and `summary_refresh_seconds` to set how frequently to re-generate the summary\n   - `summarize_related_memories`: Summarize memories that are most relevant to an observation\n   - `status` fix-objectives / traits of the character you wish not to change\n   - `traits` set Permanent traits to ascribe to the character \n   - `generate_dialogue_response`","metadata":{}},{"cell_type":"code","source":"# Relevance Score function - relevance_score_fn()\ndef relevance_score_fn(score: float) -> float:\n    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n    return 1.0 - score / math.sqrt(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:25:39.685532Z","iopub.execute_input":"2024-12-06T15:25:39.686060Z","iopub.status.idle":"2024-12-06T15:25:39.692658Z","shell.execute_reply.started":"2024-12-06T15:25:39.686020Z","shell.execute_reply":"2024-12-06T15:25:39.691089Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"# Memory Retriever function - create_new_memory_retriever()\ndef create_new_memory_retriever():\n    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n    \n    embeddings_model = selected_embeddings_model  \n    \n    # Initialize the vectorstore as empty\n    embedding_size = embedding_size_selectedLLM           #use: 1536 (GPT3.5) or 3072 (Llamma)\n    \n    index = faiss.IndexFlatL2(embedding_size)\n    vectorstore = FAISS(\n        embeddings_model.embed_query,  #use: embeddings_model.embed_query OR llama_embedding_function\n        index,\n        InMemoryDocstore({}),  # empty Memory docstore\n        {},  # index-to-document store ID mapping\n        relevance_score_fn=relevance_score_fn,\n    )\n    \n    # Time-weighted scoring mechanism\n    return TimeWeightedVectorStoreRetriever(\n        vectorstore=vectorstore,\n        other_score_keys=[\"importance\"],\n        k=15  # retrieve up to 15 relevant memories\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:25:49.342017Z","iopub.execute_input":"2024-12-06T15:25:49.342447Z","iopub.status.idle":"2024-12-06T15:25:49.349736Z","shell.execute_reply.started":"2024-12-06T15:25:49.342408Z","shell.execute_reply":"2024-12-06T15:25:49.348579Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# Agent Creation function - create_debate_agent()\ndef create_debate_agent(name, age, traits, status, \n                        #reflection_threshold, \n                        llm):\n   \n    memory = GenerativeAgentMemory(\n        llm=llm,\n        memory_retriever=create_new_memory_retriever(),\n        verbose=False,\n        #reflection_threshold=reflection_threshold,  # adjust as needed for reflection frequency\n    )\n    \n    agent = GenerativeAgent(\n        name=name,\n        age=age,\n        traits=traits,\n        status=status,\n        memory_retriever=create_new_memory_retriever(),\n        llm=llm,\n        memory=memory,\n    )\n    return agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:25:50.485770Z","iopub.execute_input":"2024-12-06T15:25:50.486158Z","iopub.status.idle":"2024-12-06T15:25:50.493169Z","shell.execute_reply.started":"2024-12-06T15:25:50.486125Z","shell.execute_reply":"2024-12-06T15:25:50.491661Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"## Define Agent Traits","metadata":{}},{"cell_type":"code","source":"# Create debate agents (MPs) with their respective characteristics\nTrott = create_debate_agent(name=\"Laura Trott\", age=38, llm = LLM,\n                            traits= \"highly disciplined, sharp, and pragmatic. Strategic, focus on “quiet competence” rather than loud rhetoric, detail-oriented and a stickler for facts\",\n                            status=\"Conservative MP\")\n\nJohnson = create_debate_agent(name=\"Boris Johnson\", age=57, llm = LLM,\n                            traits=\"charismatic, chaotic, opportunistic, larger-than-life personality, thrives on spectacle and Blitz-spirit optimism, mixes humor with charm and a dash of bluster, unpredictable yet captivating, a showman who values headlines over substance\",\n                            status=\"Conservative MP\")\n\nFarage = create_debate_agent(name=\"Nigel Farage\", age=60, llm = LLM,\n                             traits=\"unapologetically bold, confrontational, divisive, a provocateur, skilled at stirring public opinion with blunt populist rhetoric, political brawle, highly skilled at galvanizing crowds\",\n                             status=\"Former UKIP leader, Brexit Party leader, and political commentator\")\n\nSunak = create_debate_agent(name=\"Rishi Sunak\", age=44, llm = LLM,\n                            traits=\"technocratic, astute, polished, financially extremely wealthy, meticulous, highly analytical, known as the Fiscal-Guardian, out of touch with the middle-class\",\n                            status=\"Conservative MP, Former Prime Minister\")\n\nStarmer = create_debate_agent(name=\"Sir Keir Starmer\", age=61, llm = LLM,\n                              traits=\"methodical, earnest, intense focus on justice and reform, calm demeanor, seeks accountability, values facts over flair, deliver points with precision rather than emotion\",\n                              status=\"Leader of the Labour Party\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:25:51.989862Z","iopub.execute_input":"2024-12-06T15:25:51.991345Z","iopub.status.idle":"2024-12-06T15:25:52.003479Z","shell.execute_reply.started":"2024-12-06T15:25:51.991264Z","shell.execute_reply":"2024-12-06T15:25:52.001495Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"## Define Base Memories","metadata":{}},{"cell_type":"code","source":"# Creat Memory objects for each agent\nTrott_memory = Trott.memory\nJohnson_memory = Johnson.memory\nFarage_memory = Farage.memory   \nSunak_memory = Sunak.memory\nStarmer_memory = Starmer.memory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:25:53.372156Z","iopub.execute_input":"2024-12-06T15:25:53.372559Z","iopub.status.idle":"2024-12-06T15:25:53.378840Z","shell.execute_reply.started":"2024-12-06T15:25:53.372528Z","shell.execute_reply":"2024-12-06T15:25:53.377437Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Base Observations \nTrott_observations = [\n    \"Trott attended Oxted School, studied history and economics at Oxford University\",\n    \"Trott is preparing for a debate on the economy\",\n    \"Trott advocates for responsible budgeting and cautious government spending\",\n    \"Trott emphasize business growth and pragmatic economic solutions\",\n    \"Trott generally conservative but supports progressive stances on education and family policies\",\n    \"Trott focuses on pragmatic rather than ideological approaches\",\n]\nJohnson_observations = [\n    \"Johnson attended Eton College, studied Classics Oxford University\",\n    \"Johnson is Pro-Brexit and economically nationalist\",\n    \"Johnson advocates for deregulation, minimal government intervention, and strong support for British businesses\",\n    \"Johnson is a populist, often aligning with traditional conservative values, though flexible when politically advantageous\",\n    \"Johnson is support strong national identity and sovereignty\",\n]\nFarage_observations = [\n    \"Farage attended Dulwich College but did not attend university\",\n    \"Farage is strongly Eurosceptic, advocates for British sovereignty, deregulation, and cutting ties with EU economic policies\",\n    \"Farage prioritizes domestic industry and independence from European influence\",\n    \"Farage is a Nationalist, anti-globalist, and socially conservative\",\n    \"Farage advocates for strict immigration controls and promotes traditional British values\"\n]\nSunak_observations = [\n    \"Sunak studied Philosophy, Politics, and Economics at Oxford University and later earned an MBA from Stanford University\",\n    \"Sunak is fiscal conservative with a focus on budget balancing\",\n    \"Sunak advocates for responsible spending and a cautious approach to government intervention\",\n    \"Sunak prioritizes stability over drastic reforms\",\n    \"Sunak focus on pragmatism over ideology, holds relatively conservative views on social issues, often supporting traditional family values\",\n]\nStarmer_observations = [\n    \"Starmer attended Reigate Grammar School, studied law at the University of Leeds and completed studies at Oxford University\",\n    \"Starmer focuses on investment in public services, especially the NHS, and progressive taxation\",\n    \"Starmer prioritizes worker rights and social equality, advocating for a balanced but progressive approach\",\n    \"Starmer supports expanded public services, social justice, and inclusivity\",\n    \"Starmer Focuses on social reform and government accountability\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:26:01.645755Z","iopub.execute_input":"2024-12-06T15:26:01.646153Z","iopub.status.idle":"2024-12-06T15:26:01.654553Z","shell.execute_reply.started":"2024-12-06T15:26:01.646122Z","shell.execute_reply":"2024-12-06T15:26:01.653052Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Loop through the observations and add to memory\ntuples = [(Trott_observations, Trott_memory), (Johnson_observations, Johnson_memory), \n          (Farage_observations, Farage_memory), (Sunak_observations, Sunak_memory), (Starmer_observations, Starmer_memory)]\n\nfor observations, memory in tuples:\n    for observation in observations:\n        memory.add_memory(observation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:26:30.973610Z","iopub.execute_input":"2024-12-06T15:26:30.973983Z","iopub.status.idle":"2024-12-06T15:26:43.842627Z","shell.execute_reply.started":"2024-12-06T15:26:30.973950Z","shell.execute_reply":"2024-12-06T15:26:43.841131Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# View stored memories\nprint(\"Trott's stored memories:\")\nprint(Trott_memory.memory_retriever.memory_stream)\n\nprint(\"\\nJohnson's stored memories:\")\nprint(Johnson_memory.memory_retriever.memory_stream)\n\nprint(\"\\nFarage's stored memories:\")\nprint(Farage_memory.memory_retriever.memory_stream)\n\nprint(\"\\nSunak's stored memories:\")\nprint(Sunak_memory.memory_retriever.memory_stream)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:27:40.849686Z","iopub.execute_input":"2024-12-06T15:27:40.850114Z","iopub.status.idle":"2024-12-06T15:27:40.858542Z","shell.execute_reply.started":"2024-12-06T15:27:40.850078Z","shell.execute_reply":"2024-12-06T15:27:40.857237Z"}},"outputs":[{"name":"stdout","text":"Trott's stored memories:\n[Document(metadata={'importance': 0.105, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 31, 325320), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 31, 325320), 'buffer_idx': 0}, page_content='Trott attended Oxted School, studied history and economics at Oxford University'), Document(metadata={'importance': 0.075, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 31, 720445), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 31, 720445), 'buffer_idx': 1}, page_content='Trott is preparing for a debate on the economy'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 32, 81650), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 32, 81650), 'buffer_idx': 2}, page_content='Trott advocates for responsible budgeting and cautious government spending'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 32, 507625), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 32, 507625), 'buffer_idx': 3}, page_content='Trott emphasize business growth and pragmatic economic solutions'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 33, 60629), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 33, 60629), 'buffer_idx': 4}, page_content='Trott generally conservative but supports progressive stances on education and family policies'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 33, 415935), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 33, 415935), 'buffer_idx': 5}, page_content='Trott focuses on pragmatic rather than ideological approaches')]\n\nJohnson's stored memories:\n[Document(metadata={'importance': 0.12, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 33, 895469), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 33, 895469), 'buffer_idx': 0}, page_content='Johnson attended Eton College, studied Classics Oxford University'), Document(metadata={'importance': 0.06, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 34, 640647), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 34, 640647), 'buffer_idx': 1}, page_content='Johnson is Pro-Brexit and economically nationalist'), Document(metadata={'importance': 0.06, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 35, 132854), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 35, 132854), 'buffer_idx': 2}, page_content='Johnson advocates for deregulation, minimal government intervention, and strong support for British businesses'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 35, 515920), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 35, 515920), 'buffer_idx': 3}, page_content='Johnson is a populist, often aligning with traditional conservative values, though flexible when politically advantageous'), Document(metadata={'importance': 0.075, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 36, 47216), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 36, 47216), 'buffer_idx': 4}, page_content='Johnson is support strong national identity and sovereignty')]\n\nFarage's stored memories:\n[Document(metadata={'importance': 0.09, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 36, 523067), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 36, 523067), 'buffer_idx': 0}, page_content='Farage attended Dulwich College but did not attend university'), Document(metadata={'importance': 0.105, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 36, 968215), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 36, 968215), 'buffer_idx': 1}, page_content='Farage is strongly Eurosceptic, advocates for British sovereignty, deregulation, and cutting ties with EU economic policies'), Document(metadata={'importance': 0.075, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 38, 473512), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 38, 473512), 'buffer_idx': 2}, page_content='Farage prioritizes domestic industry and independence from European influence'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 38, 867443), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 38, 867443), 'buffer_idx': 3}, page_content='Farage is a Nationalist, anti-globalist, and socially conservative'), Document(metadata={'importance': 0.105, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 39, 307591), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 39, 307591), 'buffer_idx': 4}, page_content='Farage advocates for strict immigration controls and promotes traditional British values')]\n\nSunak's stored memories:\n[Document(metadata={'importance': 0.12, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 39, 777151), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 39, 777151), 'buffer_idx': 0}, page_content='Sunak studied Philosophy, Politics, and Economics at Oxford University and later earned an MBA from Stanford University'), Document(metadata={'importance': 0.03, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 40, 140897), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 40, 140897), 'buffer_idx': 1}, page_content='Sunak is fiscal conservative with a focus on budget balancing'), Document(metadata={'importance': 0.06, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 40, 521389), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 40, 521389), 'buffer_idx': 2}, page_content='Sunak advocates for responsible spending and a cautious approach to government intervention'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 41, 18200), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 41, 18200), 'buffer_idx': 3}, page_content='Sunak prioritizes stability over drastic reforms'), Document(metadata={'importance': 0.045, 'last_accessed_at': datetime.datetime(2024, 12, 6, 15, 26, 41, 457066), 'created_at': datetime.datetime(2024, 12, 6, 15, 26, 41, 457066), 'buffer_idx': 4}, page_content='Sunak focus on pragmatism over ideology, holds relatively conservative views on social issues, often supporting traditional family values')]\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"# Create Simulation","metadata":{}},{"cell_type":"code","source":"# List of agents in the debate\nagents = [Trott, Johnson, Farage, Sunak, Starmer]\n# Define the initial debate topic\ninitial_observation = \"Should the UK rejoin the European Union?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T14:55:25.798419Z","iopub.execute_input":"2024-12-06T14:55:25.799378Z","iopub.status.idle":"2024-12-06T14:55:25.803645Z","shell.execute_reply.started":"2024-12-06T14:55:25.799335Z","shell.execute_reply":"2024-12-06T14:55:25.802604Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Framework X\nArchived frameworks that DOES NOT work.\n`run_HoC_debate_framework_X1`:\n- Use the generate_reaction() function to decide \"what would be an appropriate reaction?\", and if it chooses to say something, call up the generate_dialogue_response() function to generate a response text.\n- Results: agents does not respond, agent doesn not engage in dialogue, because the prompt is \"*what would be an appropriate reaction*?\"\n","metadata":{}},{"cell_type":"code","source":"def run_HoC_debate_framework_X (agents: List[GenerativeAgent],\ninitial_observation: str) -> None:\n    \"\"\"Runs a conversation where each agent strictly chooses to either react or say a response.\"\"\"\n    \n    observation = initial_observation  # Initial observation passed into the conversation loop\n    turns = 0  # Counter to limit turns or control flow as needed\n    \n    # Loop through rounds of conversation\n    while turns < 3:  # Set a suitable limit for the number of rounds\n        \n        for agent in agents:\n            # Generate a reaction or response to the current observation\n            continue_dialogue, reaction_or_response = agent.generate_reaction(observation)\n            if continue_dialogue == False:\n                # Agent chooses to react - print reaction, but do not change observation\n                print(reaction_or_response)\n            elif continue_dialogue == True:\n                # Agent chooses to say something - print and update observation for next agent\n                stay_in_dialogue, response_text = agent.generate_dialogue_response(observation)\n                print(response_text)\n            else:\n                # Skip any output that is not a strict \"REACT\" or \"SAY\"\n                print(f\"{agent.name} output ignored as it did not conform to 'REACT' or 'SAY'\")\n        turns += 1  # Increment the turn count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:05:16.054148Z","iopub.execute_input":"2024-12-06T15:05:16.054598Z","iopub.status.idle":"2024-12-06T15:05:16.061967Z","shell.execute_reply.started":"2024-12-06T15:05:16.054562Z","shell.execute_reply":"2024-12-06T15:05:16.060852Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Run the debate\nrun_HoC_debate_framework_X (agents, initial_observation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:05:17.516165Z","iopub.execute_input":"2024-12-06T15:05:17.517111Z","iopub.status.idle":"2024-12-06T15:06:21.935456Z","shell.execute_reply.started":"2024-12-06T15:05:17.517063Z","shell.execute_reply":"2024-12-06T15:06:21.934023Z"}},"outputs":[{"name":"stdout","text":"Laura Trott would likely approach this question with caution and a focus on pragmatic economic analysis before making a decision.\nBoris Johnson would likely not support the UK rejoining the European Union, given his strong stance on national identity, sovereignty, and support for Brexit.\nNigel Farage It is unlikely that Nigel Farage would support the UK rejoining the European Union, as his staunch Eurosceptic views and belief in British sovereignty align with his opposition to EU policies and influence.\nRishi Sunak would likely oppose rejoining the European Union due to his conservative views on sovereignty and financial independence.\nSir Keir Starmer Sir Keir Starmer's reaction would be to support a thorough and transparent debate on the matter of rejoining the European Union.\nLaura Trott would likely approach this question with caution and a focus on pragmatic economic analysis before making a decision.\nBoris Johnson would likely not support the UK rejoining the European Union, given his strong stance on national identity, sovereignty, and support for Brexit.\nNigel Farage It is unlikely that Nigel Farage would support the UK rejoining the European Union, as his staunch Eurosceptic views and belief in British sovereignty align with his opposition to EU policies and influence.\nRishi Sunak would likely oppose rejoining the European Union due to his conservative views on sovereignty and financial independence.\nSir Keir Starmer Sir Keir Starmer's reaction would be to support a thorough and transparent debate on the matter of rejoining the European Union.\nLaura Trott would likely approach this question with caution and a focus on pragmatic economic analysis before making a decision.\nBoris Johnson would likely not support the UK rejoining the European Union, given his strong stance on national identity, sovereignty, and support for Brexit.\nNigel Farage It is unlikely that Nigel Farage would support the UK rejoining the European Union, as his staunch Eurosceptic views and belief in British sovereignty align with his opposition to EU policies and influence.\nRishi Sunak would likely oppose rejoining the European Union due to his conservative views on sovereignty and financial independence.\nSir Keir Starmer Sir Keir Starmer's reaction would be to support a thorough and transparent debate on the matter of rejoining the European Union.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## Framework 1\nAgents reply in fixed-order, with a set-limit of 'turns'","metadata":{}},{"cell_type":"code","source":"def run_HoC_debate_framework_1 (agents: List[GenerativeAgent],             # get a list of agents\n                     initial_observation: str) -> None:         # get the 1st observation\n    \"\"\"Runs a conversation between agents.\"\"\"\n    _, observation = agents[4].generate_reaction(initial_observation)   # generate a reaction to observation\n    print(observation)\n    \n    max_turns = 2\n    turns = 0\n    \n    # Enters a loop where agents take turns generating responses\n    while turns < max_turns:\n        for agent in agents:\n            # Each agent generates a response to the latest observation\n            spoken_response, observation = agent.generate_dialogue_response(observation)\n            print(observation)\n            if not spoken_response:\n                print(f\"{agent.name} chose not to respond.\")\n                \n        # Increment the turn count after each full round of responses\n        turns += 1","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the debate\nrun_HoC_debate_framework_1 (agents, initial_observation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Framework 2\nEach agent gets `X`-number of speaking slots allocated randomly","metadata":{}},{"cell_type":"code","source":"def run_HoC_debate_framework_2 (agents: List[GenerativeAgent],\n                              initial_observation: str) -> None:\n    \"\"\"Runs a conversation between agents, each getting X-number of speaking slots allocated randomly.\"\"\"\n    \n    # Initialize the count of speaking slots for each agent\n    max_slot_each = 2\n    speaking_slots = {agent.name: 0 for agent in agents}\n    max_speaking_slots = max_slot_each * len(agents)\n    turns = 0\n    \n    # Start the debate with an initial observation\n    observation = initial_observation\n    print(observation)\n    \n    # Continue the conversation until each agent has spoken twice\n    while sum(speaking_slots.values()) < max_speaking_slots:\n        # Randomly select an agent who hasn't spoken twice yet\n        agent = random.choice([agent for agent in agents if speaking_slots[agent.name] < max_slot_each])\n        # Each agent generates a response to the latest observation\n        stay_in_dialogue, observation = agent.generate_dialogue_response(observation)\n        print(observation)\n        # Increment the speaking slot count for the agent\n        speaking_slots[agent.name] += 1\n        # Increment the turn count\n        turns += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:14:24.715820Z","iopub.execute_input":"2024-12-06T15:14:24.716287Z","iopub.status.idle":"2024-12-06T15:14:24.724120Z","shell.execute_reply.started":"2024-12-06T15:14:24.716236Z","shell.execute_reply":"2024-12-06T15:14:24.722993Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# Run the debate\nrun_HoC_debate_framework_2 (agents, initial_observation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T15:27:47.239855Z","iopub.execute_input":"2024-12-06T15:27:47.240341Z","iopub.status.idle":"2024-12-06T15:27:52.624717Z","shell.execute_reply.started":"2024-12-06T15:27:47.240304Z","shell.execute_reply":"2024-12-06T15:27:52.623272Z"}},"outputs":[{"name":"stdout","text":"Should the UK rejoin the European Union?\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the debate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_HoC_debate_framework_2\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_observation\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[49], line 17\u001b[0m, in \u001b[0;36mrun_HoC_debate_framework_2\u001b[0;34m(agents, initial_observation)\u001b[0m\n\u001b[1;32m     15\u001b[0m agent \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice([agent \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents \u001b[38;5;28;01mif\u001b[39;00m speaking_slots[agent\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m<\u001b[39m max_slot_each])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Each agent generates a response to the latest observation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m stay_in_dialogue, observation \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dialogue_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(observation)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Increment the speaking slot count for the agent\u001b[39;00m\n","Cell \u001b[0;32mIn[9], line 165\u001b[0m, in \u001b[0;36mGenerativeAgent.generate_dialogue_response\u001b[0;34m(self, observation, now)\u001b[0m\n\u001b[1;32m    162\u001b[0m full_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_reaction(observation, call_to_action_template, now\u001b[38;5;241m=\u001b[39mnow)\n\u001b[1;32m    163\u001b[0m result \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.*?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, full_result)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mmatches\u001b[49m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response generated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m result \u001b[38;5;241m=\u001b[39m matches[\u001b[38;5;241m0\u001b[39m]\n","\u001b[0;31mNameError\u001b[0m: name 'matches' is not defined"],"ename":"NameError","evalue":"name 'matches' is not defined","output_type":"error"}],"execution_count":64},{"cell_type":"markdown","source":"## Framework 3\n1. Each agent add new-observation into memory. \n2. Each agent does a quick reflection on this new-observation, to whether to \"respond or not respond\" - depending on personal saliency (a custom function within the class `GenerativeAgent`). Output `decide_to_respond` as either True or False\n3. Randomly select one agent from the list of agents that decide to respond to the observation.\n4. Print this selected generate_dialogue_response as the new observation.","metadata":{}},{"cell_type":"code","source":"# Testing the `decide_to_respond()` function for each agent\nrandom_observation = \"Should the official UK national dish be changed??\"\nrandom_observation = initial_observation\n# Who would respond to the observation? Trott, Johnson, Farage, Sunak, Starmer\nprint(Trott.decide_to_respond(random_observation))\nprint(Johnson.decide_to_respond(random_observation))\nprint(Farage.decide_to_respond(random_observation))\nprint(Sunak.decide_to_respond(random_observation))\nprint(Starmer.decide_to_respond(random_observation))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T18:27:47.371850Z","iopub.execute_input":"2024-12-05T18:27:47.372209Z","iopub.status.idle":"2024-12-05T18:27:47.567655Z","shell.execute_reply.started":"2024-12-05T18:27:47.372176Z","shell.execute_reply":"2024-12-05T18:27:47.566494Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m random_observation \u001b[38;5;241m=\u001b[39m initial_observation\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Who would respond to the observation? Trott, Johnson, Farage, Sunak, Starmer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mTrott\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecide_to_respond\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_observation\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(Johnson\u001b[38;5;241m.\u001b[39mdecide_to_respond(random_observation))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(Farage\u001b[38;5;241m.\u001b[39mdecide_to_respond(random_observation))\n","Cell \u001b[0;32mIn[9], line 189\u001b[0m, in \u001b[0;36mGenerativeAgent.decide_to_respond\u001b[0;34m(self, observation, now, threshold)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecide_to_respond\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: \u001b[38;5;28mstr\u001b[39m, now: Optional[datetime] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m                       threshold: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decide whether the agent wants to respond to the observation.\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     call_to_action_template \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider the following discussion:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 189\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43magent_name\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, on a scale of 1 to 10, how relevant is this discussion to you? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide a number between 1 (not relevant at all) and 10 (extremely relevant).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput only a number, nothing else!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    194\u001b[0m     full_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_reaction(observation, call_to_action_template, now\u001b[38;5;241m=\u001b[39mnow)\n\u001b[1;32m    195\u001b[0m     result \u001b[38;5;241m=\u001b[39m full_result\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# Normalize result to lowercase for consistent comparison\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'agent_name' is not defined"],"ename":"NameError","evalue":"name 'agent_name' is not defined","output_type":"error"}],"execution_count":19},{"cell_type":"code","source":"# Testing the `generate_dialogue_response()` function for each agent\nTrott.generate_dialogue_response(initial_observation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_HoC_debate_framework_3 (agents: List[GenerativeAgent],             # get a list of agents\n                     initial_observation: str) -> None:         # get the 1st observation\n    \"\"\"Runs a conversation between agents, until a maximum number of turns is reached.\"\"\"\n    \n    max_turns = 10\n    turns = 0\n    \n    # Start the debate with an initial observation\n    observation = initial_observation\n    print(observation)\n    \n    # Enters a loop where agents take turns generating responses\n    while turns < max_turns:\n        \n        # Step 1: Each agent adds the new observation into memory\n        for agent in agents:\n            agent.memory.add_memory(observation)\n            \n        # Step 2: Randomly select one agent from the list of agents that decide to respond to the observation\n        responding_agents = [agent for agent in agents if agent.decide_to_respond(observation)]\n        if responding_agents:\n            agent = random.choice(responding_agents)\n            # The selected agent generates a response to the latest observation\n            stay_in_dialogue, observation = agent.generate_dialogue_response(observation)\n            print(observation)\n            \n        # Increment the turn count after each full round of responses\n        turns += 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_HoC_debate_framework_3(agents, initial_observation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save Conversation into a text file\nimport sys\nsys.stdout = open(\"HoC_debate_framework_3_output.txt\", \"w\")\nrun_HoC_debate_framework_3 (agents, initial_observation)\nsys.stdout.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}