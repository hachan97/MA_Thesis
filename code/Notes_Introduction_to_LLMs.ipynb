{"cells":[{"source":"# Positional Encoding Mechanism","metadata":{},"id":"7607bdb2-1707-4361-a984-1c443fe84370","cell_type":"markdown"},{"source":"# Write and run code here\n\nclass PositionalEncoder(nn.Module):\n    def __init__(self, d_model, max_length):\n        super(PositionalEncoder, self).__init__()\n        self.d_model = d_model\n        self.max_length = max_length\n        \n        # Initialize the positional encoding matrix\n        pe = torch.zeros(max_length, d_model)  # a function used to create a tensor filled with zeros        \n        \n        # Calculates the positional encoding for each position in the sequence and assigns the values to the pe matrix:\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) \n            # creates a tensor position containing a sequence of numbers from 0 to max_seq_length-1\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model)) \n            #calculates a term used for scaling the positional encoding values based on their position in the embedding dimension\n        \n        # Calculate and assign position encodings to the matrix:\n        pe[:, 0::2] = torch.sin(position * div_term) # assigns sine values as positional encodings for even dimensions\n        pe[:, 1::2] = torch.cos(position * div_term) # assigns cosine values as positional encodings for odd dimensions\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    # Update the embeddings tensor adding the positional encodings\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)] #slice & inject positional information into the embeddings\n        return x\n","metadata":{},"id":"7f26dd6a-20aa-4d19-8d3f-b8f64fba1821","cell_type":"code","execution_count":1,"outputs":[]},{"source":"- \tThe code that translates the positional encoding mechanism into a usable format for the computer. \t \t\n\nWhat is 'Classes' in Python:  \t\t\n- \tThey define the properties (variables) and functionalities (methods) that objects of that class will have \t\t\n- \tUnlike functions, classes don't directly perform actions. They define a structure for objects. \n- \tA class is more like a cookbook category, like \"desserts.\" It defines what a dessert is in general (properties) and might include various recipes (methods) for different desserts (objects)","metadata":{},"cell_type":"markdown","id":"ad1a1221-da5a-4938-b1d3-421edfa4d4f0"},{"source":"# Multi Head Attention","metadata":{},"cell_type":"markdown","id":"c502c781-bfbf-495e-924b-0260a2b52cbf"},{"source":"class MultiHeadAttention(nn.Module):\n\n    def __init__(self, d_model, num_heads):\n        \n        super(MultiHeadAttention, self).__init__()\n        \n        # Set the number of attention heads\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.head_dim = d_model // num_heads\n        \n\t\t# Set up the linear transformations\n        self.query_linear = nn.Linear(d_model, d_model) #transforms the input embeddings into queries \n        self.key_linear = nn.Linear(d_model, d_model)   #transforms the input embeddings into keys\n        self.value_linear = nn.Linear(d_model, d_model) #transforms the input embeddings into values\n        self.output_linear = nn.Linear(d_model, d_model)\n    \n    def split_heads(self, x, batch_size):\n        # Split the sequence embeddings in x across the attention heads\n        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n\n        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n        \n    def compute_attention(self, query, key, mask=None):\n        # Compute dot-product attention scores\n        scores = torch.matmul(query, key.permute(1, 2, 0))           # matmul = matrix multiplication\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n\n        # Normalize attention scores into attention weights\n        attention_weights = F.softmax(scores, dim=-1)\n        return attention_weights","metadata":{},"cell_type":"code","id":"aad1790c-5750-41c8-83a9-193188b372c4","outputs":[],"execution_count":null},{"source":"This class sets up the building blocks for performing multi-head attention, which allows the model to focus on different aspects of the input sequence simultaneously.","metadata":{},"cell_type":"markdown","id":"cce8c06d-c207-4360-937a-da1c6cc4f51c"},{"source":"# Feed Forward Sub Layer","metadata":{},"cell_type":"markdown","id":"2bcb5a5a-5257-4f71-8421-adf32394865a"},{"source":"class FeedForwardSubLayer(nn.Module):\n    # Specify the two linear layers' input and output sizes\n    def __init__(self, d_model, d_ff):\n        super(FeedForwardSubLayer, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n\t# Apply a forward pass\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{},"cell_type":"code","id":"40237f25-7087-499f-902a-c10690631c44","outputs":[],"execution_count":null},{"source":"- Specify in the __init__() method the sizes of the two linear fully connected layers. \n- Apply a forward pass through the two linear layers, using the ReLU() activation in between.","metadata":{},"cell_type":"markdown","id":"f6958ec1-efe4-4457-a794-45636333ebed"},{"source":"# Encoder Transformers","metadata":{},"cell_type":"markdown","id":"926ce9cd-a493-4039-b241-454f98e687d3"},{"source":"##  A Encoder Layer","metadata":{},"cell_type":"markdown","id":"215f9b6a-f361-45b3-b05d-582cf87c9dd5"},{"source":"# Complete the initialization of elements in the encoder layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        return self.norm2(x + self.dropout(ff_output))","metadata":{},"cell_type":"code","id":"a493e7a6-d8ee-47aa-a151-9e8f81629cd8","outputs":[],"execution_count":null},{"source":"Complete the implementation of the EncoderLayer class to initialize all its inner elements one by one.","metadata":{},"cell_type":"markdown","id":"ce8e5cf4-6f93-44c6-99c4-9fc478b75c8c"},{"source":"## Encoder transformer body and head","metadata":{},"cell_type":"markdown","id":"4ed3e7c2-5951-4a48-bd89-0dbb8f665538"},{"source":"#BODY\nclass TransformerEncoder(nn.Module): \n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\\\\\n        \n        # Define a stack of multiple encoder layers\n        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\t\n    # Complete the forward pass method\n    def forward(self, x, mask):\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\n#HEAD\nclass ClassifierHead(nn.Module): \n    def __init__(self, d_model, num_classes):\n        super(ClassifierHead, self).__init__()\n        # Add linear layer for multiple-class classification\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        logits = self.fc(x[:, 0, :])\n        # Obtain log class probabilities upon raw outputs\n        return F.log_softmax(logits, dim=-1)","metadata":{},"cell_type":"code","id":"1e893613-dc75-4dc0-a2e9-b2560b25dc1b","outputs":[],"execution_count":null},{"source":"- Define a stack of multiple encoder layers in the __init__() method.\n- Complete the forward() method. Note that the process starts by converting the original sequence tokens in x into embeddings.\n\n- Add final linear layer to project encoder results into raw classification outputs.\n- Apply the necessary function to map raw classification outputs into log class probabilities.","metadata":{},"cell_type":"markdown","id":"acb20a2c-b399-430e-8a7b-1baeb0c931b0"},{"source":"## Testing the encoder transformer","metadata":{},"cell_type":"markdown","id":"0b14aea8-a7f9-421a-a295-82b9163d0268"},{"source":"input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\nmask = torch.randint(0, 2, (sequence_length, sequence_length))\n\n# Instantiate the encoder transformer's body and head\nencoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n\nclassifier = ClassifierHead(d_model, num_classes)\n\n# Complete the forward pass \noutput = encoder(input_sequence, mask)\nclassification = classifier(output)\n\nprint(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\nprint(classification)","metadata":{},"cell_type":"code","id":"3f92aae9-59ef-474c-9378-017323a54dda","outputs":[],"execution_count":null},{"source":"`output:\n    Classification outputs for a batch of  8 sequences:\n    tensor([[ 0.3724,  0.0636,  0.5129],\n            [-0.1837,  0.5669, -0.9256],\n            [-0.1848, -0.2706,  0.1537],\n            [ 0.0478,  0.2004, -0.2376],\n            [ 0.6299,  0.4149,  0.2964],\n            [ 1.3734, -0.0549, -0.0309],\n            [-0.0408,  0.3052, -0.1994],\n            [ 0.5111,  0.5409,  0.2535]], grad_fn=<AddmmBackward0>)`","metadata":{},"cell_type":"markdown","id":"fa521095-f4c7-49b4-943a-ba8ac582c47a"},{"source":"# Decoder Transformers","metadata":{},"cell_type":"markdown","id":"56cd54f9-37d7-4afb-8b98-1f3c3c60f7cc"},{"source":"## Building a decoder body and head","metadata":{},"cell_type":"markdown","id":"19401c91-3fbf-4e50-96b3-74a83fe4537b"},{"source":"class TransformerDecoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n        super(TransformerDecoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        # Add a linear layer (head) for next-word prediction\n        self.fc = nn.Linear(d_model, vocab_size)\n\n    def forward(self, x, self_mask):\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        for layer in self.layers:\n            x = layer(x, self_mask)\n\n        # Apply the forward pass through the model head\n        x = self.fc(x)\n        return F.log_softmax(x, dim=-1)","metadata":{},"cell_type":"code","id":"9b024b6c-b5a3-495e-a84a-234cfe003bc5","outputs":[],"execution_count":null},{"source":"## Testing the decoder transformer","metadata":{},"cell_type":"markdown","id":"9e16414f-9cfd-4dc3-a7a8-6beb319f330a"},{"source":"input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n\n# Create a triangular attention mask for causal attention\nself_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n\n# Instantiate the decoder transformer\ndecoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n\noutput = decoder(input_sequence, self_attention_mask)\nprint(output.shape)\nprint(output)","metadata":{"executionCancelledAt":null,"executionTime":1990,"lastExecutedAt":1716997916907,"lastExecutedByKernel":"55235b69-5c9c-4856-83f3-ac757a63f33f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n\n# Create a triangular attention mask for causal attention\nself_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n\n# Instantiate the decoder transformer\ndecoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n\noutput = decoder(input_sequence, self_attention_mask)\nprint(output.shape)\nprint(output)"},"cell_type":"code","id":"a5c3d043-f979-4028-b162-75f46247470b","outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, vocab_size, (batch_size, sequence_length))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a triangular attention mask for causal attention\u001b[39;00m\n\u001b[1;32m      4\u001b[0m self_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, sequence_length, sequence_length), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mbool()\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined"}],"execution_count":null},{"source":"# Combined Transformers","metadata":{},"cell_type":"markdown","id":"2007887b-93b3-4644-a9f1-49e63ac1e726"},{"source":"## Incorporating cross-attention in a decoder","metadata":{},"cell_type":"markdown","id":"dd30fff4-32b3-4bd8-8622-7a6df7ddb599"},{"source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        \n        # Initialize the causal (masked) self-attention and cross-attention\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, causal_mask, encoder_output, cross_mask):\n        # Pass the necessary arguments to the causal self-attention and cross-attention\n        self_attn_output = self.self_attn(x, x, x, causal_mask)\n        x = self.norm1(x + self.dropout(self_attn_output))\n        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n        x = self.norm2(x + self.dropout(cross_attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x","metadata":{},"cell_type":"code","id":"ef32c404-6e05-4a48-924c-c72b70dab1b9","outputs":[],"execution_count":null},{"source":"## Testing out an encoder-decoder transformer","metadata":{},"cell_type":"markdown","id":"94e7c705-1a10-47dd-8d8b-e3d9b852403e"},{"source":"# Create a batch of random input sequences\ninput_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\npadding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\ncausal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n\n# Instantiate the two transformer bodies\nencoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\ndecoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n\n# Pass the necessary masks as arguments to the encoder and the decoder\nencoder_output = encoder(input_sequence, padding_mask)\ndecoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\nprint(\"Batch's output shape: \", decoder_output.shape)","metadata":{},"cell_type":"code","id":"b6f9b05e-cd5e-4744-8761-c7f707d4928c","outputs":[],"execution_count":null},{"source":"# Transformer assembly bottom-up","metadata":{},"cell_type":"markdown","id":"f02a6852-e9f6-4c14-b962-169dd622427c"},{"source":"Putting together the main building blocks of an encoder-only transformer architecture, using a bottom-up approach.\n\nThe following classes, their attributes, and their core functions have been defined for you:\n- `PositionalEncoding(nn.Module)`: positional encoding for input embeddings.\n- `MultiHeadAttention(nn.Module)`: multi-head attention layer.\n- `FeedForward(nn.Module)`: feed-forward layer.\n- `EncoderLayer(nn.Module)`: a replicable encoder layer that glues together multi-head attention and feed-forward layers, along with layer normalizations and dropouts.\n\nYour next task is to finalize assembling the highest-level components of the encoder transformer: the `TransformerEncoder` and `Transformer` classes.","metadata":{},"cell_type":"markdown","id":"6ea656d6-7b25-4d7f-9a19-db40ff796cf2"},{"source":"# Initialize positional encoding layer and stack of EncoderLayer modules\nclass TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.dropout = nn.Dropout(dropout)\n  \n    def forward(self, x, mask):\n        x = self.embedding(x)\n        x = self.positional_encoding(x)\n        x = self.dropout(x)\n        \n        # Pass the sequence through each layer in the encoder\n        for layer in self.layers:\n            x = layer(x, mask)\n        \n        return x\n\nclass Transformer(nn.Module):\n    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n        super(Transformer, self).__init__()\n        # Initialize the encoder stack of the Transformer\n        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n        \n    def forward(self, src, src_mask):\n        encoder_output = self.encoder(src, src_mask)\n        return encoder_output","metadata":{},"cell_type":"code","id":"c26ccbbc-e044-4192-b3e9-9b8f60d7eefc","outputs":[],"execution_count":null},{"source":"Add the whole stack of components and layers into a Transformer class object: You'll need to initialize an attribute containing the whole encoder stack.","metadata":{},"cell_type":"markdown","id":"aca37f45-22ef-4a15-8002-80518b30b371"},{"source":"# Harnessing Pre-trained LLMs","metadata":{},"cell_type":"markdown","id":"226711fd-5425-4f02-8592-42aa57f8248b"},{"source":"## Text Generation","metadata":{},"cell_type":"markdown","id":"a719e046-136b-4056-aff8-4f5dcd542bfc"},{"source":"# Load the tokenizer and pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\ntext = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n\n# Tokenize the inputs and pass them to the LLM to perform classification inference.\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True)\noutputs = model(**inputs)\nlogits = outputs.logits\n\npredicted_classes = torch.argmax(logits, dim=1).tolist()\nfor idx, predicted_class in enumerate(predicted_classes):\n    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")","metadata":{},"cell_type":"code","id":"d25834e6-25fa-4188-a52a-213c23db4c46","outputs":[],"execution_count":null},{"source":"## Text Summarization","metadata":{},"cell_type":"markdown","id":"435b5d96-b30b-4364-a7ef-91f692e5219a"},{"source":"print(f\"Number of instances: {len(dataset['train'])}\")\n\n# Show the names of features in the training fold of the dataset\nprint(f\"Feature names: {dataset['train'].column_names}\")\n\n# Encode the input example, obtain the summary, and decode it\nexample = dataset['train'][-2]['review_sents']\ninput_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n\nsummary_ids = model.generate(input_ids, max_length=150)\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nprint(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\nprint(\"\\nGenerated Summary: \\n\", summary)","metadata":{},"cell_type":"code","id":"3733598d-4a79-4463-b1db-9f05f84efa55","outputs":[],"execution_count":null},{"source":"- Display the names of the features in the data, by accessing the downloaded 'train' fold.\n- Use the necessary variables and methods to encode the input example, pass it to the model to generate a summary, and decode the summary.","metadata":{},"cell_type":"markdown","id":"60d9ad3d-f580-459d-b4fa-452133c58d23"},{"source":"## Text Translation","metadata":{},"cell_type":"markdown","id":"045eb2e5-8276-4f66-b302-c917ff8cae96"},{"source":"model_name = \"Helsinki-NLP/opus-mt-en-es\"\n\n# Load the tokenizer and the model checkpoint\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nenglish_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n\n# Encode the inputs, generate translations, decode, and print them\nfor english_input in english_inputs:\n    input_ids = tokenizer.encode(english_input, return_tensors = \"pt\")\n    translated_ids = model.generate(input_ids) \n    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n    print(f\"English: {english_input} | Spanish: {translated_text}\")","metadata":{},"cell_type":"code","id":"cb1c1ca1-da45-49fd-926c-408a043063a4","outputs":[],"execution_count":null},{"source":"- Use the appropriate task-specific classes and methods to load the tokenizer and the model (the classes needed have been already imported for you, as usual!).\n- Complete the instructions to encode the input sequences, generate translations, and decode them. For encodings, use an extra argument to return them as PyTorch tensors.","metadata":{},"cell_type":"markdown","id":"54b58084-d72b-4885-97ae-1c4cf9483159"},{"source":"## Q&A Task","metadata":{},"cell_type":"markdown","id":"e6a81a17-301b-4da3-9013-95a8fa415b3c"},{"source":"# Load a specific subset of the dataset \nfrom datasets import load_dataset\nmlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n\nquestion = mlqa[\"test\"][\"question\"][0]\ncontext = mlqa[\"test\"][\"context\"][0]\nprint(\"Question: \", question)\nprint(\"Context: \", context)\n\n# Initialize the tokenizer using the model checkpoint\ntokenizer = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n\n# Tokenize the inputs returning the result as tensors\ninputs = tokenizer(question, context, return_tensors =\"pt\")\nprint(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])\n\n## Extract and decode the answer ##\n# Initialize the LLM upon the model checkpoint\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n\nwith torch.no_grad():         # context manager disables gradient calculation\n  outputs = model(**inputs)   # passes the inputs through the model\n\n# Get the most likely start and end answer position from the raw LLM outputs\nstart_idx = torch.argmax(outputs.start_logits)\nend_idx = torch.argmax(outputs.end_logits) + 1\n\n# Access the tokenized inputs tensor to get the answer span\nanswer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n\n# Decode the answer span to get the extracted answer text\nanswer = tokenizer.decode(answer_span)\nprint(\"Answer: \", answer)","metadata":{},"cell_type":"code","id":"1121d9d9-0aa0-448c-9015-218876b054c0","outputs":[],"execution_count":null},{"source":"# Fine-Tuning and Transfer Learning","metadata":{},"cell_type":"markdown","id":"f177b0b0-251f-4149-b611-68255c3712ec"},{"source":"# Load a pre-trained LLM, specifying its use for binary classification\nmodel = AutoModelForSequenceClassification.from_pretrained(model = \"distilbert-base-uncased\", num_labels=2)\n\n# Set up training arguments with a batch size of 8 per GPU and 5 epochs\ntraining_args = TrainingArguments(\n    output_dir=\"./smaller_bert_finetuned\",\n    per_device_train_batch_size=8,\n    num_train_epochs=5,\n)\n# Set up trainer, assigning previously set up training arguments\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n)","metadata":{},"cell_type":"code","id":"2bb25a6b-a4d2-46c6-86fc-0eb9172b9fbf","outputs":[],"execution_count":null},{"source":"# Initialize the trainer and assign a training and validation set to it\ntrainer = Trainer(model=model, args=training_args,\n    \t\t\tcompute_metrics=compute_metrics,\n    \t\t\ttrain_dataset=emotions_encoded[\"train\"],\n    \t\t\teval_dataset=emotions_encoded[\"validation\"],\n    \t\t\ttokenizer=tokenizer\n)\n\n# Training loop to fine-tune the model\n#trainer.train()\n\ninput_texts = [\"It's dark and rainy outside\", \"I love penguins!\"]\n\n# Tokenize the input sequences and pass them to the model\ninputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Obtain class labels from raw predictions\npredicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n\nfor i, predicted_label in enumerate(predicted_labels):\n    print(f\"\\n Input Text {i + 1}: {input_texts[i]}\")\n    print(f\"Predicted Label: {predicted_label}\")","metadata":{},"cell_type":"code","id":"82a293fc-dc66-4b03-aa04-850f31c5ee02","outputs":[],"execution_count":null},{"source":"# Evaluation of LLM","metadata":{},"cell_type":"markdown","id":"a77c833a-5941-48a1-8d18-bcb2b842ee6d"},{"source":"## Classification Metrics","metadata":{},"cell_type":"markdown","id":"9fb9cfe1-b7a4-4b19-bb2f-04b454d5b7e9"},{"source":"# Pass the four input texts (without labels) to the pipeline\npredictions = sentiment_analysis ([example[\"text\"] for example in test_examples])\n\ntrue_labels = [example[\"label\"] for example in test_examples]\npredicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n\n# Load the accuracy metric\naccuracy = evaluate.load(\"accuracy\")\n\nresult = accuracy.compute(references=true_labels, predictions=predicted_labels)\nprint(result)","metadata":{},"cell_type":"code","id":"f08cf1f7-171d-4fd0-8477-5c6da7a643b2","outputs":[],"execution_count":null},{"source":"# Obtain a description of each metric\nprint(help(accuracy))\nprint(help(precision))\nprint(help(recall))\nprint(help(f1))\n\n# Load the accuracy, precision, recall and F1 score metrics\naccuracy = evaluate.load(\"accuracy\")\nprecision = evaluate.load(\"precision\")\nrecall = evaluate.load(\"recall\")\nf1 = evaluate.load(\"f1\")\n\n# Pass the examples to the pipeline, and obtain a list predicted labels\nsentiment_analysis = pipeline(\"sentiment-analysis\")\npredictions = sentiment_analysis([example for example in test_examples])\n\npredicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\ntest_labels = [1, 1, 1, 1, 0, 0, 0]\n\n# Compute the metrics by comparing real and predicted labels\nprint(precision.compute(references=test_labels , predictions=predicted_labels))\nprint(recall.compute(references=test_labels , predictions=predicted_labels))\nprint(f1.compute(references=test_labels , predictions=predicted_labels))","metadata":{},"cell_type":"code","id":"7aee485b-1088-407e-87e9-5da08ad36c9e","outputs":[],"execution_count":null},{"source":"## Specialized Metrics","metadata":{},"cell_type":"markdown","id":"1d30024d-078d-44cb-bf75-c77e53176a39"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}