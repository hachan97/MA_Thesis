{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10863910,"sourceType":"datasetVersion","datasetId":6361952}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport json\nimport matplotlib.pyplot as plt\nimport datetime\nfrom datasets import Dataset\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:04:24.208035Z","iopub.execute_input":"2025-02-28T14:04:24.208438Z","iopub.status.idle":"2025-02-28T14:04:24.213476Z","shell.execute_reply.started":"2025-02-28T14:04:24.208408Z","shell.execute_reply":"2025-02-28T14:04:24.212381Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n# Login to Hugging Face\nfrom huggingface_hub import login\n\nlogin(Hugging_Face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:04:25.775881Z","iopub.execute_input":"2025-02-28T14:04:25.776263Z","iopub.status.idle":"2025-02-28T14:04:26.345607Z","shell.execute_reply.started":"2025-02-28T14:04:25.776232Z","shell.execute_reply":"2025-02-28T14:04:26.344450Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df_HoC_2000s_raw = pd.read_csv('/kaggle/input/parlspeech/df_HoC_2000s.csv')\nprint(df_HoC_2000s_raw.columns)\n\ndf_HoC_2000s = df_HoC_2000s_raw[['date', 'agenda', 'speechnumber', 'speaker', 'party','text']]\nprint(df_HoC_2000s.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:05:21.066995Z","iopub.execute_input":"2025-02-28T14:05:21.067369Z","iopub.status.idle":"2025-02-28T14:05:40.689291Z","shell.execute_reply.started":"2025-02-28T14:05:21.067340Z","shell.execute_reply":"2025-02-28T14:05:40.688206Z"}},"outputs":[{"name":"stdout","text":"Index(['date', 'agenda', 'speechnumber', 'speaker', 'party', 'party.facts.id',\n       'chair', 'terms', 'text'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df_HoC_2000s.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:06:10.182112Z","iopub.execute_input":"2025-02-28T14:06:10.182562Z","iopub.status.idle":"2025-02-28T14:06:10.193085Z","shell.execute_reply.started":"2025-02-28T14:06:10.182530Z","shell.execute_reply":"2025-02-28T14:06:10.192140Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"         date                                             agenda  \\\n0  2000-01-10  Severe Disablement Allowance [Oral Answers To ...   \n1  2000-01-10  Severe Disablement Allowance [Oral Answers To ...   \n2  2000-01-10  Severe Disablement Allowance [Oral Answers To ...   \n\n   speechnumber        speaker   party  \\\n0             1  Andrew George  LibDem   \n1             2    Hugh Bayley     Lab   \n2             3  Andrew George  LibDem   \n\n                                                text  \n0  What steps the Government are taking to ensure...  \n1  Severe disablement allowance does not provide ...  \n2  Those who warned the Government against abolit...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>agenda</th>\n      <th>speechnumber</th>\n      <th>speaker</th>\n      <th>party</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2000-01-10</td>\n      <td>Severe Disablement Allowance [Oral Answers To ...</td>\n      <td>1</td>\n      <td>Andrew George</td>\n      <td>LibDem</td>\n      <td>What steps the Government are taking to ensure...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2000-01-10</td>\n      <td>Severe Disablement Allowance [Oral Answers To ...</td>\n      <td>2</td>\n      <td>Hugh Bayley</td>\n      <td>Lab</td>\n      <td>Severe disablement allowance does not provide ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2000-01-10</td>\n      <td>Severe Disablement Allowance [Oral Answers To ...</td>\n      <td>3</td>\n      <td>Andrew George</td>\n      <td>LibDem</td>\n      <td>Those who warned the Government against abolit...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Exploratory Data Analaysis","metadata":{}},{"cell_type":"code","source":"miniDebate_speakers = df_HoC_2000s[df_HoC_2000s['agenda'].str.contains('Free Movement of EU Nationals', case=False, na=False)]['speaker'].unique()#.iloc[0]['text']\n\nprint('The number of rows each speaker has in the main dataframe: df_HoC_2000s ')\nfor speaker in miniDebate_speakers:\n    print(speaker, df_HoC_2000s[df_HoC_2000s['speaker'] == speaker].shape[0])\n\nprint('The number of rows each speaker has in the main dataframe: df_HoC_2000s (exclude \"Free Movement of EU Nationals\" agenda)')\nfor speaker in miniDebate_speakers:\n    print(speaker, df_HoC_2000s[(df_HoC_2000s['speaker'] == speaker) & (df_HoC_2000s['agenda'] != 'Free Movement of EU Nationals')].shape[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:13:27.071055Z","iopub.execute_input":"2025-02-28T14:13:27.071481Z","iopub.status.idle":"2025-02-28T14:13:29.556014Z","shell.execute_reply.started":"2025-02-28T14:13:27.071448Z","shell.execute_reply":"2025-02-28T14:13:29.554827Z"}},"outputs":[{"name":"stdout","text":"The number of rows each speaker has in the main dataframe: df_HoC_2000s \nChristine Jardine 365\nKit Malthouse 732\nSteve Double 622\nTim Farron 767\nJo Stevens 419\nRachael Maskell 1029\nThe number of rows each speaker has in the main dataframe: df_HoC_2000s (exclude \"Free Movement of EU Nationals\" agenda)\nChristine Jardine 356\nKit Malthouse 726\nSteve Double 620\nTim Farron 765\nJo Stevens 417\nRachael Maskell 1027\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Load Tokenizer","metadata":{}},{"cell_type":"code","source":"BASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:13:53.121171Z","iopub.execute_input":"2025-02-28T14:13:53.121511Z","iopub.status.idle":"2025-02-28T14:13:53.126108Z","shell.execute_reply.started":"2025-02-28T14:13:53.121486Z","shell.execute_reply":"2025-02-28T14:13:53.124958Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Create the tokenizer to measure the length of the text\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, \n                                          add_bos_token=False, \n                                          trust_remote_code=True, \n                                          use_fast=True, \n                                          force_download=False)\n\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntokenizer.model_max_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:13:55.220396Z","iopub.execute_input":"2025-02-28T14:13:55.220749Z","iopub.status.idle":"2025-02-28T14:13:58.471345Z","shell.execute_reply.started":"2025-02-28T14:13:55.220724Z","shell.execute_reply":"2025-02-28T14:13:58.470097Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a191c20ef24225bdeda2fde111d07d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7cabd7dc87488da68b7ec36854531d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45efa423bb2a4935966b72fd5483aa81"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"131072"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"min(tokenizer.vocab.values()), max(tokenizer.vocab.values()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:11:41.708492Z","iopub.execute_input":"2025-02-27T19:11:41.708858Z","iopub.status.idle":"2025-02-27T19:11:41.956147Z","shell.execute_reply.started":"2025-02-27T19:11:41.708827Z","shell.execute_reply":"2025-02-27T19:11:41.955138Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(0, 128256)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"print(tokenizer.special_tokens_map)\n\ntest_text = \"Hello, how are you today?\"\ntokens = tokenizer.encode(test_text, return_tensors=\"pt\")\nprint(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T19:11:43.488099Z","iopub.execute_input":"2025-02-27T19:11:43.488483Z","iopub.status.idle":"2025-02-27T19:11:43.537835Z","shell.execute_reply.started":"2025-02-27T19:11:43.488452Z","shell.execute_reply":"2025-02-27T19:11:43.536874Z"}},"outputs":[{"name":"stdout","text":"{'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'pad_token': '[PAD]'}\ntensor([[128000,   9906,     11,   1268,    527,    499,   3432,     30]])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Prepare Data for Training\n\n**References on Preprocessing Dataset for Fine-tuning**\n- https://pytorch.org/torchtune/0.2/tutorials/chat.html\n- https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7\n\nYes, I would like to modify how many 'user' messages are included before and after each assistant response?","metadata":{}},{"cell_type":"code","source":"# Constants\nTOKEN_LENGTH_LIMIT = 3500\nMIN_TOKEN_LENGTH = 512\n\nMAX_NO_ASSISTANT_THRESHOLD = 10       # Reset conversation if assistant is absent for too long\nCHAT_OWNER = \"Christine Jardine\"\n\nSYSTEM_PROMPT = \"\"\"You are Christine Jardine, a politician in the UK's House of Commons.\n    You are responding to statements in the parliament.\n    Respond exactly as Christine Jardine would speak, \n    staying fully in character and address the observation directly.\"\"\"\n\nSYSTEM_PROMPT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:30:39.164579Z","iopub.execute_input":"2025-02-28T14:30:39.164945Z","iopub.status.idle":"2025-02-28T14:30:39.172017Z","shell.execute_reply.started":"2025-02-28T14:30:39.164920Z","shell.execute_reply":"2025-02-28T14:30:39.170771Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"\"You are Christine Jardine, a politician in the UK's House of Commons.\\n    You are responding to Observations.\\n    Respond exactly as Christine Jardine would speak, \\n    staying fully in character and address the observation directly.\""},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def preprocess_convo_1(df, output_path):\n\n    SYSTEM_PROMPT_TOKEN_LEN = len(tokenizer.encode(SYSTEM_PROMPT))\n\n    df = df.assign(role=df[\"speaker\"].apply(lambda x: \"assistant\" if x == CHAT_OWNER else \"user\"))  # Assign roles\n    grouped = df.groupby([\"date\", \"agenda\"])\n\n    conversations = []\n\n    for (date, agenda), group in tqdm(grouped):\n        conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n        token_len = SYSTEM_PROMPT_TOKEN_LEN\n        has_assistant_message = False\n        user_message_count = 0  \n\n        for _, row in group.iterrows():\n            role, message = row[\"role\"], row[\"text\"]\n            chat_message = {\"role\": role, \"content\": message}\n\n            # Simulate applying chat template before checking length\n            temp_messages = conversation + [chat_message]\n            temp_prompt = tokenizer.apply_chat_template(temp_messages, tokenize=False, add_generation_prompt=True)\n            temp_token_len = len(tokenizer.encode(temp_prompt))\n\n            # Check if adding this message exceeds the token limit\n            if temp_token_len > TOKEN_LENGTH_LIMIT or user_message_count >= MAX_NO_ASSISTANT_THRESHOLD:\n                if token_len >= MIN_TOKEN_LENGTH and has_assistant_message:\n                    conversations.append({\"conversation\": conversation})\n\n                # Restart conversation\n                conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n                token_len = SYSTEM_PROMPT_TOKEN_LEN\n                has_assistant_message = False\n                user_message_count = 0  \n\n            # Append message to conversation\n            conversation.append(chat_message)\n            token_len = temp_token_len  # Update token count\n\n            if role == \"assistant\":\n                has_assistant_message = True\n                user_message_count = 0  \n            else:\n                user_message_count += 1  \n\n        # Save the last conversation if it meets the minimum length and contains an assistant message\n        if token_len >= MIN_TOKEN_LENGTH and has_assistant_message:\n            conversations.append({\"conversation\": conversation})\n\n    # Write to JSONL file\n    with open(output_path, 'w') as f:\n        for convo in conversations:\n            f.write(json.dumps(convo) + '\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:31:23.527344Z","iopub.execute_input":"2025-02-28T14:31:23.527718Z","iopub.status.idle":"2025-02-28T14:31:23.537325Z","shell.execute_reply.started":"2025-02-28T14:31:23.527692Z","shell.execute_reply":"2025-02-28T14:31:23.536266Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"preprocess_convo_1(df_HoC_2000s, output_path = '/kaggle/working/preprocessed_ChristineJardine.jsonl')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:31:54.614509Z","iopub.execute_input":"2025-02-28T14:31:54.614881Z","iopub.status.idle":"2025-02-28T14:33:35.447608Z","shell.execute_reply.started":"2025-02-28T14:31:54.614853Z","shell.execute_reply":"2025-02-28T14:33:35.446038Z"}},"outputs":[{"name":"stderr","text":"  1%|          | 627/51318 [01:37<2:11:31,  6.42it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d652f5303920>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess_convo_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_HoC_2000s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/working/preprocessed_ChristineJardine.jsonl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-22-7c6e653991b8>\u001b[0m in \u001b[0;36mpreprocess_convo_1\u001b[0;34m(df, output_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtemp_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchat_message\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtemp_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtemp_token_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Check if adding this message exceeds the token limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                 method).\n\u001b[1;32m   2626\u001b[0m         \"\"\"\n\u001b[0;32m-> 2627\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2628\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m         )\n\u001b[1;32m   3045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3046\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3047\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     ) -> BatchEncoding:\n\u001b[1;32m    599\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"# Check token lengths in dataset (with proper estimation)\ntotal_tokens = []\nwith open(output_path, 'r') as f:\n    for line in f:\n        data = json.loads(line)\n        \n        # Apply chat template before estimating token count\n        prompt = tokenizer.apply_chat_template(data[\"conversation\"], tokenize=False, add_generation_prompt=True)\n        token_count = len(tokenizer.encode(prompt))  # Tokenize after applying template\n\n        total_tokens.append(token_count)\n\nprint(f\"Min tokens: {min(total_tokens)}, Max tokens: {max(total_tokens)}, Avg tokens: {sum(total_tokens)/len(total_tokens)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T14:33:35.448263Z","iopub.status.idle":"2025-02-28T14:33:35.448589Z","shell.execute_reply":"2025-02-28T14:33:35.448460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"over_limit = [tokens for tokens in total_tokens if tokens > 4096]\nprint(f\"Number of conversations exceeding 4096 tokens: {len(over_limit)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:55:48.115399Z","iopub.execute_input":"2025-02-14T12:55:48.115681Z","iopub.status.idle":"2025-02-14T12:55:48.120384Z","shell.execute_reply.started":"2025-02-14T12:55:48.11566Z","shell.execute_reply":"2025-02-14T12:55:48.11958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the histogram of 'Token Lengths'\nplt.figure(figsize=(10, 6))\nplt.hist(total_tokens, bins=20, edgecolor='black', alpha=0.7)\nplt.xlabel(\"Token Count\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Token Counts in Preprocessed Conversations\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:56:29.859278Z","iopub.execute_input":"2025-02-14T12:56:29.85958Z","iopub.status.idle":"2025-02-14T12:56:30.15Z","shell.execute_reply.started":"2025-02-14T12:56:29.859558Z","shell.execute_reply":"2025-02-14T12:56:30.149056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inspect a Few Samples\ndialog_blocks = []\nwith open(output_path, 'r') as f:\n    for line in f:\n        dialog_blocks.append(json.loads(line))  # Parse each line as a JSON object\n\n# Display the number of dialog blocks\nprint(f\"Total dialog blocks: {len(dialog_blocks)}\")\n#dialog_blocks[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-14T12:56:34.075367Z","iopub.execute_input":"2025-02-14T12:56:34.075653Z","iopub.status.idle":"2025-02-14T12:56:34.089205Z","shell.execute_reply.started":"2025-02-14T12:56:34.075629Z","shell.execute_reply":"2025-02-14T12:56:34.08838Z"}},"outputs":[],"execution_count":null}]}