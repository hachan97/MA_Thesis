{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":224133583,"sourceType":"kernelVersion"},{"sourceId":224219749,"sourceType":"kernelVersion"},{"sourceId":225018323,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# Standard library imports\nimport pandas as pd\nimport os\nimport re\nimport math\nimport json\nimport random\n\n# Third-party imports\nimport torch\nimport tenacity\n\n# Hugging Face imports\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig, pipeline, AutoModel\nfrom peft import PeftModel, PeftConfig\nfrom langchain_huggingface import HuggingFacePipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set API Keys\nfrom kaggle_secrets import UserSecretsClient # API Loggins\nuser_secrets = UserSecretsClient()\n\n## Hugging Face\nHugging_Face_token = user_secrets.get_secret(\"Hugging_Face_token\")\n\n# Login to Hugging Face\nfrom huggingface_hub import login\nlogin(Hugging_Face_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T01:00:47.792904Z","iopub.execute_input":"2025-02-27T01:00:47.793133Z","iopub.status.idle":"2025-02-27T01:00:48.785702Z","shell.execute_reply.started":"2025-02-27T01:00:47.793106Z","shell.execute_reply":"2025-02-27T01:00:48.784799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pairs_TheresaMay = pd.read_csv('/kaggle/input/parlspeech-eda-ipynb/df_pairs_TheresaMay.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T01:02:39.110046Z","iopub.execute_input":"2025-02-27T01:02:39.110769Z","iopub.status.idle":"2025-02-27T01:02:39.322032Z","shell.execute_reply.started":"2025-02-27T01:02:39.110735Z","shell.execute_reply":"2025-02-27T01:02:39.321077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Check if GPU exist, print yes if it does\nif torch.cuda.is_available():\n    print(\"Yes, you have a GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define model paths\nBASE_MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\nFINETUNED_MODEL_PATH_TheresaMay = \"/kaggle/input/finetune-llama-v3-theresamay/kaggle/working/fine-tuned-llama_TheresaMay\"\n\n# Load tokenizer\ntokenizer= AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:21:17.276672Z","iopub.execute_input":"2025-02-26T20:21:17.277021Z","iopub.status.idle":"2025-02-26T20:21:18.145418Z","shell.execute_reply.started":"2025-02-26T20:21:17.276991Z","shell.execute_reply":"2025-02-26T20:21:18.144588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL_ID,\n    quantization_config=bnb_config,\n    #torch_dtype=torch.float16, \n    device_map=\"auto\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:21:29.311412Z","iopub.execute_input":"2025-02-26T20:21:29.311778Z","iopub.status.idle":"2025-02-26T20:22:19.388509Z","shell.execute_reply.started":"2025-02-26T20:21:29.311746Z","shell.execute_reply":"2025-02-26T20:22:19.387597Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load fine-tuned LoRA adapters\nmodel = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_PATH_TheresaMay)\nmodel.eval()  # Set to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:22:40.047933Z","iopub.execute_input":"2025-02-26T20:22:40.048276Z","iopub.status.idle":"2025-02-26T20:22:40.189024Z","shell.execute_reply.started":"2025-02-26T20:22:40.048245Z","shell.execute_reply":"2025-02-26T20:22:40.188019Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run Inferencing\nSYSTEM_PROMPT = \"\"\"\n    You are Theresa May, a Member of Parliament responding in a debate session in the UK House of Commons.\n    Act as Theresa May would, using your distinct voice and perspective.\n    Respond to the statement.\\n\"\n    Ensure that your response is direct, fully in character, and reflects your established views and tone.\n    Respond exactly as Theresa May would speak in this context.\n\"\"\"\n\ndef generate_response(statement: str) -> str:\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": statement}\n    ]\n    \n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n    \n    outputs = model.generate(**inputs, max_new_tokens=300, num_return_sequences=1)\n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated_text = generated_text.replace('\\n', ' ')\n    \n    match = re.search(r\"assistant\\s*(.*)\", generated_text, re.IGNORECASE | re.DOTALL)\n    if match:\n        return match.group(1).strip()\n    else:\n        return generated_text.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T21:30:12.620827Z","iopub.execute_input":"2025-02-26T21:30:12.621221Z","iopub.status.idle":"2025-02-26T21:30:12.898124Z","shell.execute_reply.started":"2025-02-26T21:30:12.621151Z","shell.execute_reply":"2025-02-26T21:30:12.896811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pairs_TheresaMay[\"model_response\"] = df_pairs_TheresaMay[\"prompt\"].apply(\n    lambda statement: generate_response(statement)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T21:30:12.620827Z","iopub.execute_input":"2025-02-26T21:30:12.621221Z","iopub.status.idle":"2025-02-26T21:30:12.898124Z","shell.execute_reply.started":"2025-02-26T21:30:12.621151Z","shell.execute_reply":"2025-02-26T21:30:12.896811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_pairs_TheresaMay.to_csv('/kaggle/working/df_pairs_TheresaMay_simulated_Llama3.2_finetuned.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T21:30:12.898803Z","iopub.status.idle":"2025-02-26T21:30:12.899251Z","shell.execute_reply.started":"2025-02-26T21:30:12.89902Z","shell.execute_reply":"2025-02-26T21:30:12.899042Z"}},"outputs":[],"execution_count":null}]}