{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "import random\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chanho\\AppData\\Local\\Temp\\ipykernel_16220\\1936120068.py:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation - 2 Agents\n",
    "\n",
    "The code was entirely reused from the example by [Mehul Gupta](https://medium.com/data-science-in-your-pocket/multi-agent-conversation-debates-using-langgraph-and-langchain-9f4bf711d8ab) in his Medium's blogpost and the guide provided by [Vadym Barda](https://github.com/langchain-ai/langgraph/tree/main)'s LangGraph Github repository , with a few minor tweaks and proper configuration of the prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debate Topic\n",
    "debate_topic= \"Should the United Kingdom continue providing weapons to Israel?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the classes\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output = llm(\"I wish to have a debate on {}. What would be the fighting sides called? Output just the names and nothing else as comma separated list\".format(debate_topic))\n",
    "classes = output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK Conservative Party'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the classes\n",
    "classes = ['UK Labour Party', 'UK Conservative Party']\n",
    "classes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining state variables in Graph and workflow object\n",
    "class GraphState(TypedDict):\n",
    "    classification: Optional[str] = None\n",
    "    history: Optional[str] = None\n",
    "    current_response: Optional[str] = None\n",
    "    count: Optional[int]=None\n",
    "    results: Optional[str]=None\n",
    "    greeting: Optional[str]=None\n",
    "\n",
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\chanho\\AppData\\Local\\Temp\\ipykernel_16220\\2450468759.py:3: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  prefix_start= 'You are in support of {} . You are in a debate with {} over the topic: {}. This is the conversation so far \\n{}\\n. Put forth your next argument to support {} countering {}.\\ Dont repeat your previous arguments. Give a short, one line answer.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c82b9dac30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Graph Nodes\n",
    "\n",
    "prefix_start= 'You are in support of {} . You are in a debate with {} over the topic: {}. This is the conversation so far \\n{}\\n. Put forth your next argument to support {} countering {}.\\ Dont repeat your previous arguments. Give a short, one line answer.'\n",
    "\n",
    "def classify(question):\n",
    "    return llm(\"classify the sentiment of input as {} or {}. Output just the class. Input:{}\".format('_'.join(classes[0].split(' ')),'_'.join(classes[1].split(' ')),question)).strip()\n",
    "\n",
    "def classify_input_node(state):\n",
    "    question = state.get('current_response')\n",
    "    classification = classify(question)  # Assume a function that classifies the input\n",
    "    return {\"classification\": classification}\n",
    "\n",
    "def handle_greeting_node(state):\n",
    "    return {\"greeting\": \"Hello! Today we will witness the fight between {} vs {}\".format(classes[0],classes[1])}\n",
    "\n",
    "def handle_pro(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    if summary=='Nothing':\n",
    "        prompt = prefix_start.format(classes[0],classes[1],debate_topic,'Nothing',classes[0],\"Nothing\")\n",
    "        argument = classes[0] +\":\"+ llm(prompt)\n",
    "        summary = 'START\\n'\n",
    "    else:\n",
    "        prompt = prefix_start.format(classes[0],classes[1],debate_topic,summary,classes[0],current_response)\n",
    "        argument = classes[0] +\":\"+ llm(prompt)\n",
    "    return {\"history\":summary+'\\n'+argument,\"current_response\":argument,\"count\":state.get('count')+1}\n",
    "\n",
    "def handle_opp(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    prompt = prefix_start.format(classes[1],classes[0],debate_topic,summary,classes[1],current_response)\n",
    "    argument = classes[1] +\":\"+ llm(prompt)\n",
    "    return {\"history\":summary+'\\n'+argument,\"current_response\":argument,\"count\":state.get('count')+1}    \n",
    "\n",
    "def result(state):\n",
    "    summary = state.get('history').strip()\n",
    "    prompt = \"Summarize the conversation and judge who won the debate.No ties are allowed. Conversation:{}\".format(summary)\n",
    "    return {\"results\":llm(prompt)}\n",
    "\n",
    "workflow.add_node(\"classify_input\", classify_input_node)\n",
    "workflow.add_node(\"handle_greeting\", handle_greeting_node)\n",
    "workflow.add_node(\"handle_pro\", handle_pro)\n",
    "workflow.add_node(\"handle_opp\", handle_opp)\n",
    "workflow.add_node(\"result\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Workflow & Connections\n",
    "\n",
    "Define the round/length of debates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c82b9dac30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Conditional Edges\n",
    "def decide_next_node(state):\n",
    "    return \"handle_opp\" if state.get('classification')=='_'.join(classes[0].split(' ')) else \"handle_pro\"\n",
    "\n",
    "# Define Length of Debate\n",
    "def check_conv_length(state):\n",
    "    return \"result\" if state.get(\"count\")==10 else \"classify_input\"           # ---------------------> Define X rounds of debate\n",
    "\n",
    "# Adding Conditional Edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_next_node,\n",
    "    {\n",
    "        \"handle_pro\": \"handle_pro\",\n",
    "        \"handle_opp\": \"handle_opp\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_pro\",\n",
    "    check_conv_length,\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_opp\",\n",
    "    check_conv_length,\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Adding graph entry point and remaining definite edges\n",
    "workflow.set_entry_point(\"handle_greeting\")\n",
    "workflow.add_edge('handle_greeting', \"handle_pro\")\n",
    "workflow.add_edge('result', END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chanho\\AppData\\Local\\Temp\\ipykernel_16220\\2450468759.py:21: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  argument = classes[0] +\":\"+ llm(prompt)\n"
     ]
    }
   ],
   "source": [
    "app = workflow.compile()\n",
    "conversation = app.invoke({'count':0,'history':'Nothing','current_response':''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "\n",
      "UK Labour Party:\n",
      "\n",
      "\"Continuing to provide weapons to Israel only perpetuates the cycle of violence and oppression against Palestinians.\"\n",
      "UK Labour Party:\n",
      "\n",
      "\"Supporting Israel's military only fuels their human rights abuses against Palestinians.\"\n",
      "UK Conservative Party:\n",
      "\n",
      "\"Providing weapons to Israel helps defend against terrorist threats in the region.\"\n",
      "UK Labour Party:\n",
      "\"Arming Israel only escalates tensions and further destabilizes the region.\"\n",
      "UK Conservative Party:\n",
      "\n",
      "\"Israel has a right to defend itself against hostile forces in the region.\"\n",
      "UK Labour Party: \n",
      "\n",
      "\"Providing weapons to Israel does not address the root causes of conflict and only perpetuates the suffering of innocent civilians.\"\n",
      "UK Labour Party:\n",
      "\n",
      "\"Arming Israel goes against our party's values of promoting peace and human rights.\"\n",
      "UK Conservative Party:\n",
      "\n",
      "\"Providing weapons to Israel also supports their right to self-determination and sovereignty.\"\n",
      "UK Labour Party: \n",
      "\n",
      "\"Supporting Israel's military actions only further undermines the chances for a peaceful resolution to the conflict.\"\n",
      "UK Conservative Party: \n",
      "\n",
      "\"Providing weapons to Israel also helps to maintain a balance of power in the region and prevent larger conflicts.\"\n"
     ]
    }
   ],
   "source": [
    "print(conversation['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The conversation revolves around the topic of whether or not the UK should continue to provide weapons to Israel. The UK Labour Party argues that doing so only perpetuates the cycle of violence and human rights abuses against Palestinians, and goes against their party's values of promoting peace and human rights. They also believe that supporting Israel's military only escalates tensions and destabilizes the region. On the other hand, the UK Conservative Party argues that providing weapons to Israel helps defend against terrorist threats and supports their right to self-determination and sovereignty. They also believe that arming Israel helps maintain a balance of power in the region and prevent larger conflicts. \n",
      "\n",
      "Overall, it is difficult to determine who \"won\" the debate as both parties presented valid arguments from their perspectives. However, in terms of promoting peace and human rights, the UK Labour Party may have had a stronger argument. Ultimately, it is up to individuals to decide where they stand on this issue. \n"
     ]
    }
   ],
   "source": [
    "print(conversation[\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation - 3 Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the debate topic\n",
    "debate_topic= \"Should the United Kingdom continue providing weapons to Israel?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK Conservative Party'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the classes\n",
    "classes = ['UK Labour Party', 'UK Conservative Party', 'UK Liberal Democrats Party']\n",
    "classes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining state variables in Graph and workflow object\n",
    "class GraphState(TypedDict):\n",
    "    classification: Optional[str] = None\n",
    "    \n",
    "    history: Optional[str] = None\n",
    "    current_response: Optional[str] = None\n",
    "    count: Optional[int]=None\n",
    "    results: Optional[str]=None\n",
    "    greeting: Optional[str]=None\n",
    "\n",
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c82b6a2d20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulation Node\n",
    "\n",
    "prefix_start = 'You are in support of {}. You are in a debate with {} and {} over the topic: {}. This is the conversation so far \\n{}\\n. Put forth your next argument to support {} countering {} and {}. Don\\'t repeat your previous arguments. Give a short, one line answer.'\n",
    "\n",
    "# Classify the input\n",
    "def classify(question):\n",
    "    return llm(\"classify the sentiment of input as {}, {}, or {}. Output just the class. Input:{}\".format('_'.join(classes[0].split(' ')), \n",
    "                                                                                                          '_'.join(classes[1].split(' ')), \n",
    "                                                                                                          '_'.join(classes[2].split(' ')), question)).strip()\n",
    "\n",
    "def classify_input_node(state):\n",
    "    question = state.get('current_response')\n",
    "    classification = classify(question)  # Assume a function that classifies the input\n",
    "    return {\"classification\": classification}\n",
    "\n",
    "# Generates a greeting message\n",
    "def handle_greeting_node(state):\n",
    "    return {\"greeting\": \"Hello! Today we will witness the debate between {}, {}, and {}\".format(classes[0], \n",
    "                                                                                                classes[1], \n",
    "                                                                                                classes[2])}\n",
    "### Manages the state for the \"Labour Party\" side\n",
    "def handle_Lab(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    if summary == 'Nothing':\n",
    "        prompt = prefix_start.format(classes[0], classes[1], classes[2], debate_topic, 'Nothing', classes[0], \"Nothing\", \"Nothing\")\n",
    "        argument = classes[0] + \":\" + llm(prompt)\n",
    "        summary = 'START\\n'\n",
    "    else:\n",
    "        prompt = prefix_start.format(classes[0], classes[1], classes[2], debate_topic, summary, classes[0], current_response, current_response)\n",
    "        argument = classes[0] + \":\" + llm(prompt)\n",
    "    return {\"history\": summary + '\\n' + argument, \"current_response\": argument, \"count\": state.get('count') + 1}\n",
    "\n",
    "### Manages the state for the \"Conservative Party\" side\n",
    "def handle_Cons(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    prompt = prefix_start.format(classes[1], classes[0], classes[2], debate_topic, summary, classes[1], current_response, current_response)\n",
    "    argument = classes[1] + \":\" + llm(prompt)\n",
    "    return {\"history\": summary + '\\n' + argument, \"current_response\": argument, \"count\": state.get('count') + 1}\n",
    "\n",
    "### Manages the state for the \"Liberal Democrats Party\" side\n",
    "def handle_LibDem(state):\n",
    "    summary = state.get('history', '').strip()\n",
    "    current_response = state.get('current_response', '').strip()\n",
    "    prompt = prefix_start.format(classes[2], classes[0], classes[1], debate_topic, summary, classes[2], current_response, current_response)\n",
    "    argument = classes[2] + \":\" + llm(prompt)\n",
    "    return {\"history\": summary + '\\n' + argument, \"current_response\": argument, \"count\": state.get('count') + 1}\n",
    "\n",
    "# Summarizes the Conversation + Judges the Winner\n",
    "def result(state):\n",
    "    summary = state.get('history').strip()\n",
    "    prompt = \"Summarize the conversation and judge who won the debate. No ties are allowed. Conversation:{}\".format(summary)\n",
    "    return {\"results\": llm(prompt)}\n",
    "\n",
    "# Adding nodes to the Workflow\n",
    "workflow.add_node(\"classify_input\", classify_input_node)\n",
    "workflow.add_node(\"handle_greeting\", handle_greeting_node)\n",
    "workflow.add_node(\"handle_Lab\", handle_Lab)\n",
    "workflow.add_node(\"handle_Cons\", handle_Cons)\n",
    "workflow.add_node(\"handle_LibDem\", handle_LibDem)\n",
    "workflow.add_node(\"result\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Workflow & Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c82b6a2d20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determines the next node based on the classification\n",
    "def decide_next_node(state):\n",
    "    if state.get('classification') == '_'.join(classes[0].split(' ')):\n",
    "        return \"handle_Lab\"\n",
    "    elif state.get('classification') == '_'.join(classes[1].split(' ')):\n",
    "        return \"handle_Cons\"\n",
    "    else:\n",
    "        return \"handle_LibDem\"\n",
    "\n",
    "# Checks if the conversation has reached the maximum length\n",
    "def check_conv_length(state):\n",
    "    return \"result\" if state.get(\"count\")== 9 else \"classify_input\"     # Set Debate Length \n",
    "\n",
    "# Add Conditional Transitions \n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_input\",\n",
    "    decide_next_node,\n",
    "    {\n",
    "        \"handle_Lab\": \"handle_Lab\",\n",
    "        \"handle_Cons\": \"handle_Cons\",\n",
    "        \"handle_LibDem\": \"handle_LibDem\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_Lab\",\n",
    "    check_conv_length,                   # determine whether to transition to \"result\" or back to \"classify_input\"\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_Cons\",\n",
    "    check_conv_length,                   # determine whether to transition to \"result\" or back to \"classify_input\"\n",
    "\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"handle_LibDem\",\n",
    "    check_conv_length,                   # determine whether to transition to \"result\" or back to \"classify_input\"\n",
    "    {\n",
    "        \"result\": \"result\",\n",
    "        \"classify_input\": \"classify_input\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setting the Entry point + Ending point of the workflow\n",
    "workflow.set_entry_point(\"handle_greeting\")\n",
    "workflow.add_edge('handle_greeting', \"handle_Lab\")\n",
    "workflow.add_edge('result', END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()\n",
    "conversation = app.invoke({'count':0,'history':'Nothing','current_response':''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "\n",
      "UK Labour Party:\n",
      "\n",
      "The UK Labour Party believes in promoting peace and stability in the Middle East, and providing weapons to Israel only fuels the conflict.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "The UK Liberal Democrats Party believes in a balanced approach to the conflict and supports diplomatic solutions.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "Providing weapons only escalates the violence and hinders the possibility of peace negotiations.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "The UK Liberal Democrats Party believes in promoting non-violent solutions to conflicts.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "Providing weapons to Israel only perpetuates the cycle of violence and undermines efforts for a peaceful resolution.\n",
      "UK Liberal Democrats Party: \n",
      "\n",
      "\n",
      "Arming Israel may also have unintended consequences and could potentially harm innocent civilians in the region.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "Providing weapons to Israel goes against our values of promoting human rights and protecting innocent lives.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "Arming Israel only exacerbates the conflict and does not promote peace.\n",
      "UK Liberal Democrats Party:\n",
      "\n",
      "Instead of arming Israel, we should focus on promoting diplomatic solutions and fostering peace talks between Israel and Palestine.\n"
     ]
    }
   ],
   "source": [
    "print(conversation['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The overall conversation centered around the issue of providing weapons to Israel and how it impacts the conflict in the Middle East. The UK Labour Party argued against arming Israel, stating that it only fuels the conflict and goes against their values of promoting peace and stability. On the other hand, the UK Liberal Democrats Party argued for a more balanced approach, emphasizing the importance of non-violent solutions and the potential harm that could come from providing weapons. They also suggested focusing on diplomatic solutions and promoting peace talks between the two sides. \n",
      "\n",
      "Based on the arguments presented, it can be concluded that the UK Liberal Democrats Party won the debate. They were able to provide a more comprehensive and nuanced perspective, highlighting the potential consequences of arming Israel and offering alternative solutions that align with their values. The UK Labour Party's argument, while valid, was not as strong and did not address the potential drawbacks of not providing weapons to Israel. Therefore, the UK Liberal Democrats Party's arguments were more convincing and ultimately won the debate. \n"
     ]
    }
   ],
   "source": [
    "print(conversation['results'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation - Multiple Agents\n",
    "\n",
    "The code was entirely reused from the example by [Chester](https://medium.com/data-science-in-your-pocket/multi-agent-conversation-debates-using-langgraph-and-langchain-9f4bf711d8ab)'s notebook in Langchain Github respository, with a few minor tweaks and proper configuration of the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "import re\n",
    "import tenacity\n",
    "from typing import List, Dict, Callable\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
